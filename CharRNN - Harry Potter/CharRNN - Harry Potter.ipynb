{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharRNN - Harry Potter\n",
    "\n",
    "Character wise RNN to generate new text based on Harry Potter Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text file and convert it into integers using dictionaries to convert the characters to and from integers. We will only use the 1st book Harry Potter and Sorcerer's Stone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample text from the book -->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('HPBook1.txt', 'r')\n",
    "text = text.read()\n",
    "print('Some sample text from the book -->')\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in book : 78449\n",
      "Total characters in book : 442744\n",
      "Unique words in book : 11897\n",
      "Unique characters in book : 80\n"
     ]
    }
   ],
   "source": [
    "total_words = len(text.split())\n",
    "total_characters = len(text)\n",
    "unique_words = len(set(text.split()))\n",
    "unique_characters = len(set(text))\n",
    "\n",
    "print (\"Total words in book :\", total_words)\n",
    "print (\"Total characters in book :\", total_characters)\n",
    "print (\"Unique words in book :\", unique_words)\n",
    "print (\"Unique characters in book :\", unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Paragraphs : 3033\n"
     ]
    }
   ],
   "source": [
    "paragraphs = text.split('\\n\\n')\n",
    "print (\"Total Paragraphs :\", len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_words = Counter()\n",
    "for i in range(len(paragraphs)):\n",
    "    for x in paragraphs[i]:\n",
    "        most_words[x] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 78449),\n",
       " ('e', 39628),\n",
       " ('t', 27993),\n",
       " ('a', 25887),\n",
       " ('o', 25809),\n",
       " ('n', 21337),\n",
       " ('r', 20990),\n",
       " ('h', 19535),\n",
       " ('i', 19422),\n",
       " ('s', 18870),\n",
       " ('d', 15932),\n",
       " ('l', 14385),\n",
       " ('u', 9562),\n",
       " ('y', 8293),\n",
       " ('g', 8127),\n",
       " ('w', 7744),\n",
       " ('m', 6729),\n",
       " ('f', 6431),\n",
       " ('c', 6403),\n",
       " ('.', 6136),\n",
       " (',', 5658),\n",
       " ('b', 4980),\n",
       " ('p', 4909),\n",
       " ('\"', 4747),\n",
       " ('k', 3930),\n",
       " (\"'\", 3141),\n",
       " ('H', 2996),\n",
       " ('v', 2716),\n",
       " ('-', 1986),\n",
       " ('I', 1393),\n",
       " ('T', 1055),\n",
       " ('S', 844),\n",
       " ('?', 754),\n",
       " ('A', 703),\n",
       " ('D', 685),\n",
       " ('M', 665),\n",
       " ('R', 660),\n",
       " ('W', 653),\n",
       " ('P', 639),\n",
       " ('G', 492),\n",
       " ('N', 488),\n",
       " ('!', 474),\n",
       " ('F', 426),\n",
       " ('x', 381),\n",
       " ('B', 348),\n",
       " ('O', 332),\n",
       " ('Y', 326),\n",
       " ('j', 319),\n",
       " ('C', 293),\n",
       " ('E', 287),\n",
       " ('z', 259),\n",
       " ('q', 217),\n",
       " ('L', 209),\n",
       " ('Q', 203),\n",
       " ('U', 193),\n",
       " ('V', 192),\n",
       " (';', 135),\n",
       " ('K', 79),\n",
       " (':', 69),\n",
       " ('J', 51),\n",
       " (')', 33),\n",
       " ('(', 30),\n",
       " ('“', 11),\n",
       " ('1', 11),\n",
       " ('3', 8),\n",
       " ('4', 6),\n",
       " ('Z', 5),\n",
       " ('0', 5),\n",
       " ('7', 4),\n",
       " ('9', 4),\n",
       " ('2', 3),\n",
       " ('X', 2),\n",
       " ('5', 2),\n",
       " ('*', 2),\n",
       " ('–', 1),\n",
       " ('~', 1),\n",
       " ('8', 1),\n",
       " ('6', 1),\n",
       " ('\\\\', 1)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c : i for i,c in enumerate(vocab)}\n",
    "#int_to_vocab = {i : c for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "embeddings = np.array([vocab_to_int[i] for i in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '?': 23,\n",
       " 'A': 24,\n",
       " 'B': 25,\n",
       " 'C': 26,\n",
       " 'D': 27,\n",
       " 'E': 28,\n",
       " 'F': 29,\n",
       " 'G': 30,\n",
       " 'H': 31,\n",
       " 'I': 32,\n",
       " 'J': 33,\n",
       " 'K': 34,\n",
       " 'L': 35,\n",
       " 'M': 36,\n",
       " 'N': 37,\n",
       " 'O': 38,\n",
       " 'P': 39,\n",
       " 'Q': 40,\n",
       " 'R': 41,\n",
       " 'S': 42,\n",
       " 'T': 43,\n",
       " 'U': 44,\n",
       " 'V': 45,\n",
       " 'W': 46,\n",
       " 'X': 47,\n",
       " 'Y': 48,\n",
       " 'Z': 49,\n",
       " '\\\\': 50,\n",
       " 'a': 51,\n",
       " 'b': 52,\n",
       " 'c': 53,\n",
       " 'd': 54,\n",
       " 'e': 55,\n",
       " 'f': 56,\n",
       " 'g': 57,\n",
       " 'h': 58,\n",
       " 'i': 59,\n",
       " 'j': 60,\n",
       " 'k': 61,\n",
       " 'l': 62,\n",
       " 'm': 63,\n",
       " 'n': 64,\n",
       " 'o': 65,\n",
       " 'p': 66,\n",
       " 'q': 67,\n",
       " 'r': 68,\n",
       " 's': 69,\n",
       " 't': 70,\n",
       " 'u': 71,\n",
       " 'v': 72,\n",
       " 'w': 73,\n",
       " 'x': 74,\n",
       " 'y': 75,\n",
       " 'z': 76,\n",
       " '~': 77,\n",
       " '–': 78,\n",
       " '“': 79}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples encodings (inputs to our NN) -->>\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet D\n",
      "[31 51 68 68 75  1 39 65 70 70 55 68  1 51 64 54  1 70 58 55  1 42 65 68 53\n",
      " 55 68 55 68  4 69  1 42 70 65 64 55  1  0  0 26 31 24 39 43 28 41  1 38 37\n",
      " 28  1  0  0 43 31 28  1 25 38 48  1 46 31 38  1 35 32 45 28 27  1  0  0 36\n",
      " 68 10  1 51 64 54  1 36 68 69 10  1 27 71 68 69 62 55 75  8  1 65 56  1 64\n",
      " 71 63 52 55 68  1 56 65 71 68  8  1 39 68 59 72 55 70  1 27]\n"
     ]
    }
   ],
   "source": [
    "print (\"Examples encodings (inputs to our NN) -->>\")\n",
    "print (text[:120])\n",
    "print (embeddings[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with individual characters, it's a classification problem in which we are trying to predict the next character from the previous text. Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining placeholders for inputs, targets, learning rate and dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholders(batch_size, step_size):\n",
    "    inputs = tf.placeholder(tf.float32, shape=(batch_size, step_size), name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, shape=(batch_size, step_size), name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name= 'keep_prob')\n",
    "    \n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating LSTM cells for our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstms(lstm_size, layers, batch_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm] * layers)\n",
    "    state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_output(lstm_output, input_size, output_size):\n",
    "    sequence = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(sequence, [-1, input_size])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=(input_size, output_size), stddev=0.1))\n",
    "        bias = tf.Variable(tf.zeros(output_size))\n",
    "        \n",
    "    logits = tf.matmul(x, weights) + bias\n",
    "    output = tf.nn.softmax(logits, name='predictions')                          \n",
    "    \n",
    "    return output, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualting loss for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def losses(logits, y, lstm_size, num_classes):\n",
    "    labels = tf.reshape(tf.one_hot(y, num_classes), logits.getshape())\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100        \n",
    "lstm_size = 512 \n",
    "num_layers = 2         \n",
    "learning_rate = 0.001  \n",
    "keep_prob = 0.5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating batches to feed into NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(inputs, batch_size, num_steps):\n",
    "    char_batch = batch_size * num_steps\n",
    "    num_batches = len(inputs)//char_batch\n",
    "    \n",
    "    idx = char_batch * num_batches \n",
    "    inputs = inputs[:idx]\n",
    "    inputs = inputs.reshape((batch_size, -1))\n",
    "    \n",
    "    for i in range(0, inputs.shape[1], num_steps):\n",
    "        x = inputs[:, i : i+num_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = generate_batches(embeddings, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 51 68 68 75  1 39 65 70 70]\n",
      " [ 1 69 66 65 61 55  1 51 52 65]\n",
      " [70 69  1 52 55 51 61 10  1  0]\n",
      " [64 65 69 55 10  1  0  0  3 48]\n",
      " [70 58 55 59 68  1 56 51 53 55]\n",
      " [56 65 75 69  1 69 64 55 55 68]\n",
      " [64 70  1 57 51 59 64 69  1 70]\n",
      " [ 1 51 64 65 70 58 55 68  1 68]\n",
      " [51 68 54  1 58 59 63  1 51 64]\n",
      " [65 62 65 57 75  8  1 31 55 68]]\n",
      "\n",
      "y\n",
      " [[51 68 68 75  1 39 65 70 70 55]\n",
      " [69 66 65 61 55  1 51 52 65 71]\n",
      " [69  1 52 55 51 61 10  1  0  0]\n",
      " [65 69 55 10  1  0  0  3 48 55]\n",
      " [58 55 59 68  1 56 51 53 55 69]\n",
      " [65 75 69  1 69 64 55 55 68 59]\n",
      " [70  1 57 51 59 64 69  1 70 58]\n",
      " [51 64 65 70 58 55 68  1 68 55]\n",
      " [68 54  1 58 59 63  1 51 64 54]\n",
      " [62 65 57 75  8  1 31 55 68 63]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
