{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## CharRNN - Harry Potter\n",
    "\n",
    "Character wise RNN to generate new text based on Harry Potter Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the text file and convert it into integers using dictionaries to convert the characters to and from integers. We will only use the 1st book Harry Potter and Sorcerer's Stone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample text from the book -->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('HPBook1.txt', 'r', encoding = 'utf-8')\n",
    "text = text.read()\n",
    "print('Some sample text from the book -->')\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Some text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in book : 78449\n",
      "Total characters in book : 442744\n",
      "Unique words in book : 11897\n",
      "Unique characters in book : 80\n"
     ]
    }
   ],
   "source": [
    "total_words = len(text.split())\n",
    "total_characters = len(text)\n",
    "unique_words = len(set(text.split()))\n",
    "unique_characters = len(set(text))\n",
    "\n",
    "print (\"Total words in book :\", total_words)\n",
    "print (\"Total characters in book :\", total_characters)\n",
    "print (\"Unique words in book :\", unique_words)\n",
    "print (\"Unique characters in book :\", unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Paragraphs : 3033\n"
     ]
    }
   ],
   "source": [
    "paragraphs = text.split('\\n\\n')\n",
    "print (\"Total Paragraphs :\", len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "most_words = Counter()\n",
    "for i in range(len(paragraphs)):\n",
    "    for x in paragraphs[i]:\n",
    "        most_words[x] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 78449),\n",
       " ('e', 39628),\n",
       " ('t', 27993),\n",
       " ('a', 25887),\n",
       " ('o', 25809),\n",
       " ('n', 21337),\n",
       " ('r', 20990),\n",
       " ('h', 19535),\n",
       " ('i', 19422),\n",
       " ('s', 18870),\n",
       " ('d', 15932),\n",
       " ('l', 14385),\n",
       " ('u', 9562),\n",
       " ('y', 8293),\n",
       " ('g', 8127),\n",
       " ('w', 7744),\n",
       " ('m', 6729),\n",
       " ('f', 6431),\n",
       " ('c', 6403),\n",
       " ('.', 6136),\n",
       " (',', 5658),\n",
       " ('b', 4980),\n",
       " ('p', 4909),\n",
       " ('\"', 4747),\n",
       " ('k', 3930),\n",
       " (\"'\", 3141),\n",
       " ('H', 2996),\n",
       " ('v', 2716),\n",
       " ('-', 1986),\n",
       " ('I', 1393),\n",
       " ('T', 1055),\n",
       " ('S', 844),\n",
       " ('?', 754),\n",
       " ('A', 703),\n",
       " ('D', 685),\n",
       " ('M', 665),\n",
       " ('R', 660),\n",
       " ('W', 653),\n",
       " ('P', 639),\n",
       " ('G', 492),\n",
       " ('N', 488),\n",
       " ('!', 474),\n",
       " ('F', 426),\n",
       " ('x', 381),\n",
       " ('B', 348),\n",
       " ('O', 332),\n",
       " ('Y', 326),\n",
       " ('j', 319),\n",
       " ('C', 293),\n",
       " ('E', 287),\n",
       " ('z', 259),\n",
       " ('q', 217),\n",
       " ('L', 209),\n",
       " ('Q', 203),\n",
       " ('U', 193),\n",
       " ('V', 192),\n",
       " (';', 135),\n",
       " ('K', 79),\n",
       " (':', 69),\n",
       " ('J', 51),\n",
       " (')', 33),\n",
       " ('(', 30),\n",
       " ('“', 11),\n",
       " ('1', 11),\n",
       " ('3', 8),\n",
       " ('4', 6),\n",
       " ('0', 5),\n",
       " ('Z', 5),\n",
       " ('9', 4),\n",
       " ('7', 4),\n",
       " ('2', 3),\n",
       " ('X', 2),\n",
       " ('*', 2),\n",
       " ('5', 2),\n",
       " ('6', 1),\n",
       " ('8', 1),\n",
       " ('\\\\', 1),\n",
       " ('–', 1),\n",
       " ('~', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '\\\\',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '~',\n",
       " '–',\n",
       " '“'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c : i for i,c in enumerate(vocab)}\n",
    "#int_to_vocab = {i : c for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "embeddings = np.array([vocab_to_int[i] for i in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '?': 23,\n",
       " 'A': 24,\n",
       " 'B': 25,\n",
       " 'C': 26,\n",
       " 'D': 27,\n",
       " 'E': 28,\n",
       " 'F': 29,\n",
       " 'G': 30,\n",
       " 'H': 31,\n",
       " 'I': 32,\n",
       " 'J': 33,\n",
       " 'K': 34,\n",
       " 'L': 35,\n",
       " 'M': 36,\n",
       " 'N': 37,\n",
       " 'O': 38,\n",
       " 'P': 39,\n",
       " 'Q': 40,\n",
       " 'R': 41,\n",
       " 'S': 42,\n",
       " 'T': 43,\n",
       " 'U': 44,\n",
       " 'V': 45,\n",
       " 'W': 46,\n",
       " 'X': 47,\n",
       " 'Y': 48,\n",
       " 'Z': 49,\n",
       " '\\\\': 50,\n",
       " 'a': 51,\n",
       " 'b': 52,\n",
       " 'c': 53,\n",
       " 'd': 54,\n",
       " 'e': 55,\n",
       " 'f': 56,\n",
       " 'g': 57,\n",
       " 'h': 58,\n",
       " 'i': 59,\n",
       " 'j': 60,\n",
       " 'k': 61,\n",
       " 'l': 62,\n",
       " 'm': 63,\n",
       " 'n': 64,\n",
       " 'o': 65,\n",
       " 'p': 66,\n",
       " 'q': 67,\n",
       " 'r': 68,\n",
       " 's': 69,\n",
       " 't': 70,\n",
       " 'u': 71,\n",
       " 'v': 72,\n",
       " 'w': 73,\n",
       " 'x': 74,\n",
       " 'y': 75,\n",
       " 'z': 76,\n",
       " '~': 77,\n",
       " '–': 78,\n",
       " '“': 79}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples encodings (inputs to our NN) -->>\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet D\n",
      "[31 51 68 68 75  1 39 65 70 70 55 68  1 51 64 54  1 70 58 55  1 42 65 68 53\n",
      " 55 68 55 68  4 69  1 42 70 65 64 55  1  0  0 26 31 24 39 43 28 41  1 38 37\n",
      " 28  1  0  0 43 31 28  1 25 38 48  1 46 31 38  1 35 32 45 28 27  1  0  0 36\n",
      " 68 10  1 51 64 54  1 36 68 69 10  1 27 71 68 69 62 55 75  8  1 65 56  1 64\n",
      " 71 63 52 55 68  1 56 65 71 68  8  1 39 68 59 72 55 70  1 27]\n"
     ]
    }
   ],
   "source": [
    "print (\"Examples encodings (inputs to our NN) -->>\")\n",
    "print (text[:120])\n",
    "print (embeddings[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since we are working with individual characters, it's a classification problem in which we are trying to predict the next character from the previous text. Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Defining placeholders for inputs, targets, learning rate and dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def placeholders(batch_size, step_size):\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size, step_size), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size, step_size), name='targets')\n",
    "    #lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name= 'prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Creating LSTM cells for our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lstms(lstm_size, layers, batch_size, keep_prob):\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(layers)])\n",
    "    state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Output from RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rnn_output(lstm_output, input_size, output_size):\n",
    "    sequence = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(sequence, [-1, input_size])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=(input_size, output_size), stddev=0.1))\n",
    "        bias = tf.Variable(tf.zeros(output_size))\n",
    "        \n",
    "    logits = tf.matmul(x, weights) + bias\n",
    "    predictions = tf.nn.softmax(logits, name='predictions')                          \n",
    "    \n",
    "    return predictions, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Calcualting loss for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def losses(logits, y, lstm_size, num_classes):\n",
    "    labels = tf.reshape(tf.one_hot(y, num_classes), logits.get_shape())\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_steps = 100        \n",
    "lstm_size = 512 \n",
    "num_layers = 2         \n",
    "learning_rate = 0.001  \n",
    "prob = 0.5 \n",
    "grad_clip = 5\n",
    "num_classes = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generating batches to feed into NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(inputs, batch_size, num_steps):\n",
    "    char_batch = batch_size * num_steps\n",
    "    num_batches = len(inputs)//char_batch\n",
    "    \n",
    "    idx = char_batch * num_batches \n",
    "    inputs = inputs[:idx]\n",
    "    inputs = inputs.reshape((batch_size, -1))\n",
    "    \n",
    "    for i in range(0, inputs.shape[1], num_steps):\n",
    "        x = inputs[:, i : i+num_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = generate_batches(embeddings, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 51 68 68 75  1 39 65 70 70]\n",
      " [ 1 69 66 65 61 55  1 51 52 65]\n",
      " [70 69  1 52 55 51 61 10  1  0]\n",
      " [64 65 69 55 10  1  0  0  3 48]\n",
      " [70 58 55 59 68  1 56 51 53 55]\n",
      " [56 65 75 69  1 69 64 55 55 68]\n",
      " [64 70  1 57 51 59 64 69  1 70]\n",
      " [ 1 51 64 65 70 58 55 68  1 68]\n",
      " [51 68 54  1 58 59 63  1 51 64]\n",
      " [65 62 65 57 75  8  1 31 55 68]]\n",
      "\n",
      "y\n",
      " [[51 68 68 75  1 39 65 70 70 55]\n",
      " [69 66 65 61 55  1 51 52 65 71]\n",
      " [69  1 52 55 51 61 10  1  0  0]\n",
      " [65 69 55 10  1  0  0  3 48 55]\n",
      " [58 55 59 68  1 56 51 53 55 69]\n",
      " [65 75 69  1 69 64 55 55 68 59]\n",
      " [70  1 57 51 59 64 69  1 70 58]\n",
      " [51 64 65 70 58 55 68  1 68 55]\n",
      " [68 54  1 58 59 63  1 51 64 54]\n",
      " [62 65 57 75  8  1 31 55 68 63]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Optimizer and fixing exploding gradient problem by clipping beyond a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cal_optimizer(loss, learning_rate, clipping):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), clipping)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setting up all variables and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## Getting input tensors\n",
    "inputs, targets, keep_prob = placeholders(batch_size, num_steps)\n",
    "\n",
    "## Creating LSTM Cell\n",
    "cell, initial_state = lstms(lstm_size, num_layers, batch_size, keep_prob)\n",
    "\n",
    "## One hot encode inputs to run the data through RNN layers\n",
    "one_hot_inputs = tf.one_hot(inputs, num_classes)\n",
    "\n",
    "## Collect outputs\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, one_hot_inputs, initial_state = initial_state)\n",
    "final_state = state\n",
    "\n",
    "## Predictions and Logits\n",
    "predictions, logits = rnn_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "## Getting losses and optimizer(ADAM)\n",
    "loss = losses(logits, targets, lstm_size, num_classes)\n",
    "optimizer = cal_optimizer(loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100...  Training Step: 1...  Training loss: 4.3825...  0.5479 sec/batch\n",
      "Epoch: 1/100...  Training Step: 2...  Training loss: 4.2962...  0.4943 sec/batch\n",
      "Epoch: 1/100...  Training Step: 3...  Training loss: 3.8650...  0.4928 sec/batch\n",
      "Epoch: 1/100...  Training Step: 4...  Training loss: 5.3728...  0.4937 sec/batch\n",
      "Epoch: 1/100...  Training Step: 5...  Training loss: 4.2237...  0.4943 sec/batch\n",
      "Epoch: 1/100...  Training Step: 6...  Training loss: 3.8735...  0.4941 sec/batch\n",
      "Epoch: 1/100...  Training Step: 7...  Training loss: 3.6982...  0.4940 sec/batch\n",
      "Epoch: 1/100...  Training Step: 8...  Training loss: 3.5618...  0.4931 sec/batch\n",
      "Epoch: 1/100...  Training Step: 9...  Training loss: 3.4899...  0.4932 sec/batch\n",
      "Epoch: 1/100...  Training Step: 10...  Training loss: 3.4777...  0.4929 sec/batch\n",
      "Epoch: 1/100...  Training Step: 11...  Training loss: 3.4496...  0.4926 sec/batch\n",
      "Epoch: 1/100...  Training Step: 12...  Training loss: 3.4204...  0.4936 sec/batch\n",
      "Epoch: 1/100...  Training Step: 13...  Training loss: 3.4134...  0.4929 sec/batch\n",
      "Epoch: 1/100...  Training Step: 14...  Training loss: 3.3474...  0.4925 sec/batch\n",
      "Epoch: 1/100...  Training Step: 15...  Training loss: 3.3321...  0.4930 sec/batch\n",
      "Epoch: 1/100...  Training Step: 16...  Training loss: 3.3328...  0.4932 sec/batch\n",
      "Epoch: 1/100...  Training Step: 17...  Training loss: 3.3021...  0.4933 sec/batch\n",
      "Epoch: 1/100...  Training Step: 18...  Training loss: 3.3252...  0.4933 sec/batch\n",
      "Epoch: 1/100...  Training Step: 19...  Training loss: 3.3108...  0.4926 sec/batch\n",
      "Epoch: 1/100...  Training Step: 20...  Training loss: 3.2996...  0.4927 sec/batch\n",
      "Epoch: 1/100...  Training Step: 21...  Training loss: 3.2725...  0.4930 sec/batch\n",
      "Epoch: 1/100...  Training Step: 22...  Training loss: 3.2758...  0.4935 sec/batch\n",
      "Epoch: 1/100...  Training Step: 23...  Training loss: 3.2525...  0.4933 sec/batch\n",
      "Epoch: 1/100...  Training Step: 24...  Training loss: 3.2786...  0.4934 sec/batch\n",
      "Epoch: 1/100...  Training Step: 25...  Training loss: 3.2377...  0.4937 sec/batch\n",
      "Epoch: 1/100...  Training Step: 26...  Training loss: 3.2634...  0.4935 sec/batch\n",
      "Epoch: 1/100...  Training Step: 27...  Training loss: 3.2773...  0.4942 sec/batch\n",
      "Epoch: 1/100...  Training Step: 28...  Training loss: 3.2265...  0.4934 sec/batch\n",
      "Epoch: 1/100...  Training Step: 29...  Training loss: 3.2328...  0.4934 sec/batch\n",
      "Epoch: 1/100...  Training Step: 30...  Training loss: 3.2773...  0.4942 sec/batch\n",
      "Epoch: 1/100...  Training Step: 31...  Training loss: 3.2563...  0.4935 sec/batch\n",
      "Epoch: 1/100...  Training Step: 32...  Training loss: 3.2334...  0.4929 sec/batch\n",
      "Epoch: 1/100...  Training Step: 33...  Training loss: 3.2207...  0.4937 sec/batch\n",
      "Epoch: 1/100...  Training Step: 34...  Training loss: 3.2405...  0.4931 sec/batch\n",
      "Epoch: 2/100...  Training Step: 35...  Training loss: 3.3292...  0.4929 sec/batch\n",
      "Epoch: 2/100...  Training Step: 36...  Training loss: 3.2245...  0.4932 sec/batch\n",
      "Epoch: 2/100...  Training Step: 37...  Training loss: 3.2076...  0.4939 sec/batch\n",
      "Epoch: 2/100...  Training Step: 38...  Training loss: 3.2130...  0.4942 sec/batch\n",
      "Epoch: 2/100...  Training Step: 39...  Training loss: 3.2252...  0.4943 sec/batch\n",
      "Epoch: 2/100...  Training Step: 40...  Training loss: 3.2179...  0.4933 sec/batch\n",
      "Epoch: 2/100...  Training Step: 41...  Training loss: 3.2058...  0.4936 sec/batch\n",
      "Epoch: 2/100...  Training Step: 42...  Training loss: 3.2229...  0.4947 sec/batch\n",
      "Epoch: 2/100...  Training Step: 43...  Training loss: 3.2083...  0.4932 sec/batch\n",
      "Epoch: 2/100...  Training Step: 44...  Training loss: 3.2197...  0.4948 sec/batch\n",
      "Epoch: 2/100...  Training Step: 45...  Training loss: 3.2123...  0.4933 sec/batch\n",
      "Epoch: 2/100...  Training Step: 46...  Training loss: 3.2037...  0.4936 sec/batch\n",
      "Epoch: 2/100...  Training Step: 47...  Training loss: 3.2146...  0.4939 sec/batch\n",
      "Epoch: 2/100...  Training Step: 48...  Training loss: 3.1723...  0.4936 sec/batch\n",
      "Epoch: 2/100...  Training Step: 49...  Training loss: 3.1752...  0.4938 sec/batch\n",
      "Epoch: 2/100...  Training Step: 50...  Training loss: 3.1935...  0.4934 sec/batch\n",
      "Epoch: 2/100...  Training Step: 51...  Training loss: 3.1808...  0.4936 sec/batch\n",
      "Epoch: 2/100...  Training Step: 52...  Training loss: 3.2059...  0.4926 sec/batch\n",
      "Epoch: 2/100...  Training Step: 53...  Training loss: 3.1904...  0.4929 sec/batch\n",
      "Epoch: 2/100...  Training Step: 54...  Training loss: 3.1861...  0.4936 sec/batch\n",
      "Epoch: 2/100...  Training Step: 55...  Training loss: 3.1716...  0.4936 sec/batch\n",
      "Epoch: 2/100...  Training Step: 56...  Training loss: 3.1875...  0.4932 sec/batch\n",
      "Epoch: 2/100...  Training Step: 57...  Training loss: 3.1673...  0.4927 sec/batch\n",
      "Epoch: 2/100...  Training Step: 58...  Training loss: 3.2026...  0.4928 sec/batch\n",
      "Epoch: 2/100...  Training Step: 59...  Training loss: 3.1615...  0.4933 sec/batch\n",
      "Epoch: 2/100...  Training Step: 60...  Training loss: 3.1909...  0.4940 sec/batch\n",
      "Epoch: 2/100...  Training Step: 61...  Training loss: 3.2030...  0.4948 sec/batch\n",
      "Epoch: 2/100...  Training Step: 62...  Training loss: 3.1613...  0.4942 sec/batch\n",
      "Epoch: 2/100...  Training Step: 63...  Training loss: 3.1728...  0.4938 sec/batch\n",
      "Epoch: 2/100...  Training Step: 64...  Training loss: 3.2181...  0.4930 sec/batch\n",
      "Epoch: 2/100...  Training Step: 65...  Training loss: 3.1932...  0.4939 sec/batch\n",
      "Epoch: 2/100...  Training Step: 66...  Training loss: 3.1768...  0.4927 sec/batch\n",
      "Epoch: 2/100...  Training Step: 67...  Training loss: 3.1595...  0.4937 sec/batch\n",
      "Epoch: 2/100...  Training Step: 68...  Training loss: 3.2973...  0.4938 sec/batch\n",
      "Epoch: 3/100...  Training Step: 69...  Training loss: 3.2424...  0.4934 sec/batch\n",
      "Epoch: 3/100...  Training Step: 70...  Training loss: 3.2536...  0.4936 sec/batch\n",
      "Epoch: 3/100...  Training Step: 71...  Training loss: 3.2233...  0.4947 sec/batch\n",
      "Epoch: 3/100...  Training Step: 72...  Training loss: 3.1692...  0.4946 sec/batch\n",
      "Epoch: 3/100...  Training Step: 73...  Training loss: 3.1723...  0.4945 sec/batch\n",
      "Epoch: 3/100...  Training Step: 74...  Training loss: 3.1625...  0.4935 sec/batch\n",
      "Epoch: 3/100...  Training Step: 75...  Training loss: 3.1497...  0.4930 sec/batch\n",
      "Epoch: 3/100...  Training Step: 76...  Training loss: 3.1705...  0.4935 sec/batch\n",
      "Epoch: 3/100...  Training Step: 77...  Training loss: 3.1536...  0.4917 sec/batch\n",
      "Epoch: 3/100...  Training Step: 78...  Training loss: 3.1553...  0.4937 sec/batch\n",
      "Epoch: 3/100...  Training Step: 79...  Training loss: 3.1677...  0.4934 sec/batch\n",
      "Epoch: 3/100...  Training Step: 80...  Training loss: 3.1511...  0.4935 sec/batch\n",
      "Epoch: 3/100...  Training Step: 81...  Training loss: 3.1670...  0.4939 sec/batch\n",
      "Epoch: 3/100...  Training Step: 82...  Training loss: 3.1201...  0.4936 sec/batch\n",
      "Epoch: 3/100...  Training Step: 83...  Training loss: 3.1262...  0.4934 sec/batch\n",
      "Epoch: 3/100...  Training Step: 84...  Training loss: 3.1423...  0.4935 sec/batch\n",
      "Epoch: 3/100...  Training Step: 85...  Training loss: 3.1187...  0.4931 sec/batch\n",
      "Epoch: 3/100...  Training Step: 86...  Training loss: 3.1474...  0.4936 sec/batch\n",
      "Epoch: 3/100...  Training Step: 87...  Training loss: 3.1306...  0.4941 sec/batch\n",
      "Epoch: 3/100...  Training Step: 88...  Training loss: 3.1182...  0.4950 sec/batch\n",
      "Epoch: 3/100...  Training Step: 89...  Training loss: 3.1097...  0.4934 sec/batch\n",
      "Epoch: 3/100...  Training Step: 90...  Training loss: 3.1150...  0.4944 sec/batch\n",
      "Epoch: 3/100...  Training Step: 91...  Training loss: 3.0919...  0.4930 sec/batch\n",
      "Epoch: 3/100...  Training Step: 92...  Training loss: 3.1249...  0.4927 sec/batch\n",
      "Epoch: 3/100...  Training Step: 93...  Training loss: 3.0781...  0.4938 sec/batch\n",
      "Epoch: 3/100...  Training Step: 94...  Training loss: 3.1349...  0.4927 sec/batch\n",
      "Epoch: 3/100...  Training Step: 95...  Training loss: 3.1226...  0.4928 sec/batch\n",
      "Epoch: 3/100...  Training Step: 96...  Training loss: 3.0857...  0.4932 sec/batch\n",
      "Epoch: 3/100...  Training Step: 97...  Training loss: 3.1019...  0.4946 sec/batch\n",
      "Epoch: 3/100...  Training Step: 98...  Training loss: 3.1291...  0.4939 sec/batch\n",
      "Epoch: 3/100...  Training Step: 99...  Training loss: 3.1068...  0.4935 sec/batch\n",
      "Epoch: 3/100...  Training Step: 100...  Training loss: 3.0924...  0.4929 sec/batch\n",
      "Epoch: 3/100...  Training Step: 101...  Training loss: 3.0730...  0.4936 sec/batch\n",
      "Epoch: 3/100...  Training Step: 102...  Training loss: 3.0771...  0.4945 sec/batch\n",
      "Epoch: 4/100...  Training Step: 103...  Training loss: 3.1256...  0.4933 sec/batch\n",
      "Epoch: 4/100...  Training Step: 104...  Training loss: 3.0593...  0.4940 sec/batch\n",
      "Epoch: 4/100...  Training Step: 105...  Training loss: 3.0411...  0.4935 sec/batch\n",
      "Epoch: 4/100...  Training Step: 106...  Training loss: 3.0402...  0.4936 sec/batch\n",
      "Epoch: 4/100...  Training Step: 107...  Training loss: 3.0365...  0.4941 sec/batch\n",
      "Epoch: 4/100...  Training Step: 108...  Training loss: 3.0067...  0.4938 sec/batch\n",
      "Epoch: 4/100...  Training Step: 109...  Training loss: 2.9917...  0.4936 sec/batch\n",
      "Epoch: 4/100...  Training Step: 110...  Training loss: 3.0283...  0.4938 sec/batch\n",
      "Epoch: 4/100...  Training Step: 111...  Training loss: 2.9934...  0.4938 sec/batch\n",
      "Epoch: 4/100...  Training Step: 112...  Training loss: 2.9945...  0.4939 sec/batch\n",
      "Epoch: 4/100...  Training Step: 113...  Training loss: 2.9944...  0.4934 sec/batch\n",
      "Epoch: 4/100...  Training Step: 114...  Training loss: 2.9791...  0.4935 sec/batch\n",
      "Epoch: 4/100...  Training Step: 115...  Training loss: 2.9775...  0.4932 sec/batch\n",
      "Epoch: 4/100...  Training Step: 116...  Training loss: 2.9296...  0.4942 sec/batch\n",
      "Epoch: 4/100...  Training Step: 117...  Training loss: 2.9341...  0.4934 sec/batch\n",
      "Epoch: 4/100...  Training Step: 118...  Training loss: 2.9446...  0.4938 sec/batch\n",
      "Epoch: 4/100...  Training Step: 119...  Training loss: 2.9265...  0.4941 sec/batch\n",
      "Epoch: 4/100...  Training Step: 120...  Training loss: 2.9348...  0.4937 sec/batch\n",
      "Epoch: 4/100...  Training Step: 121...  Training loss: 2.9102...  0.4935 sec/batch\n",
      "Epoch: 4/100...  Training Step: 122...  Training loss: 2.8742...  0.4940 sec/batch\n",
      "Epoch: 4/100...  Training Step: 123...  Training loss: 2.8745...  0.4936 sec/batch\n",
      "Epoch: 4/100...  Training Step: 124...  Training loss: 2.8736...  0.4933 sec/batch\n",
      "Epoch: 4/100...  Training Step: 125...  Training loss: 2.8457...  0.4933 sec/batch\n",
      "Epoch: 4/100...  Training Step: 126...  Training loss: 2.8672...  0.4940 sec/batch\n",
      "Epoch: 4/100...  Training Step: 127...  Training loss: 2.8240...  0.4938 sec/batch\n",
      "Epoch: 4/100...  Training Step: 128...  Training loss: 2.8374...  0.4943 sec/batch\n",
      "Epoch: 4/100...  Training Step: 129...  Training loss: 2.8345...  0.4943 sec/batch\n",
      "Epoch: 4/100...  Training Step: 130...  Training loss: 2.7921...  0.4949 sec/batch\n",
      "Epoch: 4/100...  Training Step: 131...  Training loss: 2.7964...  0.4939 sec/batch\n",
      "Epoch: 4/100...  Training Step: 132...  Training loss: 2.8906...  0.4937 sec/batch\n",
      "Epoch: 4/100...  Training Step: 133...  Training loss: 2.9078...  0.4944 sec/batch\n",
      "Epoch: 4/100...  Training Step: 134...  Training loss: 2.8285...  0.4936 sec/batch\n",
      "Epoch: 4/100...  Training Step: 135...  Training loss: 2.8273...  0.4935 sec/batch\n",
      "Epoch: 4/100...  Training Step: 136...  Training loss: 2.8226...  0.4937 sec/batch\n",
      "Epoch: 5/100...  Training Step: 137...  Training loss: 2.8689...  0.4925 sec/batch\n",
      "Epoch: 5/100...  Training Step: 138...  Training loss: 2.8021...  0.4934 sec/batch\n",
      "Epoch: 5/100...  Training Step: 139...  Training loss: 2.7828...  0.4944 sec/batch\n",
      "Epoch: 5/100...  Training Step: 140...  Training loss: 2.7859...  0.4940 sec/batch\n",
      "Epoch: 5/100...  Training Step: 141...  Training loss: 2.7780...  0.4932 sec/batch\n",
      "Epoch: 5/100...  Training Step: 142...  Training loss: 2.7500...  0.4933 sec/batch\n",
      "Epoch: 5/100...  Training Step: 143...  Training loss: 2.7313...  0.4939 sec/batch\n",
      "Epoch: 5/100...  Training Step: 144...  Training loss: 2.7265...  0.4939 sec/batch\n",
      "Epoch: 5/100...  Training Step: 145...  Training loss: 2.7160...  0.4938 sec/batch\n",
      "Epoch: 5/100...  Training Step: 146...  Training loss: 2.7162...  0.4934 sec/batch\n",
      "Epoch: 5/100...  Training Step: 147...  Training loss: 2.7180...  0.4948 sec/batch\n",
      "Epoch: 5/100...  Training Step: 148...  Training loss: 2.6947...  0.4924 sec/batch\n",
      "Epoch: 5/100...  Training Step: 149...  Training loss: 2.7111...  0.4939 sec/batch\n",
      "Epoch: 5/100...  Training Step: 150...  Training loss: 2.6633...  0.4940 sec/batch\n",
      "Epoch: 5/100...  Training Step: 151...  Training loss: 2.6547...  0.4942 sec/batch\n",
      "Epoch: 5/100...  Training Step: 152...  Training loss: 2.6664...  0.4928 sec/batch\n",
      "Epoch: 5/100...  Training Step: 153...  Training loss: 2.6535...  0.4929 sec/batch\n",
      "Epoch: 5/100...  Training Step: 154...  Training loss: 2.6596...  0.4936 sec/batch\n",
      "Epoch: 5/100...  Training Step: 155...  Training loss: 2.6299...  0.4934 sec/batch\n",
      "Epoch: 5/100...  Training Step: 156...  Training loss: 2.6024...  0.4938 sec/batch\n",
      "Epoch: 5/100...  Training Step: 157...  Training loss: 2.5927...  0.4937 sec/batch\n",
      "Epoch: 5/100...  Training Step: 158...  Training loss: 2.6021...  0.4938 sec/batch\n",
      "Epoch: 5/100...  Training Step: 159...  Training loss: 2.5908...  0.4937 sec/batch\n",
      "Epoch: 5/100...  Training Step: 160...  Training loss: 2.6110...  0.4933 sec/batch\n",
      "Epoch: 5/100...  Training Step: 161...  Training loss: 2.5599...  0.4936 sec/batch\n",
      "Epoch: 5/100...  Training Step: 162...  Training loss: 2.5814...  0.4937 sec/batch\n",
      "Epoch: 5/100...  Training Step: 163...  Training loss: 2.5730...  0.4936 sec/batch\n",
      "Epoch: 5/100...  Training Step: 164...  Training loss: 2.5453...  0.4942 sec/batch\n",
      "Epoch: 5/100...  Training Step: 165...  Training loss: 2.5578...  0.4937 sec/batch\n",
      "Epoch: 5/100...  Training Step: 166...  Training loss: 2.5980...  0.4941 sec/batch\n",
      "Epoch: 5/100...  Training Step: 167...  Training loss: 2.5545...  0.4942 sec/batch\n",
      "Epoch: 5/100...  Training Step: 168...  Training loss: 2.5494...  0.4937 sec/batch\n",
      "Epoch: 5/100...  Training Step: 169...  Training loss: 2.5531...  0.4929 sec/batch\n",
      "Epoch: 5/100...  Training Step: 170...  Training loss: 2.5368...  0.4941 sec/batch\n",
      "Epoch: 6/100...  Training Step: 171...  Training loss: 2.6111...  0.4936 sec/batch\n",
      "Epoch: 6/100...  Training Step: 172...  Training loss: 2.5304...  0.4954 sec/batch\n",
      "Epoch: 6/100...  Training Step: 173...  Training loss: 2.5150...  0.4940 sec/batch\n",
      "Epoch: 6/100...  Training Step: 174...  Training loss: 2.5280...  0.4939 sec/batch\n",
      "Epoch: 6/100...  Training Step: 175...  Training loss: 2.5328...  0.4934 sec/batch\n",
      "Epoch: 6/100...  Training Step: 176...  Training loss: 2.4885...  0.4939 sec/batch\n",
      "Epoch: 6/100...  Training Step: 177...  Training loss: 2.4988...  0.4937 sec/batch\n",
      "Epoch: 6/100...  Training Step: 178...  Training loss: 2.4902...  0.4936 sec/batch\n",
      "Epoch: 6/100...  Training Step: 179...  Training loss: 2.4874...  0.4942 sec/batch\n",
      "Epoch: 6/100...  Training Step: 180...  Training loss: 2.4839...  0.4936 sec/batch\n",
      "Epoch: 6/100...  Training Step: 181...  Training loss: 2.5021...  0.4940 sec/batch\n",
      "Epoch: 6/100...  Training Step: 182...  Training loss: 2.4949...  0.4937 sec/batch\n",
      "Epoch: 6/100...  Training Step: 183...  Training loss: 2.4995...  0.4940 sec/batch\n",
      "Epoch: 6/100...  Training Step: 184...  Training loss: 2.4383...  0.4944 sec/batch\n",
      "Epoch: 6/100...  Training Step: 185...  Training loss: 2.4415...  0.4936 sec/batch\n",
      "Epoch: 6/100...  Training Step: 186...  Training loss: 2.4814...  0.4928 sec/batch\n",
      "Epoch: 6/100...  Training Step: 187...  Training loss: 2.4670...  0.4941 sec/batch\n",
      "Epoch: 6/100...  Training Step: 188...  Training loss: 2.4745...  0.4944 sec/batch\n",
      "Epoch: 6/100...  Training Step: 189...  Training loss: 2.4468...  0.4935 sec/batch\n",
      "Epoch: 6/100...  Training Step: 190...  Training loss: 2.4247...  0.4937 sec/batch\n",
      "Epoch: 6/100...  Training Step: 191...  Training loss: 2.4330...  0.4933 sec/batch\n",
      "Epoch: 6/100...  Training Step: 192...  Training loss: 2.4227...  0.4937 sec/batch\n",
      "Epoch: 6/100...  Training Step: 193...  Training loss: 2.4176...  0.4939 sec/batch\n",
      "Epoch: 6/100...  Training Step: 194...  Training loss: 2.4443...  0.4938 sec/batch\n",
      "Epoch: 6/100...  Training Step: 195...  Training loss: 2.4045...  0.4934 sec/batch\n",
      "Epoch: 6/100...  Training Step: 196...  Training loss: 2.4236...  0.4930 sec/batch\n",
      "Epoch: 6/100...  Training Step: 197...  Training loss: 2.4230...  0.4936 sec/batch\n",
      "Epoch: 6/100...  Training Step: 198...  Training loss: 2.4039...  0.4937 sec/batch\n",
      "Epoch: 6/100...  Training Step: 199...  Training loss: 2.4114...  0.4935 sec/batch\n",
      "Epoch: 6/100...  Training Step: 200...  Training loss: 2.4460...  0.4935 sec/batch\n",
      "Epoch: 6/100...  Training Step: 201...  Training loss: 2.4131...  0.4936 sec/batch\n",
      "Epoch: 6/100...  Training Step: 202...  Training loss: 2.4152...  0.4938 sec/batch\n",
      "Epoch: 6/100...  Training Step: 203...  Training loss: 2.4217...  0.4936 sec/batch\n",
      "Epoch: 6/100...  Training Step: 204...  Training loss: 2.4100...  0.4952 sec/batch\n",
      "Epoch: 7/100...  Training Step: 205...  Training loss: 2.4838...  0.4931 sec/batch\n",
      "Epoch: 7/100...  Training Step: 206...  Training loss: 2.3944...  0.4936 sec/batch\n",
      "Epoch: 7/100...  Training Step: 207...  Training loss: 2.3983...  0.4936 sec/batch\n",
      "Epoch: 7/100...  Training Step: 208...  Training loss: 2.4222...  0.4937 sec/batch\n",
      "Epoch: 7/100...  Training Step: 209...  Training loss: 2.4120...  0.4943 sec/batch\n",
      "Epoch: 7/100...  Training Step: 210...  Training loss: 2.3686...  0.4934 sec/batch\n",
      "Epoch: 7/100...  Training Step: 211...  Training loss: 2.3800...  0.4936 sec/batch\n",
      "Epoch: 7/100...  Training Step: 212...  Training loss: 2.3687...  0.4937 sec/batch\n",
      "Epoch: 7/100...  Training Step: 213...  Training loss: 2.3649...  0.4937 sec/batch\n",
      "Epoch: 7/100...  Training Step: 214...  Training loss: 2.3740...  0.4939 sec/batch\n",
      "Epoch: 7/100...  Training Step: 215...  Training loss: 2.3979...  0.4948 sec/batch\n",
      "Epoch: 7/100...  Training Step: 216...  Training loss: 2.3904...  0.4938 sec/batch\n",
      "Epoch: 7/100...  Training Step: 217...  Training loss: 2.3871...  0.4935 sec/batch\n",
      "Epoch: 7/100...  Training Step: 218...  Training loss: 2.3360...  0.4931 sec/batch\n",
      "Epoch: 7/100...  Training Step: 219...  Training loss: 2.3405...  0.4941 sec/batch\n",
      "Epoch: 7/100...  Training Step: 220...  Training loss: 2.3792...  0.4939 sec/batch\n",
      "Epoch: 7/100...  Training Step: 221...  Training loss: 2.3560...  0.4938 sec/batch\n",
      "Epoch: 7/100...  Training Step: 222...  Training loss: 2.3631...  0.4943 sec/batch\n",
      "Epoch: 7/100...  Training Step: 223...  Training loss: 2.3530...  0.4935 sec/batch\n",
      "Epoch: 7/100...  Training Step: 224...  Training loss: 2.3188...  0.4930 sec/batch\n",
      "Epoch: 7/100...  Training Step: 225...  Training loss: 2.3314...  0.4932 sec/batch\n",
      "Epoch: 7/100...  Training Step: 226...  Training loss: 2.3152...  0.4939 sec/batch\n",
      "Epoch: 7/100...  Training Step: 227...  Training loss: 2.3265...  0.4933 sec/batch\n",
      "Epoch: 7/100...  Training Step: 228...  Training loss: 2.3454...  0.4947 sec/batch\n",
      "Epoch: 7/100...  Training Step: 229...  Training loss: 2.2962...  0.4938 sec/batch\n",
      "Epoch: 7/100...  Training Step: 230...  Training loss: 2.3283...  0.4933 sec/batch\n",
      "Epoch: 7/100...  Training Step: 231...  Training loss: 2.3181...  0.4939 sec/batch\n",
      "Epoch: 7/100...  Training Step: 232...  Training loss: 2.3068...  0.4942 sec/batch\n",
      "Epoch: 7/100...  Training Step: 233...  Training loss: 2.3159...  0.4945 sec/batch\n",
      "Epoch: 7/100...  Training Step: 234...  Training loss: 2.3479...  0.4935 sec/batch\n",
      "Epoch: 7/100...  Training Step: 235...  Training loss: 2.3188...  0.4936 sec/batch\n",
      "Epoch: 7/100...  Training Step: 236...  Training loss: 2.3230...  0.4932 sec/batch\n",
      "Epoch: 7/100...  Training Step: 237...  Training loss: 2.3224...  0.4938 sec/batch\n",
      "Epoch: 7/100...  Training Step: 238...  Training loss: 2.3042...  0.4932 sec/batch\n",
      "Epoch: 8/100...  Training Step: 239...  Training loss: 2.3857...  0.4935 sec/batch\n",
      "Epoch: 8/100...  Training Step: 240...  Training loss: 2.2935...  0.4966 sec/batch\n",
      "Epoch: 8/100...  Training Step: 241...  Training loss: 2.2964...  0.4942 sec/batch\n",
      "Epoch: 8/100...  Training Step: 242...  Training loss: 2.3190...  0.4940 sec/batch\n",
      "Epoch: 8/100...  Training Step: 243...  Training loss: 2.3168...  0.4933 sec/batch\n",
      "Epoch: 8/100...  Training Step: 244...  Training loss: 2.2754...  0.4934 sec/batch\n",
      "Epoch: 8/100...  Training Step: 245...  Training loss: 2.2979...  0.4931 sec/batch\n",
      "Epoch: 8/100...  Training Step: 246...  Training loss: 2.2753...  0.4929 sec/batch\n",
      "Epoch: 8/100...  Training Step: 247...  Training loss: 2.2709...  0.4933 sec/batch\n",
      "Epoch: 8/100...  Training Step: 248...  Training loss: 2.2856...  0.4935 sec/batch\n",
      "Epoch: 8/100...  Training Step: 249...  Training loss: 2.2951...  0.4935 sec/batch\n",
      "Epoch: 8/100...  Training Step: 250...  Training loss: 2.2905...  0.4943 sec/batch\n",
      "Epoch: 8/100...  Training Step: 251...  Training loss: 2.2928...  0.4928 sec/batch\n",
      "Epoch: 8/100...  Training Step: 252...  Training loss: 2.2483...  0.4961 sec/batch\n",
      "Epoch: 8/100...  Training Step: 253...  Training loss: 2.2522...  0.4935 sec/batch\n",
      "Epoch: 8/100...  Training Step: 254...  Training loss: 2.2834...  0.4949 sec/batch\n",
      "Epoch: 8/100...  Training Step: 255...  Training loss: 2.2621...  0.4945 sec/batch\n",
      "Epoch: 8/100...  Training Step: 256...  Training loss: 2.2812...  0.4935 sec/batch\n",
      "Epoch: 8/100...  Training Step: 257...  Training loss: 2.2633...  0.4929 sec/batch\n",
      "Epoch: 8/100...  Training Step: 258...  Training loss: 2.2380...  0.4947 sec/batch\n",
      "Epoch: 8/100...  Training Step: 259...  Training loss: 2.2476...  0.4935 sec/batch\n",
      "Epoch: 8/100...  Training Step: 260...  Training loss: 2.2272...  0.4933 sec/batch\n",
      "Epoch: 8/100...  Training Step: 261...  Training loss: 2.2337...  0.4939 sec/batch\n",
      "Epoch: 8/100...  Training Step: 262...  Training loss: 2.2606...  0.4940 sec/batch\n",
      "Epoch: 8/100...  Training Step: 263...  Training loss: 2.2155...  0.4953 sec/batch\n",
      "Epoch: 8/100...  Training Step: 264...  Training loss: 2.2261...  0.4945 sec/batch\n",
      "Epoch: 8/100...  Training Step: 265...  Training loss: 2.2402...  0.4937 sec/batch\n",
      "Epoch: 8/100...  Training Step: 266...  Training loss: 2.2144...  0.4937 sec/batch\n",
      "Epoch: 8/100...  Training Step: 267...  Training loss: 2.2343...  0.4935 sec/batch\n",
      "Epoch: 8/100...  Training Step: 268...  Training loss: 2.2606...  0.4937 sec/batch\n",
      "Epoch: 8/100...  Training Step: 269...  Training loss: 2.2444...  0.4931 sec/batch\n",
      "Epoch: 8/100...  Training Step: 270...  Training loss: 2.2368...  0.4930 sec/batch\n",
      "Epoch: 8/100...  Training Step: 271...  Training loss: 2.2574...  0.4934 sec/batch\n",
      "Epoch: 8/100...  Training Step: 272...  Training loss: 2.2191...  0.4941 sec/batch\n",
      "Epoch: 9/100...  Training Step: 273...  Training loss: 2.3167...  0.4939 sec/batch\n",
      "Epoch: 9/100...  Training Step: 274...  Training loss: 2.2214...  0.4944 sec/batch\n",
      "Epoch: 9/100...  Training Step: 275...  Training loss: 2.2101...  0.4946 sec/batch\n",
      "Epoch: 9/100...  Training Step: 276...  Training loss: 2.2455...  0.4934 sec/batch\n",
      "Epoch: 9/100...  Training Step: 277...  Training loss: 2.2393...  0.4932 sec/batch\n",
      "Epoch: 9/100...  Training Step: 278...  Training loss: 2.1959...  0.4938 sec/batch\n",
      "Epoch: 9/100...  Training Step: 279...  Training loss: 2.2219...  0.4940 sec/batch\n",
      "Epoch: 9/100...  Training Step: 280...  Training loss: 2.1872...  0.4941 sec/batch\n",
      "Epoch: 9/100...  Training Step: 281...  Training loss: 2.1933...  0.4936 sec/batch\n",
      "Epoch: 9/100...  Training Step: 282...  Training loss: 2.2139...  0.4933 sec/batch\n",
      "Epoch: 9/100...  Training Step: 283...  Training loss: 2.2270...  0.4938 sec/batch\n",
      "Epoch: 9/100...  Training Step: 284...  Training loss: 2.2278...  0.4948 sec/batch\n",
      "Epoch: 9/100...  Training Step: 285...  Training loss: 2.2187...  0.4949 sec/batch\n",
      "Epoch: 9/100...  Training Step: 286...  Training loss: 2.1797...  0.4938 sec/batch\n",
      "Epoch: 9/100...  Training Step: 287...  Training loss: 2.1783...  0.4943 sec/batch\n",
      "Epoch: 9/100...  Training Step: 288...  Training loss: 2.2070...  0.4939 sec/batch\n",
      "Epoch: 9/100...  Training Step: 289...  Training loss: 2.1917...  0.4945 sec/batch\n",
      "Epoch: 9/100...  Training Step: 290...  Training loss: 2.1963...  0.4936 sec/batch\n",
      "Epoch: 9/100...  Training Step: 291...  Training loss: 2.1846...  0.4940 sec/batch\n",
      "Epoch: 9/100...  Training Step: 292...  Training loss: 2.1732...  0.4942 sec/batch\n",
      "Epoch: 9/100...  Training Step: 293...  Training loss: 2.1737...  0.4928 sec/batch\n",
      "Epoch: 9/100...  Training Step: 294...  Training loss: 2.1453...  0.4931 sec/batch\n",
      "Epoch: 9/100...  Training Step: 295...  Training loss: 2.1621...  0.4935 sec/batch\n",
      "Epoch: 9/100...  Training Step: 296...  Training loss: 2.1783...  0.4940 sec/batch\n",
      "Epoch: 9/100...  Training Step: 297...  Training loss: 2.1368...  0.4937 sec/batch\n",
      "Epoch: 9/100...  Training Step: 298...  Training loss: 2.1666...  0.4938 sec/batch\n",
      "Epoch: 9/100...  Training Step: 299...  Training loss: 2.1548...  0.4939 sec/batch\n",
      "Epoch: 9/100...  Training Step: 300...  Training loss: 2.1511...  0.4937 sec/batch\n",
      "Epoch: 9/100...  Training Step: 301...  Training loss: 2.1713...  0.4961 sec/batch\n",
      "Epoch: 9/100...  Training Step: 302...  Training loss: 2.1965...  0.4938 sec/batch\n",
      "Epoch: 9/100...  Training Step: 303...  Training loss: 2.1769...  0.4937 sec/batch\n",
      "Epoch: 9/100...  Training Step: 304...  Training loss: 2.1778...  0.4946 sec/batch\n",
      "Epoch: 9/100...  Training Step: 305...  Training loss: 2.1940...  0.4935 sec/batch\n",
      "Epoch: 9/100...  Training Step: 306...  Training loss: 2.1504...  0.4943 sec/batch\n",
      "Epoch: 10/100...  Training Step: 307...  Training loss: 2.2547...  0.4930 sec/batch\n",
      "Epoch: 10/100...  Training Step: 308...  Training loss: 2.1474...  0.4934 sec/batch\n",
      "Epoch: 10/100...  Training Step: 309...  Training loss: 2.1518...  0.4942 sec/batch\n",
      "Epoch: 10/100...  Training Step: 310...  Training loss: 2.1610...  0.4938 sec/batch\n",
      "Epoch: 10/100...  Training Step: 311...  Training loss: 2.1752...  0.4936 sec/batch\n",
      "Epoch: 10/100...  Training Step: 312...  Training loss: 2.1247...  0.4938 sec/batch\n",
      "Epoch: 10/100...  Training Step: 313...  Training loss: 2.1428...  0.4931 sec/batch\n",
      "Epoch: 10/100...  Training Step: 314...  Training loss: 2.1259...  0.4935 sec/batch\n",
      "Epoch: 10/100...  Training Step: 315...  Training loss: 2.1199...  0.4941 sec/batch\n",
      "Epoch: 10/100...  Training Step: 316...  Training loss: 2.1352...  0.4945 sec/batch\n",
      "Epoch: 10/100...  Training Step: 317...  Training loss: 2.1619...  0.4936 sec/batch\n",
      "Epoch: 10/100...  Training Step: 318...  Training loss: 2.1556...  0.4935 sec/batch\n",
      "Epoch: 10/100...  Training Step: 319...  Training loss: 2.1528...  0.4945 sec/batch\n",
      "Epoch: 10/100...  Training Step: 320...  Training loss: 2.1220...  0.4939 sec/batch\n",
      "Epoch: 10/100...  Training Step: 321...  Training loss: 2.1083...  0.4944 sec/batch\n",
      "Epoch: 10/100...  Training Step: 322...  Training loss: 2.1472...  0.4937 sec/batch\n",
      "Epoch: 10/100...  Training Step: 323...  Training loss: 2.1214...  0.4941 sec/batch\n",
      "Epoch: 10/100...  Training Step: 324...  Training loss: 2.1423...  0.4933 sec/batch\n",
      "Epoch: 10/100...  Training Step: 325...  Training loss: 2.1191...  0.4961 sec/batch\n",
      "Epoch: 10/100...  Training Step: 326...  Training loss: 2.1010...  0.4936 sec/batch\n",
      "Epoch: 10/100...  Training Step: 327...  Training loss: 2.1158...  0.4945 sec/batch\n",
      "Epoch: 10/100...  Training Step: 328...  Training loss: 2.0882...  0.4940 sec/batch\n",
      "Epoch: 10/100...  Training Step: 329...  Training loss: 2.1026...  0.4942 sec/batch\n",
      "Epoch: 10/100...  Training Step: 330...  Training loss: 2.1291...  0.4938 sec/batch\n",
      "Epoch: 10/100...  Training Step: 331...  Training loss: 2.0848...  0.4939 sec/batch\n",
      "Epoch: 10/100...  Training Step: 332...  Training loss: 2.1004...  0.4936 sec/batch\n",
      "Epoch: 10/100...  Training Step: 333...  Training loss: 2.1025...  0.4938 sec/batch\n",
      "Epoch: 10/100...  Training Step: 334...  Training loss: 2.0793...  0.4939 sec/batch\n",
      "Epoch: 10/100...  Training Step: 335...  Training loss: 2.1152...  0.4931 sec/batch\n",
      "Epoch: 10/100...  Training Step: 336...  Training loss: 2.1302...  0.4940 sec/batch\n",
      "Epoch: 10/100...  Training Step: 337...  Training loss: 2.1208...  0.4934 sec/batch\n",
      "Epoch: 10/100...  Training Step: 338...  Training loss: 2.1119...  0.4942 sec/batch\n",
      "Epoch: 10/100...  Training Step: 339...  Training loss: 2.1379...  0.4945 sec/batch\n",
      "Epoch: 10/100...  Training Step: 340...  Training loss: 2.0838...  0.4933 sec/batch\n",
      "Epoch: 11/100...  Training Step: 341...  Training loss: 2.1835...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 342...  Training loss: 2.0922...  0.4939 sec/batch\n",
      "Epoch: 11/100...  Training Step: 343...  Training loss: 2.0868...  0.4932 sec/batch\n",
      "Epoch: 11/100...  Training Step: 344...  Training loss: 2.1108...  0.4946 sec/batch\n",
      "Epoch: 11/100...  Training Step: 345...  Training loss: 2.1160...  0.4938 sec/batch\n",
      "Epoch: 11/100...  Training Step: 346...  Training loss: 2.0666...  0.4944 sec/batch\n",
      "Epoch: 11/100...  Training Step: 347...  Training loss: 2.0955...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 348...  Training loss: 2.0569...  0.4938 sec/batch\n",
      "Epoch: 11/100...  Training Step: 349...  Training loss: 2.0572...  0.4953 sec/batch\n",
      "Epoch: 11/100...  Training Step: 350...  Training loss: 2.0784...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 351...  Training loss: 2.1003...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 352...  Training loss: 2.0960...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 353...  Training loss: 2.0933...  0.4946 sec/batch\n",
      "Epoch: 11/100...  Training Step: 354...  Training loss: 2.0654...  0.4937 sec/batch\n",
      "Epoch: 11/100...  Training Step: 355...  Training loss: 2.0566...  0.4950 sec/batch\n",
      "Epoch: 11/100...  Training Step: 356...  Training loss: 2.0829...  0.4934 sec/batch\n",
      "Epoch: 11/100...  Training Step: 357...  Training loss: 2.0695...  0.4933 sec/batch\n",
      "Epoch: 11/100...  Training Step: 358...  Training loss: 2.0724...  0.4930 sec/batch\n",
      "Epoch: 11/100...  Training Step: 359...  Training loss: 2.0671...  0.4940 sec/batch\n",
      "Epoch: 11/100...  Training Step: 360...  Training loss: 2.0479...  0.4939 sec/batch\n",
      "Epoch: 11/100...  Training Step: 361...  Training loss: 2.0584...  0.4930 sec/batch\n",
      "Epoch: 11/100...  Training Step: 362...  Training loss: 2.0297...  0.4934 sec/batch\n",
      "Epoch: 11/100...  Training Step: 363...  Training loss: 2.0480...  0.4934 sec/batch\n",
      "Epoch: 11/100...  Training Step: 364...  Training loss: 2.0623...  0.4937 sec/batch\n",
      "Epoch: 11/100...  Training Step: 365...  Training loss: 2.0259...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 366...  Training loss: 2.0539...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 367...  Training loss: 2.0486...  0.4934 sec/batch\n",
      "Epoch: 11/100...  Training Step: 368...  Training loss: 2.0330...  0.4950 sec/batch\n",
      "Epoch: 11/100...  Training Step: 369...  Training loss: 2.0642...  0.4952 sec/batch\n",
      "Epoch: 11/100...  Training Step: 370...  Training loss: 2.0746...  0.4947 sec/batch\n",
      "Epoch: 11/100...  Training Step: 371...  Training loss: 2.0609...  0.4929 sec/batch\n",
      "Epoch: 11/100...  Training Step: 372...  Training loss: 2.0554...  0.4935 sec/batch\n",
      "Epoch: 11/100...  Training Step: 373...  Training loss: 2.0819...  0.4941 sec/batch\n",
      "Epoch: 11/100...  Training Step: 374...  Training loss: 2.0295...  0.4937 sec/batch\n",
      "Epoch: 12/100...  Training Step: 375...  Training loss: 2.1275...  0.4937 sec/batch\n",
      "Epoch: 12/100...  Training Step: 376...  Training loss: 2.0342...  0.4949 sec/batch\n",
      "Epoch: 12/100...  Training Step: 377...  Training loss: 2.0341...  0.4938 sec/batch\n",
      "Epoch: 12/100...  Training Step: 378...  Training loss: 2.0498...  0.4936 sec/batch\n",
      "Epoch: 12/100...  Training Step: 379...  Training loss: 2.0606...  0.4939 sec/batch\n",
      "Epoch: 12/100...  Training Step: 380...  Training loss: 2.0163...  0.4931 sec/batch\n",
      "Epoch: 12/100...  Training Step: 381...  Training loss: 2.0347...  0.4935 sec/batch\n",
      "Epoch: 12/100...  Training Step: 382...  Training loss: 2.0143...  0.4941 sec/batch\n",
      "Epoch: 12/100...  Training Step: 383...  Training loss: 2.0010...  0.4940 sec/batch\n",
      "Epoch: 12/100...  Training Step: 384...  Training loss: 2.0311...  0.4927 sec/batch\n",
      "Epoch: 12/100...  Training Step: 385...  Training loss: 2.0408...  0.4936 sec/batch\n",
      "Epoch: 12/100...  Training Step: 386...  Training loss: 2.0498...  0.4938 sec/batch\n",
      "Epoch: 12/100...  Training Step: 387...  Training loss: 2.0459...  0.4929 sec/batch\n",
      "Epoch: 12/100...  Training Step: 388...  Training loss: 2.0143...  0.4934 sec/batch\n",
      "Epoch: 12/100...  Training Step: 389...  Training loss: 1.9933...  0.4944 sec/batch\n",
      "Epoch: 12/100...  Training Step: 390...  Training loss: 2.0230...  0.4934 sec/batch\n",
      "Epoch: 12/100...  Training Step: 391...  Training loss: 2.0314...  0.4942 sec/batch\n",
      "Epoch: 12/100...  Training Step: 392...  Training loss: 2.0125...  0.4950 sec/batch\n",
      "Epoch: 12/100...  Training Step: 393...  Training loss: 2.0177...  0.4960 sec/batch\n",
      "Epoch: 12/100...  Training Step: 394...  Training loss: 1.9978...  0.4938 sec/batch\n",
      "Epoch: 12/100...  Training Step: 395...  Training loss: 2.0026...  0.4939 sec/batch\n",
      "Epoch: 12/100...  Training Step: 396...  Training loss: 1.9670...  0.4936 sec/batch\n",
      "Epoch: 12/100...  Training Step: 397...  Training loss: 1.9970...  0.4931 sec/batch\n",
      "Epoch: 12/100...  Training Step: 398...  Training loss: 2.0086...  0.4933 sec/batch\n",
      "Epoch: 12/100...  Training Step: 399...  Training loss: 1.9746...  0.4934 sec/batch\n",
      "Epoch: 12/100...  Training Step: 400...  Training loss: 1.9897...  0.4939 sec/batch\n",
      "Epoch: 12/100...  Training Step: 401...  Training loss: 1.9850...  0.4938 sec/batch\n",
      "Epoch: 12/100...  Training Step: 402...  Training loss: 1.9835...  0.4936 sec/batch\n",
      "Epoch: 12/100...  Training Step: 403...  Training loss: 2.0117...  0.4937 sec/batch\n",
      "Epoch: 12/100...  Training Step: 404...  Training loss: 2.0253...  0.4941 sec/batch\n",
      "Epoch: 12/100...  Training Step: 405...  Training loss: 2.0162...  0.4934 sec/batch\n",
      "Epoch: 12/100...  Training Step: 406...  Training loss: 2.0136...  0.4945 sec/batch\n",
      "Epoch: 12/100...  Training Step: 407...  Training loss: 2.0376...  0.4942 sec/batch\n",
      "Epoch: 12/100...  Training Step: 408...  Training loss: 1.9801...  0.4943 sec/batch\n",
      "Epoch: 13/100...  Training Step: 409...  Training loss: 2.0857...  0.4932 sec/batch\n",
      "Epoch: 13/100...  Training Step: 410...  Training loss: 1.9886...  0.4930 sec/batch\n",
      "Epoch: 13/100...  Training Step: 411...  Training loss: 1.9883...  0.4934 sec/batch\n",
      "Epoch: 13/100...  Training Step: 412...  Training loss: 2.0115...  0.4937 sec/batch\n",
      "Epoch: 13/100...  Training Step: 413...  Training loss: 2.0190...  0.4933 sec/batch\n",
      "Epoch: 13/100...  Training Step: 414...  Training loss: 1.9695...  0.4936 sec/batch\n",
      "Epoch: 13/100...  Training Step: 415...  Training loss: 1.9873...  0.4935 sec/batch\n",
      "Epoch: 13/100...  Training Step: 416...  Training loss: 1.9574...  0.4940 sec/batch\n",
      "Epoch: 13/100...  Training Step: 417...  Training loss: 1.9608...  0.4937 sec/batch\n",
      "Epoch: 13/100...  Training Step: 418...  Training loss: 1.9914...  0.4938 sec/batch\n",
      "Epoch: 13/100...  Training Step: 419...  Training loss: 1.9968...  0.4936 sec/batch\n",
      "Epoch: 13/100...  Training Step: 420...  Training loss: 2.0023...  0.4935 sec/batch\n",
      "Epoch: 13/100...  Training Step: 421...  Training loss: 1.9926...  0.4937 sec/batch\n",
      "Epoch: 13/100...  Training Step: 422...  Training loss: 1.9698...  0.4949 sec/batch\n",
      "Epoch: 13/100...  Training Step: 423...  Training loss: 1.9453...  0.4937 sec/batch\n",
      "Epoch: 13/100...  Training Step: 424...  Training loss: 1.9831...  0.4939 sec/batch\n",
      "Epoch: 13/100...  Training Step: 425...  Training loss: 1.9792...  0.4953 sec/batch\n",
      "Epoch: 13/100...  Training Step: 426...  Training loss: 1.9616...  0.4940 sec/batch\n",
      "Epoch: 13/100...  Training Step: 427...  Training loss: 1.9676...  0.4934 sec/batch\n",
      "Epoch: 13/100...  Training Step: 428...  Training loss: 1.9374...  0.4938 sec/batch\n",
      "Epoch: 13/100...  Training Step: 429...  Training loss: 1.9624...  0.4954 sec/batch\n",
      "Epoch: 13/100...  Training Step: 430...  Training loss: 1.9173...  0.4936 sec/batch\n",
      "Epoch: 13/100...  Training Step: 431...  Training loss: 1.9507...  0.4936 sec/batch\n",
      "Epoch: 13/100...  Training Step: 432...  Training loss: 1.9592...  0.4934 sec/batch\n",
      "Epoch: 13/100...  Training Step: 433...  Training loss: 1.9249...  0.4929 sec/batch\n",
      "Epoch: 13/100...  Training Step: 434...  Training loss: 1.9456...  0.4944 sec/batch\n",
      "Epoch: 13/100...  Training Step: 435...  Training loss: 1.9440...  0.4936 sec/batch\n",
      "Epoch: 13/100...  Training Step: 436...  Training loss: 1.9319...  0.4942 sec/batch\n",
      "Epoch: 13/100...  Training Step: 437...  Training loss: 1.9703...  0.4939 sec/batch\n",
      "Epoch: 13/100...  Training Step: 438...  Training loss: 1.9789...  0.4935 sec/batch\n",
      "Epoch: 13/100...  Training Step: 439...  Training loss: 1.9668...  0.4938 sec/batch\n",
      "Epoch: 13/100...  Training Step: 440...  Training loss: 1.9654...  0.4936 sec/batch\n",
      "Epoch: 13/100...  Training Step: 441...  Training loss: 1.9890...  0.4938 sec/batch\n",
      "Epoch: 13/100...  Training Step: 442...  Training loss: 1.9303...  0.4944 sec/batch\n",
      "Epoch: 14/100...  Training Step: 443...  Training loss: 2.0250...  0.4931 sec/batch\n",
      "Epoch: 14/100...  Training Step: 444...  Training loss: 1.9377...  0.4933 sec/batch\n",
      "Epoch: 14/100...  Training Step: 445...  Training loss: 1.9408...  0.4933 sec/batch\n",
      "Epoch: 14/100...  Training Step: 446...  Training loss: 1.9590...  0.4938 sec/batch\n",
      "Epoch: 14/100...  Training Step: 447...  Training loss: 1.9615...  0.4945 sec/batch\n",
      "Epoch: 14/100...  Training Step: 448...  Training loss: 1.9208...  0.4946 sec/batch\n",
      "Epoch: 14/100...  Training Step: 449...  Training loss: 1.9460...  0.4941 sec/batch\n",
      "Epoch: 14/100...  Training Step: 450...  Training loss: 1.9189...  0.4928 sec/batch\n",
      "Epoch: 14/100...  Training Step: 451...  Training loss: 1.9106...  0.4929 sec/batch\n",
      "Epoch: 14/100...  Training Step: 452...  Training loss: 1.9421...  0.4947 sec/batch\n",
      "Epoch: 14/100...  Training Step: 453...  Training loss: 1.9509...  0.4945 sec/batch\n",
      "Epoch: 14/100...  Training Step: 454...  Training loss: 1.9579...  0.4953 sec/batch\n",
      "Epoch: 14/100...  Training Step: 455...  Training loss: 1.9521...  0.4926 sec/batch\n",
      "Epoch: 14/100...  Training Step: 456...  Training loss: 1.9246...  0.4940 sec/batch\n",
      "Epoch: 14/100...  Training Step: 457...  Training loss: 1.9067...  0.4937 sec/batch\n",
      "Epoch: 14/100...  Training Step: 458...  Training loss: 1.9362...  0.4946 sec/batch\n",
      "Epoch: 14/100...  Training Step: 459...  Training loss: 1.9407...  0.4938 sec/batch\n",
      "Epoch: 14/100...  Training Step: 460...  Training loss: 1.9132...  0.4951 sec/batch\n",
      "Epoch: 14/100...  Training Step: 461...  Training loss: 1.9307...  0.4927 sec/batch\n",
      "Epoch: 14/100...  Training Step: 462...  Training loss: 1.9112...  0.4942 sec/batch\n",
      "Epoch: 14/100...  Training Step: 463...  Training loss: 1.9171...  0.4926 sec/batch\n",
      "Epoch: 14/100...  Training Step: 464...  Training loss: 1.8872...  0.4932 sec/batch\n",
      "Epoch: 14/100...  Training Step: 465...  Training loss: 1.9078...  0.4937 sec/batch\n",
      "Epoch: 14/100...  Training Step: 466...  Training loss: 1.9272...  0.4932 sec/batch\n",
      "Epoch: 14/100...  Training Step: 467...  Training loss: 1.8859...  0.4935 sec/batch\n",
      "Epoch: 14/100...  Training Step: 468...  Training loss: 1.8921...  0.4932 sec/batch\n",
      "Epoch: 14/100...  Training Step: 469...  Training loss: 1.8944...  0.4933 sec/batch\n",
      "Epoch: 14/100...  Training Step: 470...  Training loss: 1.8929...  0.4940 sec/batch\n",
      "Epoch: 14/100...  Training Step: 471...  Training loss: 1.9339...  0.4943 sec/batch\n",
      "Epoch: 14/100...  Training Step: 472...  Training loss: 1.9439...  0.4937 sec/batch\n",
      "Epoch: 14/100...  Training Step: 473...  Training loss: 1.9313...  0.4931 sec/batch\n",
      "Epoch: 14/100...  Training Step: 474...  Training loss: 1.9323...  0.4945 sec/batch\n",
      "Epoch: 14/100...  Training Step: 475...  Training loss: 1.9450...  0.4945 sec/batch\n",
      "Epoch: 14/100...  Training Step: 476...  Training loss: 1.8897...  0.4937 sec/batch\n",
      "Epoch: 15/100...  Training Step: 477...  Training loss: 1.9946...  0.4930 sec/batch\n",
      "Epoch: 15/100...  Training Step: 478...  Training loss: 1.9002...  0.4937 sec/batch\n",
      "Epoch: 15/100...  Training Step: 479...  Training loss: 1.8997...  0.4938 sec/batch\n",
      "Epoch: 15/100...  Training Step: 480...  Training loss: 1.9313...  0.4933 sec/batch\n",
      "Epoch: 15/100...  Training Step: 481...  Training loss: 1.9296...  0.4938 sec/batch\n",
      "Epoch: 15/100...  Training Step: 482...  Training loss: 1.8858...  0.4933 sec/batch\n",
      "Epoch: 15/100...  Training Step: 483...  Training loss: 1.9067...  0.4937 sec/batch\n",
      "Epoch: 15/100...  Training Step: 484...  Training loss: 1.8741...  0.4931 sec/batch\n",
      "Epoch: 15/100...  Training Step: 485...  Training loss: 1.8784...  0.4926 sec/batch\n",
      "Epoch: 15/100...  Training Step: 486...  Training loss: 1.9067...  0.4938 sec/batch\n",
      "Epoch: 15/100...  Training Step: 487...  Training loss: 1.9064...  0.4929 sec/batch\n",
      "Epoch: 15/100...  Training Step: 488...  Training loss: 1.9143...  0.4943 sec/batch\n",
      "Epoch: 15/100...  Training Step: 489...  Training loss: 1.9108...  0.4935 sec/batch\n",
      "Epoch: 15/100...  Training Step: 490...  Training loss: 1.8851...  0.4941 sec/batch\n",
      "Epoch: 15/100...  Training Step: 491...  Training loss: 1.8653...  0.4932 sec/batch\n",
      "Epoch: 15/100...  Training Step: 492...  Training loss: 1.9043...  0.4931 sec/batch\n",
      "Epoch: 15/100...  Training Step: 493...  Training loss: 1.8972...  0.4938 sec/batch\n",
      "Epoch: 15/100...  Training Step: 494...  Training loss: 1.8750...  0.4930 sec/batch\n",
      "Epoch: 15/100...  Training Step: 495...  Training loss: 1.8874...  0.4933 sec/batch\n",
      "Epoch: 15/100...  Training Step: 496...  Training loss: 1.8647...  0.4928 sec/batch\n",
      "Epoch: 15/100...  Training Step: 497...  Training loss: 1.8805...  0.4930 sec/batch\n",
      "Epoch: 15/100...  Training Step: 498...  Training loss: 1.8421...  0.4932 sec/batch\n",
      "Epoch: 15/100...  Training Step: 499...  Training loss: 1.8666...  0.4935 sec/batch\n",
      "Epoch: 15/100...  Training Step: 500...  Training loss: 1.8816...  0.4944 sec/batch\n",
      "Epoch: 15/100...  Training Step: 501...  Training loss: 1.8463...  0.4954 sec/batch\n",
      "Epoch: 15/100...  Training Step: 502...  Training loss: 1.8644...  0.4938 sec/batch\n",
      "Epoch: 15/100...  Training Step: 503...  Training loss: 1.8581...  0.4941 sec/batch\n",
      "Epoch: 15/100...  Training Step: 504...  Training loss: 1.8443...  0.4927 sec/batch\n",
      "Epoch: 15/100...  Training Step: 505...  Training loss: 1.8891...  0.4923 sec/batch\n",
      "Epoch: 15/100...  Training Step: 506...  Training loss: 1.8985...  0.4930 sec/batch\n",
      "Epoch: 15/100...  Training Step: 507...  Training loss: 1.8852...  0.4941 sec/batch\n",
      "Epoch: 15/100...  Training Step: 508...  Training loss: 1.8854...  0.4931 sec/batch\n",
      "Epoch: 15/100...  Training Step: 509...  Training loss: 1.9121...  0.4945 sec/batch\n",
      "Epoch: 15/100...  Training Step: 510...  Training loss: 1.8560...  0.4935 sec/batch\n",
      "Epoch: 16/100...  Training Step: 511...  Training loss: 1.9576...  0.4944 sec/batch\n",
      "Epoch: 16/100...  Training Step: 512...  Training loss: 1.8708...  0.4944 sec/batch\n",
      "Epoch: 16/100...  Training Step: 513...  Training loss: 1.8579...  0.4945 sec/batch\n",
      "Epoch: 16/100...  Training Step: 514...  Training loss: 1.8879...  0.4934 sec/batch\n",
      "Epoch: 16/100...  Training Step: 515...  Training loss: 1.8836...  0.4932 sec/batch\n",
      "Epoch: 16/100...  Training Step: 516...  Training loss: 1.8523...  0.4937 sec/batch\n",
      "Epoch: 16/100...  Training Step: 517...  Training loss: 1.8632...  0.4949 sec/batch\n",
      "Epoch: 16/100...  Training Step: 518...  Training loss: 1.8354...  0.4934 sec/batch\n",
      "Epoch: 16/100...  Training Step: 519...  Training loss: 1.8338...  0.4935 sec/batch\n",
      "Epoch: 16/100...  Training Step: 520...  Training loss: 1.8754...  0.4931 sec/batch\n",
      "Epoch: 16/100...  Training Step: 521...  Training loss: 1.8607...  0.4937 sec/batch\n",
      "Epoch: 16/100...  Training Step: 522...  Training loss: 1.8756...  0.4937 sec/batch\n",
      "Epoch: 16/100...  Training Step: 523...  Training loss: 1.8725...  0.4934 sec/batch\n",
      "Epoch: 16/100...  Training Step: 524...  Training loss: 1.8524...  0.4934 sec/batch\n",
      "Epoch: 16/100...  Training Step: 525...  Training loss: 1.8281...  0.4929 sec/batch\n",
      "Epoch: 16/100...  Training Step: 526...  Training loss: 1.8645...  0.4927 sec/batch\n",
      "Epoch: 16/100...  Training Step: 527...  Training loss: 1.8628...  0.4957 sec/batch\n",
      "Epoch: 16/100...  Training Step: 528...  Training loss: 1.8402...  0.4947 sec/batch\n",
      "Epoch: 16/100...  Training Step: 529...  Training loss: 1.8521...  0.4932 sec/batch\n",
      "Epoch: 16/100...  Training Step: 530...  Training loss: 1.8278...  0.4930 sec/batch\n",
      "Epoch: 16/100...  Training Step: 531...  Training loss: 1.8474...  0.4940 sec/batch\n",
      "Epoch: 16/100...  Training Step: 532...  Training loss: 1.7993...  0.4936 sec/batch\n",
      "Epoch: 16/100...  Training Step: 533...  Training loss: 1.8429...  0.4942 sec/batch\n",
      "Epoch: 16/100...  Training Step: 534...  Training loss: 1.8443...  0.4934 sec/batch\n",
      "Epoch: 16/100...  Training Step: 535...  Training loss: 1.8185...  0.4939 sec/batch\n",
      "Epoch: 16/100...  Training Step: 536...  Training loss: 1.8265...  0.4936 sec/batch\n",
      "Epoch: 16/100...  Training Step: 537...  Training loss: 1.8249...  0.4929 sec/batch\n",
      "Epoch: 16/100...  Training Step: 538...  Training loss: 1.8197...  0.4938 sec/batch\n",
      "Epoch: 16/100...  Training Step: 539...  Training loss: 1.8636...  0.4935 sec/batch\n",
      "Epoch: 16/100...  Training Step: 540...  Training loss: 1.8752...  0.4937 sec/batch\n",
      "Epoch: 16/100...  Training Step: 541...  Training loss: 1.8559...  0.4932 sec/batch\n",
      "Epoch: 16/100...  Training Step: 542...  Training loss: 1.8503...  0.4948 sec/batch\n",
      "Epoch: 16/100...  Training Step: 543...  Training loss: 1.8739...  0.4936 sec/batch\n",
      "Epoch: 16/100...  Training Step: 544...  Training loss: 1.8175...  0.4937 sec/batch\n",
      "Epoch: 17/100...  Training Step: 545...  Training loss: 1.9187...  0.4931 sec/batch\n",
      "Epoch: 17/100...  Training Step: 546...  Training loss: 1.8316...  0.4939 sec/batch\n",
      "Epoch: 17/100...  Training Step: 547...  Training loss: 1.8264...  0.4931 sec/batch\n",
      "Epoch: 17/100...  Training Step: 548...  Training loss: 1.8469...  0.4949 sec/batch\n",
      "Epoch: 17/100...  Training Step: 549...  Training loss: 1.8413...  0.4937 sec/batch\n",
      "Epoch: 17/100...  Training Step: 550...  Training loss: 1.8079...  0.4934 sec/batch\n",
      "Epoch: 17/100...  Training Step: 551...  Training loss: 1.8322...  0.4933 sec/batch\n",
      "Epoch: 17/100...  Training Step: 552...  Training loss: 1.8046...  0.4936 sec/batch\n",
      "Epoch: 17/100...  Training Step: 553...  Training loss: 1.7997...  0.4932 sec/batch\n",
      "Epoch: 17/100...  Training Step: 554...  Training loss: 1.8267...  0.4923 sec/batch\n",
      "Epoch: 17/100...  Training Step: 555...  Training loss: 1.8352...  0.4937 sec/batch\n",
      "Epoch: 17/100...  Training Step: 556...  Training loss: 1.8428...  0.4941 sec/batch\n",
      "Epoch: 17/100...  Training Step: 557...  Training loss: 1.8410...  0.4938 sec/batch\n",
      "Epoch: 17/100...  Training Step: 558...  Training loss: 1.8209...  0.4934 sec/batch\n",
      "Epoch: 17/100...  Training Step: 559...  Training loss: 1.7899...  0.4951 sec/batch\n",
      "Epoch: 17/100...  Training Step: 560...  Training loss: 1.8254...  0.4936 sec/batch\n",
      "Epoch: 17/100...  Training Step: 561...  Training loss: 1.8271...  0.4934 sec/batch\n",
      "Epoch: 17/100...  Training Step: 562...  Training loss: 1.8028...  0.4938 sec/batch\n",
      "Epoch: 17/100...  Training Step: 563...  Training loss: 1.8200...  0.4933 sec/batch\n",
      "Epoch: 17/100...  Training Step: 564...  Training loss: 1.7977...  0.4953 sec/batch\n",
      "Epoch: 17/100...  Training Step: 565...  Training loss: 1.8142...  0.4938 sec/batch\n",
      "Epoch: 17/100...  Training Step: 566...  Training loss: 1.7659...  0.4937 sec/batch\n",
      "Epoch: 17/100...  Training Step: 567...  Training loss: 1.7982...  0.4934 sec/batch\n",
      "Epoch: 17/100...  Training Step: 568...  Training loss: 1.8005...  0.4942 sec/batch\n",
      "Epoch: 17/100...  Training Step: 569...  Training loss: 1.7701...  0.4940 sec/batch\n",
      "Epoch: 17/100...  Training Step: 570...  Training loss: 1.7782...  0.4935 sec/batch\n",
      "Epoch: 17/100...  Training Step: 571...  Training loss: 1.7750...  0.4933 sec/batch\n",
      "Epoch: 17/100...  Training Step: 572...  Training loss: 1.7762...  0.4936 sec/batch\n",
      "Epoch: 17/100...  Training Step: 573...  Training loss: 1.8227...  0.4937 sec/batch\n",
      "Epoch: 17/100...  Training Step: 574...  Training loss: 1.8282...  0.4945 sec/batch\n",
      "Epoch: 17/100...  Training Step: 575...  Training loss: 1.8185...  0.4936 sec/batch\n",
      "Epoch: 17/100...  Training Step: 576...  Training loss: 1.8219...  0.4928 sec/batch\n",
      "Epoch: 17/100...  Training Step: 577...  Training loss: 1.8462...  0.4947 sec/batch\n",
      "Epoch: 17/100...  Training Step: 578...  Training loss: 1.7775...  0.4940 sec/batch\n",
      "Epoch: 18/100...  Training Step: 579...  Training loss: 1.8894...  0.4933 sec/batch\n",
      "Epoch: 18/100...  Training Step: 580...  Training loss: 1.7912...  0.4932 sec/batch\n",
      "Epoch: 18/100...  Training Step: 581...  Training loss: 1.7964...  0.4942 sec/batch\n",
      "Epoch: 18/100...  Training Step: 582...  Training loss: 1.8156...  0.4938 sec/batch\n",
      "Epoch: 18/100...  Training Step: 583...  Training loss: 1.8087...  0.4935 sec/batch\n",
      "Epoch: 18/100...  Training Step: 584...  Training loss: 1.7839...  0.4937 sec/batch\n",
      "Epoch: 18/100...  Training Step: 585...  Training loss: 1.7973...  0.4935 sec/batch\n",
      "Epoch: 18/100...  Training Step: 586...  Training loss: 1.7677...  0.4936 sec/batch\n",
      "Epoch: 18/100...  Training Step: 587...  Training loss: 1.7644...  0.4946 sec/batch\n",
      "Epoch: 18/100...  Training Step: 588...  Training loss: 1.7977...  0.4940 sec/batch\n",
      "Epoch: 18/100...  Training Step: 589...  Training loss: 1.8056...  0.4935 sec/batch\n",
      "Epoch: 18/100...  Training Step: 590...  Training loss: 1.8063...  0.4936 sec/batch\n",
      "Epoch: 18/100...  Training Step: 591...  Training loss: 1.8145...  0.4940 sec/batch\n",
      "Epoch: 18/100...  Training Step: 592...  Training loss: 1.7767...  0.4937 sec/batch\n",
      "Epoch: 18/100...  Training Step: 593...  Training loss: 1.7577...  0.4941 sec/batch\n",
      "Epoch: 18/100...  Training Step: 594...  Training loss: 1.8034...  0.4935 sec/batch\n",
      "Epoch: 18/100...  Training Step: 595...  Training loss: 1.7961...  0.4934 sec/batch\n",
      "Epoch: 18/100...  Training Step: 596...  Training loss: 1.7729...  0.4941 sec/batch\n",
      "Epoch: 18/100...  Training Step: 597...  Training loss: 1.7891...  0.4937 sec/batch\n",
      "Epoch: 18/100...  Training Step: 598...  Training loss: 1.7610...  0.4931 sec/batch\n",
      "Epoch: 18/100...  Training Step: 599...  Training loss: 1.7806...  0.4942 sec/batch\n",
      "Epoch: 18/100...  Training Step: 600...  Training loss: 1.7387...  0.4934 sec/batch\n",
      "Epoch: 18/100...  Training Step: 601...  Training loss: 1.7683...  0.4937 sec/batch\n",
      "Epoch: 18/100...  Training Step: 602...  Training loss: 1.7733...  0.4942 sec/batch\n",
      "Epoch: 18/100...  Training Step: 603...  Training loss: 1.7328...  0.4941 sec/batch\n",
      "Epoch: 18/100...  Training Step: 604...  Training loss: 1.7612...  0.4941 sec/batch\n",
      "Epoch: 18/100...  Training Step: 605...  Training loss: 1.7497...  0.4944 sec/batch\n",
      "Epoch: 18/100...  Training Step: 606...  Training loss: 1.7572...  0.4930 sec/batch\n",
      "Epoch: 18/100...  Training Step: 607...  Training loss: 1.8037...  0.4941 sec/batch\n",
      "Epoch: 18/100...  Training Step: 608...  Training loss: 1.7847...  0.5011 sec/batch\n",
      "Epoch: 18/100...  Training Step: 609...  Training loss: 1.7954...  0.4930 sec/batch\n",
      "Epoch: 18/100...  Training Step: 610...  Training loss: 1.7930...  0.4946 sec/batch\n",
      "Epoch: 18/100...  Training Step: 611...  Training loss: 1.8067...  0.4942 sec/batch\n",
      "Epoch: 18/100...  Training Step: 612...  Training loss: 1.7498...  0.4934 sec/batch\n",
      "Epoch: 19/100...  Training Step: 613...  Training loss: 1.8562...  0.4929 sec/batch\n",
      "Epoch: 19/100...  Training Step: 614...  Training loss: 1.7626...  0.4922 sec/batch\n",
      "Epoch: 19/100...  Training Step: 615...  Training loss: 1.7618...  0.4933 sec/batch\n",
      "Epoch: 19/100...  Training Step: 616...  Training loss: 1.7865...  0.4936 sec/batch\n",
      "Epoch: 19/100...  Training Step: 617...  Training loss: 1.7764...  0.4934 sec/batch\n",
      "Epoch: 19/100...  Training Step: 618...  Training loss: 1.7498...  0.4945 sec/batch\n",
      "Epoch: 19/100...  Training Step: 619...  Training loss: 1.7626...  0.4935 sec/batch\n",
      "Epoch: 19/100...  Training Step: 620...  Training loss: 1.7332...  0.4932 sec/batch\n",
      "Epoch: 19/100...  Training Step: 621...  Training loss: 1.7332...  0.4942 sec/batch\n",
      "Epoch: 19/100...  Training Step: 622...  Training loss: 1.7703...  0.4936 sec/batch\n",
      "Epoch: 19/100...  Training Step: 623...  Training loss: 1.7746...  0.4938 sec/batch\n",
      "Epoch: 19/100...  Training Step: 624...  Training loss: 1.7745...  0.4937 sec/batch\n",
      "Epoch: 19/100...  Training Step: 625...  Training loss: 1.7773...  0.4932 sec/batch\n",
      "Epoch: 19/100...  Training Step: 626...  Training loss: 1.7558...  0.4938 sec/batch\n",
      "Epoch: 19/100...  Training Step: 627...  Training loss: 1.7233...  0.4935 sec/batch\n",
      "Epoch: 19/100...  Training Step: 628...  Training loss: 1.7769...  0.4938 sec/batch\n",
      "Epoch: 19/100...  Training Step: 629...  Training loss: 1.7743...  0.4940 sec/batch\n",
      "Epoch: 19/100...  Training Step: 630...  Training loss: 1.7434...  0.4943 sec/batch\n",
      "Epoch: 19/100...  Training Step: 631...  Training loss: 1.7566...  0.4945 sec/batch\n",
      "Epoch: 19/100...  Training Step: 632...  Training loss: 1.7292...  0.4935 sec/batch\n",
      "Epoch: 19/100...  Training Step: 633...  Training loss: 1.7543...  0.4930 sec/batch\n",
      "Epoch: 19/100...  Training Step: 634...  Training loss: 1.7096...  0.4938 sec/batch\n",
      "Epoch: 19/100...  Training Step: 635...  Training loss: 1.7296...  0.4927 sec/batch\n",
      "Epoch: 19/100...  Training Step: 636...  Training loss: 1.7355...  0.4931 sec/batch\n",
      "Epoch: 19/100...  Training Step: 637...  Training loss: 1.7125...  0.4941 sec/batch\n",
      "Epoch: 19/100...  Training Step: 638...  Training loss: 1.7207...  0.4934 sec/batch\n",
      "Epoch: 19/100...  Training Step: 639...  Training loss: 1.7193...  0.4940 sec/batch\n",
      "Epoch: 19/100...  Training Step: 640...  Training loss: 1.7189...  0.4934 sec/batch\n",
      "Epoch: 19/100...  Training Step: 641...  Training loss: 1.7663...  0.4945 sec/batch\n",
      "Epoch: 19/100...  Training Step: 642...  Training loss: 1.7645...  0.4929 sec/batch\n",
      "Epoch: 19/100...  Training Step: 643...  Training loss: 1.7482...  0.4935 sec/batch\n",
      "Epoch: 19/100...  Training Step: 644...  Training loss: 1.7641...  0.4944 sec/batch\n",
      "Epoch: 19/100...  Training Step: 645...  Training loss: 1.7761...  0.4941 sec/batch\n",
      "Epoch: 19/100...  Training Step: 646...  Training loss: 1.7126...  0.4937 sec/batch\n",
      "Epoch: 20/100...  Training Step: 647...  Training loss: 1.8202...  0.4932 sec/batch\n",
      "Epoch: 20/100...  Training Step: 648...  Training loss: 1.7297...  0.4941 sec/batch\n",
      "Epoch: 20/100...  Training Step: 649...  Training loss: 1.7366...  0.4931 sec/batch\n",
      "Epoch: 20/100...  Training Step: 650...  Training loss: 1.7597...  0.4935 sec/batch\n",
      "Epoch: 20/100...  Training Step: 651...  Training loss: 1.7493...  0.4936 sec/batch\n",
      "Epoch: 20/100...  Training Step: 652...  Training loss: 1.7238...  0.4937 sec/batch\n",
      "Epoch: 20/100...  Training Step: 653...  Training loss: 1.7385...  0.4941 sec/batch\n",
      "Epoch: 20/100...  Training Step: 654...  Training loss: 1.7087...  0.4933 sec/batch\n",
      "Epoch: 20/100...  Training Step: 655...  Training loss: 1.6989...  0.4939 sec/batch\n",
      "Epoch: 20/100...  Training Step: 656...  Training loss: 1.7484...  0.4939 sec/batch\n",
      "Epoch: 20/100...  Training Step: 657...  Training loss: 1.7387...  0.4954 sec/batch\n",
      "Epoch: 20/100...  Training Step: 658...  Training loss: 1.7546...  0.4941 sec/batch\n",
      "Epoch: 20/100...  Training Step: 659...  Training loss: 1.7420...  0.4936 sec/batch\n",
      "Epoch: 20/100...  Training Step: 660...  Training loss: 1.7333...  0.4945 sec/batch\n",
      "Epoch: 20/100...  Training Step: 661...  Training loss: 1.7013...  0.4939 sec/batch\n",
      "Epoch: 20/100...  Training Step: 662...  Training loss: 1.7433...  0.4932 sec/batch\n",
      "Epoch: 20/100...  Training Step: 663...  Training loss: 1.7470...  0.4934 sec/batch\n",
      "Epoch: 20/100...  Training Step: 664...  Training loss: 1.7130...  0.4933 sec/batch\n",
      "Epoch: 20/100...  Training Step: 665...  Training loss: 1.7245...  0.4933 sec/batch\n",
      "Epoch: 20/100...  Training Step: 666...  Training loss: 1.7134...  0.4929 sec/batch\n",
      "Epoch: 20/100...  Training Step: 667...  Training loss: 1.7124...  0.4931 sec/batch\n",
      "Epoch: 20/100...  Training Step: 668...  Training loss: 1.6857...  0.4930 sec/batch\n",
      "Epoch: 20/100...  Training Step: 669...  Training loss: 1.7051...  0.4938 sec/batch\n",
      "Epoch: 20/100...  Training Step: 670...  Training loss: 1.7113...  0.4935 sec/batch\n",
      "Epoch: 20/100...  Training Step: 671...  Training loss: 1.6768...  0.4933 sec/batch\n",
      "Epoch: 20/100...  Training Step: 672...  Training loss: 1.7052...  0.4938 sec/batch\n",
      "Epoch: 20/100...  Training Step: 673...  Training loss: 1.6966...  0.4935 sec/batch\n",
      "Epoch: 20/100...  Training Step: 674...  Training loss: 1.6909...  0.4939 sec/batch\n",
      "Epoch: 20/100...  Training Step: 675...  Training loss: 1.7363...  0.4949 sec/batch\n",
      "Epoch: 20/100...  Training Step: 676...  Training loss: 1.7320...  0.4937 sec/batch\n",
      "Epoch: 20/100...  Training Step: 677...  Training loss: 1.7248...  0.4935 sec/batch\n",
      "Epoch: 20/100...  Training Step: 678...  Training loss: 1.7315...  0.4940 sec/batch\n",
      "Epoch: 20/100...  Training Step: 679...  Training loss: 1.7529...  0.4937 sec/batch\n",
      "Epoch: 20/100...  Training Step: 680...  Training loss: 1.6925...  0.4938 sec/batch\n",
      "Epoch: 21/100...  Training Step: 681...  Training loss: 1.7947...  0.4927 sec/batch\n",
      "Epoch: 21/100...  Training Step: 682...  Training loss: 1.6965...  0.4939 sec/batch\n",
      "Epoch: 21/100...  Training Step: 683...  Training loss: 1.7049...  0.4939 sec/batch\n",
      "Epoch: 21/100...  Training Step: 684...  Training loss: 1.7402...  0.4939 sec/batch\n",
      "Epoch: 21/100...  Training Step: 685...  Training loss: 1.7212...  0.4933 sec/batch\n",
      "Epoch: 21/100...  Training Step: 686...  Training loss: 1.6887...  0.4932 sec/batch\n",
      "Epoch: 21/100...  Training Step: 687...  Training loss: 1.7101...  0.4957 sec/batch\n",
      "Epoch: 21/100...  Training Step: 688...  Training loss: 1.6780...  0.4938 sec/batch\n",
      "Epoch: 21/100...  Training Step: 689...  Training loss: 1.6737...  0.4937 sec/batch\n",
      "Epoch: 21/100...  Training Step: 690...  Training loss: 1.7079...  0.4947 sec/batch\n",
      "Epoch: 21/100...  Training Step: 691...  Training loss: 1.7132...  0.4937 sec/batch\n",
      "Epoch: 21/100...  Training Step: 692...  Training loss: 1.7254...  0.4932 sec/batch\n",
      "Epoch: 21/100...  Training Step: 693...  Training loss: 1.7156...  0.4945 sec/batch\n",
      "Epoch: 21/100...  Training Step: 694...  Training loss: 1.7091...  0.4935 sec/batch\n",
      "Epoch: 21/100...  Training Step: 695...  Training loss: 1.6789...  0.4934 sec/batch\n",
      "Epoch: 21/100...  Training Step: 696...  Training loss: 1.7100...  0.4942 sec/batch\n",
      "Epoch: 21/100...  Training Step: 697...  Training loss: 1.7258...  0.4934 sec/batch\n",
      "Epoch: 21/100...  Training Step: 698...  Training loss: 1.6849...  0.4937 sec/batch\n",
      "Epoch: 21/100...  Training Step: 699...  Training loss: 1.7019...  0.4934 sec/batch\n",
      "Epoch: 21/100...  Training Step: 700...  Training loss: 1.6773...  0.4940 sec/batch\n",
      "Epoch: 21/100...  Training Step: 701...  Training loss: 1.6986...  0.4938 sec/batch\n",
      "Epoch: 21/100...  Training Step: 702...  Training loss: 1.6622...  0.4936 sec/batch\n",
      "Epoch: 21/100...  Training Step: 703...  Training loss: 1.6771...  0.4934 sec/batch\n",
      "Epoch: 21/100...  Training Step: 704...  Training loss: 1.6893...  0.4934 sec/batch\n",
      "Epoch: 21/100...  Training Step: 705...  Training loss: 1.6576...  0.4938 sec/batch\n",
      "Epoch: 21/100...  Training Step: 706...  Training loss: 1.6702...  0.4932 sec/batch\n",
      "Epoch: 21/100...  Training Step: 707...  Training loss: 1.6730...  0.4935 sec/batch\n",
      "Epoch: 21/100...  Training Step: 708...  Training loss: 1.6692...  0.4934 sec/batch\n",
      "Epoch: 21/100...  Training Step: 709...  Training loss: 1.7069...  0.4937 sec/batch\n",
      "Epoch: 21/100...  Training Step: 710...  Training loss: 1.7173...  0.4938 sec/batch\n",
      "Epoch: 21/100...  Training Step: 711...  Training loss: 1.6963...  0.4930 sec/batch\n",
      "Epoch: 21/100...  Training Step: 712...  Training loss: 1.7068...  0.4942 sec/batch\n",
      "Epoch: 21/100...  Training Step: 713...  Training loss: 1.7246...  0.4935 sec/batch\n",
      "Epoch: 21/100...  Training Step: 714...  Training loss: 1.6687...  0.4950 sec/batch\n",
      "Epoch: 22/100...  Training Step: 715...  Training loss: 1.7700...  0.4930 sec/batch\n",
      "Epoch: 22/100...  Training Step: 716...  Training loss: 1.6753...  0.4933 sec/batch\n",
      "Epoch: 22/100...  Training Step: 717...  Training loss: 1.6851...  0.4939 sec/batch\n",
      "Epoch: 22/100...  Training Step: 718...  Training loss: 1.7081...  0.4941 sec/batch\n",
      "Epoch: 22/100...  Training Step: 719...  Training loss: 1.6902...  0.4944 sec/batch\n",
      "Epoch: 22/100...  Training Step: 720...  Training loss: 1.6661...  0.4935 sec/batch\n",
      "Epoch: 22/100...  Training Step: 721...  Training loss: 1.6884...  0.4935 sec/batch\n",
      "Epoch: 22/100...  Training Step: 722...  Training loss: 1.6582...  0.4934 sec/batch\n",
      "Epoch: 22/100...  Training Step: 723...  Training loss: 1.6480...  0.4942 sec/batch\n",
      "Epoch: 22/100...  Training Step: 724...  Training loss: 1.6922...  0.4944 sec/batch\n",
      "Epoch: 22/100...  Training Step: 725...  Training loss: 1.6897...  0.4937 sec/batch\n",
      "Epoch: 22/100...  Training Step: 726...  Training loss: 1.6969...  0.4944 sec/batch\n",
      "Epoch: 22/100...  Training Step: 727...  Training loss: 1.6893...  0.4940 sec/batch\n",
      "Epoch: 22/100...  Training Step: 728...  Training loss: 1.6772...  0.4939 sec/batch\n",
      "Epoch: 22/100...  Training Step: 729...  Training loss: 1.6477...  0.4932 sec/batch\n",
      "Epoch: 22/100...  Training Step: 730...  Training loss: 1.6867...  0.4949 sec/batch\n",
      "Epoch: 22/100...  Training Step: 731...  Training loss: 1.6995...  0.4943 sec/batch\n",
      "Epoch: 22/100...  Training Step: 732...  Training loss: 1.6697...  0.4938 sec/batch\n",
      "Epoch: 22/100...  Training Step: 733...  Training loss: 1.6749...  0.4939 sec/batch\n",
      "Epoch: 22/100...  Training Step: 734...  Training loss: 1.6454...  0.4942 sec/batch\n",
      "Epoch: 22/100...  Training Step: 735...  Training loss: 1.6667...  0.4938 sec/batch\n",
      "Epoch: 22/100...  Training Step: 736...  Training loss: 1.6365...  0.4936 sec/batch\n",
      "Epoch: 22/100...  Training Step: 737...  Training loss: 1.6554...  0.4942 sec/batch\n",
      "Epoch: 22/100...  Training Step: 738...  Training loss: 1.6582...  0.4930 sec/batch\n",
      "Epoch: 22/100...  Training Step: 739...  Training loss: 1.6304...  0.4932 sec/batch\n",
      "Epoch: 22/100...  Training Step: 740...  Training loss: 1.6476...  0.4937 sec/batch\n",
      "Epoch: 22/100...  Training Step: 741...  Training loss: 1.6399...  0.4931 sec/batch\n",
      "Epoch: 22/100...  Training Step: 742...  Training loss: 1.6437...  0.4945 sec/batch\n",
      "Epoch: 22/100...  Training Step: 743...  Training loss: 1.6873...  0.4942 sec/batch\n",
      "Epoch: 22/100...  Training Step: 744...  Training loss: 1.6882...  0.4953 sec/batch\n",
      "Epoch: 22/100...  Training Step: 745...  Training loss: 1.6700...  0.4946 sec/batch\n",
      "Epoch: 22/100...  Training Step: 746...  Training loss: 1.6886...  0.4937 sec/batch\n",
      "Epoch: 22/100...  Training Step: 747...  Training loss: 1.6991...  0.4935 sec/batch\n",
      "Epoch: 22/100...  Training Step: 748...  Training loss: 1.6481...  0.4935 sec/batch\n",
      "Epoch: 23/100...  Training Step: 749...  Training loss: 1.7448...  0.4940 sec/batch\n",
      "Epoch: 23/100...  Training Step: 750...  Training loss: 1.6518...  0.4938 sec/batch\n",
      "Epoch: 23/100...  Training Step: 751...  Training loss: 1.6579...  0.4927 sec/batch\n",
      "Epoch: 23/100...  Training Step: 752...  Training loss: 1.6977...  0.4931 sec/batch\n",
      "Epoch: 23/100...  Training Step: 753...  Training loss: 1.6674...  0.4942 sec/batch\n",
      "Epoch: 23/100...  Training Step: 754...  Training loss: 1.6489...  0.4946 sec/batch\n",
      "Epoch: 23/100...  Training Step: 755...  Training loss: 1.6620...  0.4933 sec/batch\n",
      "Epoch: 23/100...  Training Step: 756...  Training loss: 1.6328...  0.4940 sec/batch\n",
      "Epoch: 23/100...  Training Step: 757...  Training loss: 1.6234...  0.4933 sec/batch\n",
      "Epoch: 23/100...  Training Step: 758...  Training loss: 1.6603...  0.4939 sec/batch\n",
      "Epoch: 23/100...  Training Step: 759...  Training loss: 1.6657...  0.4939 sec/batch\n",
      "Epoch: 23/100...  Training Step: 760...  Training loss: 1.6695...  0.4931 sec/batch\n",
      "Epoch: 23/100...  Training Step: 761...  Training loss: 1.6637...  0.4937 sec/batch\n",
      "Epoch: 23/100...  Training Step: 762...  Training loss: 1.6592...  0.4934 sec/batch\n",
      "Epoch: 23/100...  Training Step: 763...  Training loss: 1.6269...  0.4946 sec/batch\n",
      "Epoch: 23/100...  Training Step: 764...  Training loss: 1.6723...  0.4939 sec/batch\n",
      "Epoch: 23/100...  Training Step: 765...  Training loss: 1.6719...  0.4935 sec/batch\n",
      "Epoch: 23/100...  Training Step: 766...  Training loss: 1.6409...  0.4941 sec/batch\n",
      "Epoch: 23/100...  Training Step: 767...  Training loss: 1.6486...  0.4939 sec/batch\n",
      "Epoch: 23/100...  Training Step: 768...  Training loss: 1.6264...  0.4937 sec/batch\n",
      "Epoch: 23/100...  Training Step: 769...  Training loss: 1.6460...  0.4941 sec/batch\n",
      "Epoch: 23/100...  Training Step: 770...  Training loss: 1.6160...  0.4936 sec/batch\n",
      "Epoch: 23/100...  Training Step: 771...  Training loss: 1.6298...  0.4932 sec/batch\n",
      "Epoch: 23/100...  Training Step: 772...  Training loss: 1.6345...  0.4936 sec/batch\n",
      "Epoch: 23/100...  Training Step: 773...  Training loss: 1.6142...  0.4933 sec/batch\n",
      "Epoch: 23/100...  Training Step: 774...  Training loss: 1.6190...  0.4938 sec/batch\n",
      "Epoch: 23/100...  Training Step: 775...  Training loss: 1.6194...  0.4933 sec/batch\n",
      "Epoch: 23/100...  Training Step: 776...  Training loss: 1.6219...  0.4940 sec/batch\n",
      "Epoch: 23/100...  Training Step: 777...  Training loss: 1.6682...  0.4939 sec/batch\n",
      "Epoch: 23/100...  Training Step: 778...  Training loss: 1.6633...  0.4940 sec/batch\n",
      "Epoch: 23/100...  Training Step: 779...  Training loss: 1.6544...  0.4934 sec/batch\n",
      "Epoch: 23/100...  Training Step: 780...  Training loss: 1.6664...  0.4939 sec/batch\n",
      "Epoch: 23/100...  Training Step: 781...  Training loss: 1.6808...  0.4940 sec/batch\n",
      "Epoch: 23/100...  Training Step: 782...  Training loss: 1.6246...  0.4931 sec/batch\n",
      "Epoch: 24/100...  Training Step: 783...  Training loss: 1.7129...  0.4944 sec/batch\n",
      "Epoch: 24/100...  Training Step: 784...  Training loss: 1.6308...  0.4941 sec/batch\n",
      "Epoch: 24/100...  Training Step: 785...  Training loss: 1.6324...  0.4947 sec/batch\n",
      "Epoch: 24/100...  Training Step: 786...  Training loss: 1.6594...  0.4939 sec/batch\n",
      "Epoch: 24/100...  Training Step: 787...  Training loss: 1.6515...  0.4940 sec/batch\n",
      "Epoch: 24/100...  Training Step: 788...  Training loss: 1.6248...  0.4939 sec/batch\n",
      "Epoch: 24/100...  Training Step: 789...  Training loss: 1.6316...  0.4961 sec/batch\n",
      "Epoch: 24/100...  Training Step: 790...  Training loss: 1.6081...  0.4940 sec/batch\n",
      "Epoch: 24/100...  Training Step: 791...  Training loss: 1.6025...  0.4937 sec/batch\n",
      "Epoch: 24/100...  Training Step: 792...  Training loss: 1.6446...  0.4929 sec/batch\n",
      "Epoch: 24/100...  Training Step: 793...  Training loss: 1.6329...  0.4952 sec/batch\n",
      "Epoch: 24/100...  Training Step: 794...  Training loss: 1.6471...  0.4931 sec/batch\n",
      "Epoch: 24/100...  Training Step: 795...  Training loss: 1.6427...  0.4938 sec/batch\n",
      "Epoch: 24/100...  Training Step: 796...  Training loss: 1.6314...  0.4938 sec/batch\n",
      "Epoch: 24/100...  Training Step: 797...  Training loss: 1.6025...  0.4940 sec/batch\n",
      "Epoch: 24/100...  Training Step: 798...  Training loss: 1.6488...  0.4944 sec/batch\n",
      "Epoch: 24/100...  Training Step: 799...  Training loss: 1.6431...  0.4940 sec/batch\n",
      "Epoch: 24/100...  Training Step: 800...  Training loss: 1.6230...  0.4938 sec/batch\n",
      "Epoch: 24/100...  Training Step: 801...  Training loss: 1.6394...  0.4926 sec/batch\n",
      "Epoch: 24/100...  Training Step: 802...  Training loss: 1.6050...  0.4932 sec/batch\n",
      "Epoch: 24/100...  Training Step: 803...  Training loss: 1.6179...  0.4941 sec/batch\n",
      "Epoch: 24/100...  Training Step: 804...  Training loss: 1.5914...  0.4932 sec/batch\n",
      "Epoch: 24/100...  Training Step: 805...  Training loss: 1.6081...  0.4930 sec/batch\n",
      "Epoch: 24/100...  Training Step: 806...  Training loss: 1.6191...  0.4936 sec/batch\n",
      "Epoch: 24/100...  Training Step: 807...  Training loss: 1.5826...  0.4947 sec/batch\n",
      "Epoch: 24/100...  Training Step: 808...  Training loss: 1.6030...  0.4935 sec/batch\n",
      "Epoch: 24/100...  Training Step: 809...  Training loss: 1.5871...  0.4928 sec/batch\n",
      "Epoch: 24/100...  Training Step: 810...  Training loss: 1.5939...  0.4933 sec/batch\n",
      "Epoch: 24/100...  Training Step: 811...  Training loss: 1.6401...  0.4937 sec/batch\n",
      "Epoch: 24/100...  Training Step: 812...  Training loss: 1.6373...  0.4936 sec/batch\n",
      "Epoch: 24/100...  Training Step: 813...  Training loss: 1.6256...  0.4938 sec/batch\n",
      "Epoch: 24/100...  Training Step: 814...  Training loss: 1.6527...  0.4944 sec/batch\n",
      "Epoch: 24/100...  Training Step: 815...  Training loss: 1.6579...  0.4942 sec/batch\n",
      "Epoch: 24/100...  Training Step: 816...  Training loss: 1.6037...  0.4942 sec/batch\n",
      "Epoch: 25/100...  Training Step: 817...  Training loss: 1.6901...  0.4943 sec/batch\n",
      "Epoch: 25/100...  Training Step: 818...  Training loss: 1.6028...  0.4931 sec/batch\n",
      "Epoch: 25/100...  Training Step: 819...  Training loss: 1.6160...  0.4933 sec/batch\n",
      "Epoch: 25/100...  Training Step: 820...  Training loss: 1.6416...  0.4941 sec/batch\n",
      "Epoch: 25/100...  Training Step: 821...  Training loss: 1.6246...  0.4937 sec/batch\n",
      "Epoch: 25/100...  Training Step: 822...  Training loss: 1.5955...  0.4943 sec/batch\n",
      "Epoch: 25/100...  Training Step: 823...  Training loss: 1.6199...  0.4936 sec/batch\n",
      "Epoch: 25/100...  Training Step: 824...  Training loss: 1.5823...  0.4938 sec/batch\n",
      "Epoch: 25/100...  Training Step: 825...  Training loss: 1.5741...  0.4939 sec/batch\n",
      "Epoch: 25/100...  Training Step: 826...  Training loss: 1.6263...  0.4942 sec/batch\n",
      "Epoch: 25/100...  Training Step: 827...  Training loss: 1.6155...  0.4937 sec/batch\n",
      "Epoch: 25/100...  Training Step: 828...  Training loss: 1.6251...  0.4940 sec/batch\n",
      "Epoch: 25/100...  Training Step: 829...  Training loss: 1.6259...  0.4937 sec/batch\n",
      "Epoch: 25/100...  Training Step: 830...  Training loss: 1.6124...  0.4935 sec/batch\n",
      "Epoch: 25/100...  Training Step: 831...  Training loss: 1.5864...  0.4939 sec/batch\n",
      "Epoch: 25/100...  Training Step: 832...  Training loss: 1.6250...  0.4937 sec/batch\n",
      "Epoch: 25/100...  Training Step: 833...  Training loss: 1.6332...  0.4936 sec/batch\n",
      "Epoch: 25/100...  Training Step: 834...  Training loss: 1.6010...  0.4939 sec/batch\n",
      "Epoch: 25/100...  Training Step: 835...  Training loss: 1.6020...  0.4938 sec/batch\n",
      "Epoch: 25/100...  Training Step: 836...  Training loss: 1.5905...  0.4939 sec/batch\n",
      "Epoch: 25/100...  Training Step: 837...  Training loss: 1.5960...  0.4940 sec/batch\n",
      "Epoch: 25/100...  Training Step: 838...  Training loss: 1.5675...  0.4948 sec/batch\n",
      "Epoch: 25/100...  Training Step: 839...  Training loss: 1.5916...  0.4940 sec/batch\n",
      "Epoch: 25/100...  Training Step: 840...  Training loss: 1.5955...  0.4927 sec/batch\n",
      "Epoch: 25/100...  Training Step: 841...  Training loss: 1.5639...  0.4937 sec/batch\n",
      "Epoch: 25/100...  Training Step: 842...  Training loss: 1.5822...  0.4932 sec/batch\n",
      "Epoch: 25/100...  Training Step: 843...  Training loss: 1.5748...  0.4934 sec/batch\n",
      "Epoch: 25/100...  Training Step: 844...  Training loss: 1.5747...  0.4943 sec/batch\n",
      "Epoch: 25/100...  Training Step: 845...  Training loss: 1.6215...  0.4948 sec/batch\n",
      "Epoch: 25/100...  Training Step: 846...  Training loss: 1.6278...  0.4945 sec/batch\n",
      "Epoch: 25/100...  Training Step: 847...  Training loss: 1.6031...  0.4936 sec/batch\n",
      "Epoch: 25/100...  Training Step: 848...  Training loss: 1.6236...  0.4957 sec/batch\n",
      "Epoch: 25/100...  Training Step: 849...  Training loss: 1.6296...  0.4941 sec/batch\n",
      "Epoch: 25/100...  Training Step: 850...  Training loss: 1.5874...  0.4952 sec/batch\n",
      "Epoch: 26/100...  Training Step: 851...  Training loss: 1.6732...  0.4927 sec/batch\n",
      "Epoch: 26/100...  Training Step: 852...  Training loss: 1.5832...  0.4942 sec/batch\n",
      "Epoch: 26/100...  Training Step: 853...  Training loss: 1.6023...  0.4933 sec/batch\n",
      "Epoch: 26/100...  Training Step: 854...  Training loss: 1.6377...  0.4937 sec/batch\n",
      "Epoch: 26/100...  Training Step: 855...  Training loss: 1.6050...  0.4937 sec/batch\n",
      "Epoch: 26/100...  Training Step: 856...  Training loss: 1.5839...  0.4934 sec/batch\n",
      "Epoch: 26/100...  Training Step: 857...  Training loss: 1.6065...  0.4941 sec/batch\n",
      "Epoch: 26/100...  Training Step: 858...  Training loss: 1.5653...  0.4935 sec/batch\n",
      "Epoch: 26/100...  Training Step: 859...  Training loss: 1.5596...  0.4940 sec/batch\n",
      "Epoch: 26/100...  Training Step: 860...  Training loss: 1.6043...  0.4938 sec/batch\n",
      "Epoch: 26/100...  Training Step: 861...  Training loss: 1.5923...  0.4933 sec/batch\n",
      "Epoch: 26/100...  Training Step: 862...  Training loss: 1.6071...  0.4934 sec/batch\n",
      "Epoch: 26/100...  Training Step: 863...  Training loss: 1.6096...  0.4932 sec/batch\n",
      "Epoch: 26/100...  Training Step: 864...  Training loss: 1.5965...  0.4929 sec/batch\n",
      "Epoch: 26/100...  Training Step: 865...  Training loss: 1.5658...  0.4934 sec/batch\n",
      "Epoch: 26/100...  Training Step: 866...  Training loss: 1.6099...  0.4943 sec/batch\n",
      "Epoch: 26/100...  Training Step: 867...  Training loss: 1.6176...  0.4946 sec/batch\n",
      "Epoch: 26/100...  Training Step: 868...  Training loss: 1.5826...  0.4947 sec/batch\n",
      "Epoch: 26/100...  Training Step: 869...  Training loss: 1.5974...  0.4940 sec/batch\n",
      "Epoch: 26/100...  Training Step: 870...  Training loss: 1.5673...  0.4936 sec/batch\n",
      "Epoch: 26/100...  Training Step: 871...  Training loss: 1.5858...  0.4940 sec/batch\n",
      "Epoch: 26/100...  Training Step: 872...  Training loss: 1.5477...  0.4942 sec/batch\n",
      "Epoch: 26/100...  Training Step: 873...  Training loss: 1.5741...  0.4939 sec/batch\n",
      "Epoch: 26/100...  Training Step: 874...  Training loss: 1.5838...  0.4936 sec/batch\n",
      "Epoch: 26/100...  Training Step: 875...  Training loss: 1.5481...  0.4935 sec/batch\n",
      "Epoch: 26/100...  Training Step: 876...  Training loss: 1.5589...  0.4939 sec/batch\n",
      "Epoch: 26/100...  Training Step: 877...  Training loss: 1.5599...  0.4934 sec/batch\n",
      "Epoch: 26/100...  Training Step: 878...  Training loss: 1.5664...  0.4939 sec/batch\n",
      "Epoch: 26/100...  Training Step: 879...  Training loss: 1.6021...  0.4950 sec/batch\n",
      "Epoch: 26/100...  Training Step: 880...  Training loss: 1.6035...  0.4940 sec/batch\n",
      "Epoch: 26/100...  Training Step: 881...  Training loss: 1.5933...  0.4937 sec/batch\n",
      "Epoch: 26/100...  Training Step: 882...  Training loss: 1.6029...  0.4937 sec/batch\n",
      "Epoch: 26/100...  Training Step: 883...  Training loss: 1.6099...  0.4939 sec/batch\n",
      "Epoch: 26/100...  Training Step: 884...  Training loss: 1.5739...  0.4933 sec/batch\n",
      "Epoch: 27/100...  Training Step: 885...  Training loss: 1.6487...  0.4937 sec/batch\n",
      "Epoch: 27/100...  Training Step: 886...  Training loss: 1.5615...  0.4942 sec/batch\n",
      "Epoch: 27/100...  Training Step: 887...  Training loss: 1.5721...  0.4948 sec/batch\n",
      "Epoch: 27/100...  Training Step: 888...  Training loss: 1.6117...  0.4938 sec/batch\n",
      "Epoch: 27/100...  Training Step: 889...  Training loss: 1.5890...  0.4943 sec/batch\n",
      "Epoch: 27/100...  Training Step: 890...  Training loss: 1.5577...  0.4947 sec/batch\n",
      "Epoch: 27/100...  Training Step: 891...  Training loss: 1.5717...  0.4939 sec/batch\n",
      "Epoch: 27/100...  Training Step: 892...  Training loss: 1.5519...  0.4961 sec/batch\n",
      "Epoch: 27/100...  Training Step: 893...  Training loss: 1.5294...  0.4935 sec/batch\n",
      "Epoch: 27/100...  Training Step: 894...  Training loss: 1.5843...  0.4942 sec/batch\n",
      "Epoch: 27/100...  Training Step: 895...  Training loss: 1.5733...  0.4936 sec/batch\n",
      "Epoch: 27/100...  Training Step: 896...  Training loss: 1.5893...  0.4936 sec/batch\n",
      "Epoch: 27/100...  Training Step: 897...  Training loss: 1.5936...  0.4939 sec/batch\n",
      "Epoch: 27/100...  Training Step: 898...  Training loss: 1.5738...  0.4944 sec/batch\n",
      "Epoch: 27/100...  Training Step: 899...  Training loss: 1.5481...  0.4940 sec/batch\n",
      "Epoch: 27/100...  Training Step: 900...  Training loss: 1.5801...  0.4949 sec/batch\n",
      "Epoch: 27/100...  Training Step: 901...  Training loss: 1.5910...  0.4946 sec/batch\n",
      "Epoch: 27/100...  Training Step: 902...  Training loss: 1.5634...  0.4926 sec/batch\n",
      "Epoch: 27/100...  Training Step: 903...  Training loss: 1.5695...  0.4922 sec/batch\n",
      "Epoch: 27/100...  Training Step: 904...  Training loss: 1.5433...  0.4939 sec/batch\n",
      "Epoch: 27/100...  Training Step: 905...  Training loss: 1.5640...  0.4939 sec/batch\n",
      "Epoch: 27/100...  Training Step: 906...  Training loss: 1.5252...  0.4941 sec/batch\n",
      "Epoch: 27/100...  Training Step: 907...  Training loss: 1.5542...  0.4941 sec/batch\n",
      "Epoch: 27/100...  Training Step: 908...  Training loss: 1.5547...  0.4938 sec/batch\n",
      "Epoch: 27/100...  Training Step: 909...  Training loss: 1.5223...  0.4936 sec/batch\n",
      "Epoch: 27/100...  Training Step: 910...  Training loss: 1.5356...  0.4936 sec/batch\n",
      "Epoch: 27/100...  Training Step: 911...  Training loss: 1.5409...  0.4938 sec/batch\n",
      "Epoch: 27/100...  Training Step: 912...  Training loss: 1.5350...  0.4926 sec/batch\n",
      "Epoch: 27/100...  Training Step: 913...  Training loss: 1.5769...  0.4925 sec/batch\n",
      "Epoch: 27/100...  Training Step: 914...  Training loss: 1.5787...  0.4936 sec/batch\n",
      "Epoch: 27/100...  Training Step: 915...  Training loss: 1.5513...  0.4936 sec/batch\n",
      "Epoch: 27/100...  Training Step: 916...  Training loss: 1.5801...  0.4937 sec/batch\n",
      "Epoch: 27/100...  Training Step: 917...  Training loss: 1.5906...  0.4938 sec/batch\n",
      "Epoch: 27/100...  Training Step: 918...  Training loss: 1.5446...  0.4946 sec/batch\n",
      "Epoch: 28/100...  Training Step: 919...  Training loss: 1.6333...  0.4935 sec/batch\n",
      "Epoch: 28/100...  Training Step: 920...  Training loss: 1.5454...  0.4936 sec/batch\n",
      "Epoch: 28/100...  Training Step: 921...  Training loss: 1.5541...  0.4941 sec/batch\n",
      "Epoch: 28/100...  Training Step: 922...  Training loss: 1.5862...  0.4934 sec/batch\n",
      "Epoch: 28/100...  Training Step: 923...  Training loss: 1.5584...  0.4932 sec/batch\n",
      "Epoch: 28/100...  Training Step: 924...  Training loss: 1.5394...  0.4938 sec/batch\n",
      "Epoch: 28/100...  Training Step: 925...  Training loss: 1.5542...  0.4940 sec/batch\n",
      "Epoch: 28/100...  Training Step: 926...  Training loss: 1.5232...  0.4933 sec/batch\n",
      "Epoch: 28/100...  Training Step: 927...  Training loss: 1.5146...  0.4938 sec/batch\n",
      "Epoch: 28/100...  Training Step: 928...  Training loss: 1.5618...  0.4939 sec/batch\n",
      "Epoch: 28/100...  Training Step: 929...  Training loss: 1.5587...  0.4937 sec/batch\n",
      "Epoch: 28/100...  Training Step: 930...  Training loss: 1.5718...  0.4947 sec/batch\n",
      "Epoch: 28/100...  Training Step: 931...  Training loss: 1.5708...  0.4942 sec/batch\n",
      "Epoch: 28/100...  Training Step: 932...  Training loss: 1.5540...  0.4938 sec/batch\n",
      "Epoch: 28/100...  Training Step: 933...  Training loss: 1.5330...  0.4936 sec/batch\n",
      "Epoch: 28/100...  Training Step: 934...  Training loss: 1.5649...  0.4956 sec/batch\n",
      "Epoch: 28/100...  Training Step: 935...  Training loss: 1.5753...  0.4942 sec/batch\n",
      "Epoch: 28/100...  Training Step: 936...  Training loss: 1.5455...  0.4934 sec/batch\n",
      "Epoch: 28/100...  Training Step: 937...  Training loss: 1.5474...  0.4942 sec/batch\n",
      "Epoch: 28/100...  Training Step: 938...  Training loss: 1.5280...  0.4942 sec/batch\n",
      "Epoch: 28/100...  Training Step: 939...  Training loss: 1.5483...  0.4938 sec/batch\n",
      "Epoch: 28/100...  Training Step: 940...  Training loss: 1.5171...  0.4936 sec/batch\n",
      "Epoch: 28/100...  Training Step: 941...  Training loss: 1.5332...  0.4943 sec/batch\n",
      "Epoch: 28/100...  Training Step: 942...  Training loss: 1.5415...  0.4949 sec/batch\n",
      "Epoch: 28/100...  Training Step: 943...  Training loss: 1.5095...  0.4950 sec/batch\n",
      "Epoch: 28/100...  Training Step: 944...  Training loss: 1.5202...  0.4933 sec/batch\n",
      "Epoch: 28/100...  Training Step: 945...  Training loss: 1.5165...  0.4946 sec/batch\n",
      "Epoch: 28/100...  Training Step: 946...  Training loss: 1.5201...  0.4943 sec/batch\n",
      "Epoch: 28/100...  Training Step: 947...  Training loss: 1.5592...  0.4939 sec/batch\n",
      "Epoch: 28/100...  Training Step: 948...  Training loss: 1.5588...  0.4944 sec/batch\n",
      "Epoch: 28/100...  Training Step: 949...  Training loss: 1.5407...  0.4938 sec/batch\n",
      "Epoch: 28/100...  Training Step: 950...  Training loss: 1.5605...  0.4931 sec/batch\n",
      "Epoch: 28/100...  Training Step: 951...  Training loss: 1.5657...  0.4939 sec/batch\n",
      "Epoch: 28/100...  Training Step: 952...  Training loss: 1.5358...  0.4946 sec/batch\n",
      "Epoch: 29/100...  Training Step: 953...  Training loss: 1.6144...  0.4931 sec/batch\n",
      "Epoch: 29/100...  Training Step: 954...  Training loss: 1.5181...  0.4933 sec/batch\n",
      "Epoch: 29/100...  Training Step: 955...  Training loss: 1.5396...  0.4937 sec/batch\n",
      "Epoch: 29/100...  Training Step: 956...  Training loss: 1.5700...  0.4939 sec/batch\n",
      "Epoch: 29/100...  Training Step: 957...  Training loss: 1.5505...  0.4946 sec/batch\n",
      "Epoch: 29/100...  Training Step: 958...  Training loss: 1.5181...  0.4947 sec/batch\n",
      "Epoch: 29/100...  Training Step: 959...  Training loss: 1.5460...  0.4937 sec/batch\n",
      "Epoch: 29/100...  Training Step: 960...  Training loss: 1.5139...  0.4934 sec/batch\n",
      "Epoch: 29/100...  Training Step: 961...  Training loss: 1.5024...  0.4928 sec/batch\n",
      "Epoch: 29/100...  Training Step: 962...  Training loss: 1.5468...  0.4928 sec/batch\n",
      "Epoch: 29/100...  Training Step: 963...  Training loss: 1.5382...  0.4928 sec/batch\n",
      "Epoch: 29/100...  Training Step: 964...  Training loss: 1.5442...  0.4937 sec/batch\n",
      "Epoch: 29/100...  Training Step: 965...  Training loss: 1.5466...  0.4947 sec/batch\n",
      "Epoch: 29/100...  Training Step: 966...  Training loss: 1.5362...  0.4932 sec/batch\n",
      "Epoch: 29/100...  Training Step: 967...  Training loss: 1.5062...  0.4936 sec/batch\n",
      "Epoch: 29/100...  Training Step: 968...  Training loss: 1.5412...  0.4943 sec/batch\n",
      "Epoch: 29/100...  Training Step: 969...  Training loss: 1.5531...  0.4937 sec/batch\n",
      "Epoch: 29/100...  Training Step: 970...  Training loss: 1.5254...  0.4950 sec/batch\n",
      "Epoch: 29/100...  Training Step: 971...  Training loss: 1.5353...  0.4938 sec/batch\n",
      "Epoch: 29/100...  Training Step: 972...  Training loss: 1.5062...  0.4939 sec/batch\n",
      "Epoch: 29/100...  Training Step: 973...  Training loss: 1.5297...  0.4935 sec/batch\n",
      "Epoch: 29/100...  Training Step: 974...  Training loss: 1.5003...  0.4938 sec/batch\n",
      "Epoch: 29/100...  Training Step: 975...  Training loss: 1.5161...  0.4936 sec/batch\n",
      "Epoch: 29/100...  Training Step: 976...  Training loss: 1.5238...  0.4943 sec/batch\n",
      "Epoch: 29/100...  Training Step: 977...  Training loss: 1.4950...  0.4930 sec/batch\n",
      "Epoch: 29/100...  Training Step: 978...  Training loss: 1.5138...  0.4929 sec/batch\n",
      "Epoch: 29/100...  Training Step: 979...  Training loss: 1.4935...  0.4932 sec/batch\n",
      "Epoch: 29/100...  Training Step: 980...  Training loss: 1.5001...  0.4935 sec/batch\n",
      "Epoch: 29/100...  Training Step: 981...  Training loss: 1.5414...  0.4930 sec/batch\n",
      "Epoch: 29/100...  Training Step: 982...  Training loss: 1.5413...  0.4932 sec/batch\n",
      "Epoch: 29/100...  Training Step: 983...  Training loss: 1.5201...  0.4929 sec/batch\n",
      "Epoch: 29/100...  Training Step: 984...  Training loss: 1.5564...  0.4929 sec/batch\n",
      "Epoch: 29/100...  Training Step: 985...  Training loss: 1.5572...  0.4931 sec/batch\n",
      "Epoch: 29/100...  Training Step: 986...  Training loss: 1.5227...  0.4939 sec/batch\n",
      "Epoch: 30/100...  Training Step: 987...  Training loss: 1.6026...  0.4921 sec/batch\n",
      "Epoch: 30/100...  Training Step: 988...  Training loss: 1.5075...  0.4936 sec/batch\n",
      "Epoch: 30/100...  Training Step: 989...  Training loss: 1.5169...  0.4933 sec/batch\n",
      "Epoch: 30/100...  Training Step: 990...  Training loss: 1.5578...  0.4933 sec/batch\n",
      "Epoch: 30/100...  Training Step: 991...  Training loss: 1.5276...  0.4933 sec/batch\n",
      "Epoch: 30/100...  Training Step: 992...  Training loss: 1.4989...  0.4927 sec/batch\n",
      "Epoch: 30/100...  Training Step: 993...  Training loss: 1.5174...  0.4925 sec/batch\n",
      "Epoch: 30/100...  Training Step: 994...  Training loss: 1.4970...  0.4927 sec/batch\n",
      "Epoch: 30/100...  Training Step: 995...  Training loss: 1.4861...  0.4930 sec/batch\n",
      "Epoch: 30/100...  Training Step: 996...  Training loss: 1.5303...  0.4931 sec/batch\n",
      "Epoch: 30/100...  Training Step: 997...  Training loss: 1.5234...  0.4940 sec/batch\n",
      "Epoch: 30/100...  Training Step: 998...  Training loss: 1.5232...  0.4934 sec/batch\n",
      "Epoch: 30/100...  Training Step: 999...  Training loss: 1.5352...  0.4947 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1000...  Training loss: 1.5212...  0.4942 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1001...  Training loss: 1.4906...  0.4938 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1002...  Training loss: 1.5385...  0.4931 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1003...  Training loss: 1.5417...  0.4929 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1004...  Training loss: 1.5077...  0.4932 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1005...  Training loss: 1.5172...  0.4939 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1006...  Training loss: 1.4852...  0.4937 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1007...  Training loss: 1.5107...  0.4935 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1008...  Training loss: 1.4791...  0.4939 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1009...  Training loss: 1.4974...  0.4936 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1010...  Training loss: 1.4998...  0.4948 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1011...  Training loss: 1.4849...  0.4935 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1012...  Training loss: 1.4915...  0.4938 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1013...  Training loss: 1.4771...  0.4936 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1014...  Training loss: 1.4827...  0.4939 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1015...  Training loss: 1.5213...  0.4938 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1016...  Training loss: 1.5261...  0.4940 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1017...  Training loss: 1.5075...  0.4944 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1018...  Training loss: 1.5293...  0.4939 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1019...  Training loss: 1.5472...  0.4938 sec/batch\n",
      "Epoch: 30/100...  Training Step: 1020...  Training loss: 1.4990...  0.4932 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1021...  Training loss: 1.5775...  0.4931 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1022...  Training loss: 1.4914...  0.4940 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1023...  Training loss: 1.5095...  0.4937 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1024...  Training loss: 1.5391...  0.4940 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1025...  Training loss: 1.5127...  0.4941 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1026...  Training loss: 1.4849...  0.4935 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1027...  Training loss: 1.5084...  0.4953 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1028...  Training loss: 1.4764...  0.4964 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1029...  Training loss: 1.4649...  0.4933 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1030...  Training loss: 1.5204...  0.4940 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1031...  Training loss: 1.5109...  0.4943 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1032...  Training loss: 1.5019...  0.4935 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1033...  Training loss: 1.5162...  0.4937 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1034...  Training loss: 1.5035...  0.4940 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1035...  Training loss: 1.4831...  0.4939 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1036...  Training loss: 1.5155...  0.4939 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1037...  Training loss: 1.5323...  0.4935 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1038...  Training loss: 1.4948...  0.4945 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1039...  Training loss: 1.5023...  0.4944 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1040...  Training loss: 1.4851...  0.4938 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1041...  Training loss: 1.5023...  0.4935 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1042...  Training loss: 1.4586...  0.4936 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1043...  Training loss: 1.4832...  0.4933 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1044...  Training loss: 1.4946...  0.4930 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1045...  Training loss: 1.4635...  0.4937 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1046...  Training loss: 1.4802...  0.4939 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1047...  Training loss: 1.4626...  0.4934 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1048...  Training loss: 1.4596...  0.4950 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1049...  Training loss: 1.5118...  0.4940 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1050...  Training loss: 1.5041...  0.4934 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1051...  Training loss: 1.4789...  0.4941 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1052...  Training loss: 1.5114...  0.4936 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1053...  Training loss: 1.5234...  0.4938 sec/batch\n",
      "Epoch: 31/100...  Training Step: 1054...  Training loss: 1.4893...  0.4944 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1055...  Training loss: 1.5640...  0.4931 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1056...  Training loss: 1.4833...  0.4942 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1057...  Training loss: 1.4893...  0.4952 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1058...  Training loss: 1.5211...  0.4940 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1059...  Training loss: 1.5093...  0.4937 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1060...  Training loss: 1.4727...  0.4939 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1061...  Training loss: 1.4911...  0.4941 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1062...  Training loss: 1.4599...  0.4943 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1063...  Training loss: 1.4661...  0.4941 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1064...  Training loss: 1.4868...  0.4944 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1065...  Training loss: 1.4900...  0.4935 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1066...  Training loss: 1.4942...  0.4945 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1067...  Training loss: 1.5053...  0.4932 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1068...  Training loss: 1.4938...  0.4938 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1069...  Training loss: 1.4627...  0.4940 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1070...  Training loss: 1.5072...  0.4939 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1071...  Training loss: 1.5164...  0.4947 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1072...  Training loss: 1.4798...  0.4940 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1073...  Training loss: 1.4863...  0.4938 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1074...  Training loss: 1.4673...  0.4935 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1075...  Training loss: 1.4846...  0.4934 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1076...  Training loss: 1.4534...  0.4948 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1077...  Training loss: 1.4683...  0.4933 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1078...  Training loss: 1.4619...  0.4948 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1079...  Training loss: 1.4472...  0.4938 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1080...  Training loss: 1.4662...  0.4935 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1081...  Training loss: 1.4481...  0.4936 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1082...  Training loss: 1.4575...  0.4937 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1083...  Training loss: 1.4882...  0.4937 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1084...  Training loss: 1.4907...  0.4935 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1085...  Training loss: 1.4636...  0.4932 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1086...  Training loss: 1.5082...  0.4939 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1087...  Training loss: 1.5067...  0.4945 sec/batch\n",
      "Epoch: 32/100...  Training Step: 1088...  Training loss: 1.4750...  0.4936 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1089...  Training loss: 1.5452...  0.4939 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1090...  Training loss: 1.4671...  0.4935 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1091...  Training loss: 1.4770...  0.4937 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1092...  Training loss: 1.5099...  0.4932 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1093...  Training loss: 1.4957...  0.4937 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1094...  Training loss: 1.4612...  0.4933 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1095...  Training loss: 1.4894...  0.4928 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1096...  Training loss: 1.4518...  0.4939 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1097...  Training loss: 1.4397...  0.4939 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1098...  Training loss: 1.4839...  0.4942 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1099...  Training loss: 1.4773...  0.4934 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1100...  Training loss: 1.4800...  0.4936 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1101...  Training loss: 1.4850...  0.4945 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1102...  Training loss: 1.4878...  0.4936 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1103...  Training loss: 1.4520...  0.4940 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1104...  Training loss: 1.4822...  0.4941 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1105...  Training loss: 1.4955...  0.4938 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1106...  Training loss: 1.4712...  0.4943 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1107...  Training loss: 1.4686...  0.4950 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1108...  Training loss: 1.4456...  0.4936 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1109...  Training loss: 1.4663...  0.4935 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1110...  Training loss: 1.4326...  0.4946 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1111...  Training loss: 1.4555...  0.4939 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1112...  Training loss: 1.4555...  0.4940 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1113...  Training loss: 1.4340...  0.4933 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1114...  Training loss: 1.4398...  0.4938 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1115...  Training loss: 1.4403...  0.4933 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1116...  Training loss: 1.4350...  0.4937 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1117...  Training loss: 1.4744...  0.4927 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1118...  Training loss: 1.4723...  0.4936 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1119...  Training loss: 1.4555...  0.4943 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1120...  Training loss: 1.4850...  0.4935 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1121...  Training loss: 1.4906...  0.4948 sec/batch\n",
      "Epoch: 33/100...  Training Step: 1122...  Training loss: 1.4556...  0.4940 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1123...  Training loss: 1.5398...  0.4938 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1124...  Training loss: 1.4491...  0.4952 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1125...  Training loss: 1.4649...  0.4948 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1126...  Training loss: 1.4870...  0.4940 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1127...  Training loss: 1.4823...  0.4931 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1128...  Training loss: 1.4429...  0.4938 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1129...  Training loss: 1.4740...  0.4943 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1130...  Training loss: 1.4387...  0.4941 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1131...  Training loss: 1.4224...  0.4944 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1132...  Training loss: 1.4703...  0.4939 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1133...  Training loss: 1.4621...  0.4934 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1134...  Training loss: 1.4603...  0.4931 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1135...  Training loss: 1.4688...  0.4953 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1136...  Training loss: 1.4634...  0.4945 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1137...  Training loss: 1.4360...  0.4948 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1138...  Training loss: 1.4735...  0.4945 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1139...  Training loss: 1.4762...  0.4934 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1140...  Training loss: 1.4570...  0.4938 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1141...  Training loss: 1.4520...  0.4950 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1142...  Training loss: 1.4348...  0.4963 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1143...  Training loss: 1.4473...  0.4936 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1144...  Training loss: 1.4217...  0.4937 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1145...  Training loss: 1.4502...  0.4935 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1146...  Training loss: 1.4455...  0.4933 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1147...  Training loss: 1.4241...  0.4944 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1148...  Training loss: 1.4299...  0.4943 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1149...  Training loss: 1.4159...  0.4940 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1150...  Training loss: 1.4267...  0.4945 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1151...  Training loss: 1.4571...  0.4937 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1152...  Training loss: 1.4578...  0.4943 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1153...  Training loss: 1.4337...  0.4939 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1154...  Training loss: 1.4611...  0.4938 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1155...  Training loss: 1.4838...  0.4937 sec/batch\n",
      "Epoch: 34/100...  Training Step: 1156...  Training loss: 1.4460...  0.4925 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1157...  Training loss: 1.5107...  0.4935 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1158...  Training loss: 1.4379...  0.4935 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1159...  Training loss: 1.4351...  0.4955 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1160...  Training loss: 1.4790...  0.4939 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1161...  Training loss: 1.4593...  0.4933 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1162...  Training loss: 1.4286...  0.4940 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1163...  Training loss: 1.4562...  0.4933 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1164...  Training loss: 1.4191...  0.4936 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1165...  Training loss: 1.4125...  0.4949 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1166...  Training loss: 1.4513...  0.4938 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1167...  Training loss: 1.4457...  0.4946 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1168...  Training loss: 1.4531...  0.4936 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1169...  Training loss: 1.4476...  0.4939 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1170...  Training loss: 1.4489...  0.4936 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1171...  Training loss: 1.4230...  0.4942 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1172...  Training loss: 1.4545...  0.4939 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1173...  Training loss: 1.4695...  0.4940 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1174...  Training loss: 1.4347...  0.4952 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1175...  Training loss: 1.4386...  0.4927 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1176...  Training loss: 1.4248...  0.4936 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1177...  Training loss: 1.4368...  0.4926 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1178...  Training loss: 1.4084...  0.4932 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1179...  Training loss: 1.4233...  0.4934 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1180...  Training loss: 1.4349...  0.4939 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1181...  Training loss: 1.4072...  0.4936 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1182...  Training loss: 1.4138...  0.4936 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1183...  Training loss: 1.4067...  0.4952 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1184...  Training loss: 1.4172...  0.4936 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1185...  Training loss: 1.4401...  0.4928 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1186...  Training loss: 1.4458...  0.4945 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1187...  Training loss: 1.4250...  0.4944 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1188...  Training loss: 1.4495...  0.4927 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1189...  Training loss: 1.4562...  0.4933 sec/batch\n",
      "Epoch: 35/100...  Training Step: 1190...  Training loss: 1.4259...  0.4941 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1191...  Training loss: 1.4964...  0.4934 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1192...  Training loss: 1.4171...  0.4936 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1193...  Training loss: 1.4336...  0.4930 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1194...  Training loss: 1.4676...  0.4940 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1195...  Training loss: 1.4511...  0.4955 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1196...  Training loss: 1.4111...  0.4936 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1197...  Training loss: 1.4351...  0.4938 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1198...  Training loss: 1.4144...  0.4943 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1199...  Training loss: 1.3848...  0.4938 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1200...  Training loss: 1.4355...  0.4935 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1201...  Training loss: 1.4324...  0.4950 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1202...  Training loss: 1.4367...  0.4946 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1203...  Training loss: 1.4502...  0.4940 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1204...  Training loss: 1.4306...  0.4934 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1205...  Training loss: 1.4073...  0.4940 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1206...  Training loss: 1.4449...  0.4933 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1207...  Training loss: 1.4454...  0.4937 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1208...  Training loss: 1.4224...  0.4932 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1209...  Training loss: 1.4233...  0.4954 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1210...  Training loss: 1.4098...  0.4937 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1211...  Training loss: 1.4282...  0.4933 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1212...  Training loss: 1.4010...  0.4938 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1213...  Training loss: 1.4239...  0.4937 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1214...  Training loss: 1.4173...  0.4942 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1215...  Training loss: 1.3970...  0.4929 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1216...  Training loss: 1.4030...  0.4940 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1217...  Training loss: 1.3902...  0.4941 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1218...  Training loss: 1.4004...  0.4942 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1219...  Training loss: 1.4369...  0.4940 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1220...  Training loss: 1.4455...  0.4943 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1221...  Training loss: 1.4206...  0.4948 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1222...  Training loss: 1.4404...  0.4937 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1223...  Training loss: 1.4402...  0.4936 sec/batch\n",
      "Epoch: 36/100...  Training Step: 1224...  Training loss: 1.4062...  0.4936 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1225...  Training loss: 1.4890...  0.4936 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1226...  Training loss: 1.4148...  0.4934 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1227...  Training loss: 1.4145...  0.4952 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1228...  Training loss: 1.4600...  0.4941 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1229...  Training loss: 1.4325...  0.4932 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1230...  Training loss: 1.3967...  0.4932 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1231...  Training loss: 1.4300...  0.4940 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1232...  Training loss: 1.3984...  0.4942 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1233...  Training loss: 1.3761...  0.4935 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1234...  Training loss: 1.4255...  0.4943 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1235...  Training loss: 1.4262...  0.4944 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1236...  Training loss: 1.4245...  0.4930 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1237...  Training loss: 1.4316...  0.4937 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1238...  Training loss: 1.4369...  0.4938 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1239...  Training loss: 1.3960...  0.4941 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1240...  Training loss: 1.4209...  0.4940 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1241...  Training loss: 1.4449...  0.4938 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1242...  Training loss: 1.4216...  0.4934 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1243...  Training loss: 1.4094...  0.4942 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1244...  Training loss: 1.4057...  0.4941 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1245...  Training loss: 1.4185...  0.4945 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1246...  Training loss: 1.3767...  0.4936 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1247...  Training loss: 1.4101...  0.4940 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1248...  Training loss: 1.4020...  0.4949 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1249...  Training loss: 1.3863...  0.4942 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1250...  Training loss: 1.3977...  0.4927 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1251...  Training loss: 1.3862...  0.4955 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1252...  Training loss: 1.3841...  0.4940 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1253...  Training loss: 1.4140...  0.4929 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1254...  Training loss: 1.4198...  0.4936 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1255...  Training loss: 1.3957...  0.4946 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1256...  Training loss: 1.4343...  0.4934 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1257...  Training loss: 1.4351...  0.4940 sec/batch\n",
      "Epoch: 37/100...  Training Step: 1258...  Training loss: 1.4067...  0.4942 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1259...  Training loss: 1.4763...  0.4933 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1260...  Training loss: 1.3986...  0.4930 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1261...  Training loss: 1.4015...  0.4934 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1262...  Training loss: 1.4338...  0.4951 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1263...  Training loss: 1.4230...  0.4932 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1264...  Training loss: 1.3904...  0.4936 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1265...  Training loss: 1.4030...  0.4933 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1266...  Training loss: 1.3850...  0.4939 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1267...  Training loss: 1.3722...  0.4929 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1268...  Training loss: 1.4126...  0.4938 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1269...  Training loss: 1.4135...  0.4933 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1270...  Training loss: 1.4137...  0.4935 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1271...  Training loss: 1.4127...  0.4935 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1272...  Training loss: 1.4136...  0.4938 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1273...  Training loss: 1.3951...  0.4939 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1274...  Training loss: 1.4222...  0.4943 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1275...  Training loss: 1.4302...  0.4936 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1276...  Training loss: 1.3977...  0.4934 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1277...  Training loss: 1.4029...  0.4929 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1278...  Training loss: 1.3831...  0.4930 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1279...  Training loss: 1.4027...  0.4938 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1280...  Training loss: 1.3729...  0.4938 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1281...  Training loss: 1.3864...  0.4936 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1282...  Training loss: 1.3876...  0.4936 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1283...  Training loss: 1.3761...  0.4943 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1284...  Training loss: 1.3837...  0.4951 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1285...  Training loss: 1.3624...  0.4934 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1286...  Training loss: 1.3747...  0.4938 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1287...  Training loss: 1.4093...  0.4928 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1288...  Training loss: 1.4037...  0.4932 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1289...  Training loss: 1.3875...  0.4938 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1290...  Training loss: 1.4212...  0.4937 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1291...  Training loss: 1.4269...  0.4946 sec/batch\n",
      "Epoch: 38/100...  Training Step: 1292...  Training loss: 1.3888...  0.4933 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1293...  Training loss: 1.4645...  0.4933 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1294...  Training loss: 1.3883...  0.4933 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1295...  Training loss: 1.3887...  0.4942 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1296...  Training loss: 1.4248...  0.4937 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1297...  Training loss: 1.4052...  0.4939 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1298...  Training loss: 1.3678...  0.4959 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1299...  Training loss: 1.4009...  0.4937 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1300...  Training loss: 1.3712...  0.4939 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1301...  Training loss: 1.3651...  0.4934 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1302...  Training loss: 1.4019...  0.4938 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1303...  Training loss: 1.4014...  0.4945 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1304...  Training loss: 1.3959...  0.4941 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1305...  Training loss: 1.4017...  0.4937 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1306...  Training loss: 1.3950...  0.4934 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1307...  Training loss: 1.3726...  0.4935 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1308...  Training loss: 1.4037...  0.4935 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1309...  Training loss: 1.4238...  0.4933 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1310...  Training loss: 1.3882...  0.4942 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1311...  Training loss: 1.3890...  0.4932 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1312...  Training loss: 1.3737...  0.4948 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1313...  Training loss: 1.3932...  0.4939 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1314...  Training loss: 1.3618...  0.4938 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1315...  Training loss: 1.3860...  0.4938 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1316...  Training loss: 1.3843...  0.4935 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1317...  Training loss: 1.3672...  0.4933 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1318...  Training loss: 1.3690...  0.4935 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1319...  Training loss: 1.3593...  0.4949 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1320...  Training loss: 1.3665...  0.4935 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1321...  Training loss: 1.3857...  0.4943 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1322...  Training loss: 1.4023...  0.4932 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1323...  Training loss: 1.3645...  0.4939 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1324...  Training loss: 1.4025...  0.4934 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1325...  Training loss: 1.4157...  0.4931 sec/batch\n",
      "Epoch: 39/100...  Training Step: 1326...  Training loss: 1.3803...  0.4937 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1327...  Training loss: 1.4445...  0.4943 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1328...  Training loss: 1.3719...  0.4937 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1329...  Training loss: 1.3846...  0.4931 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1330...  Training loss: 1.4118...  0.4929 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1331...  Training loss: 1.3955...  0.4923 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1332...  Training loss: 1.3506...  0.4921 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1333...  Training loss: 1.3924...  0.4956 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1334...  Training loss: 1.3649...  0.4941 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1335...  Training loss: 1.3486...  0.4941 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1336...  Training loss: 1.3891...  0.4934 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1337...  Training loss: 1.3877...  0.4942 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1338...  Training loss: 1.3879...  0.4935 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1339...  Training loss: 1.3887...  0.4934 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1340...  Training loss: 1.3935...  0.4936 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1341...  Training loss: 1.3597...  0.4934 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1342...  Training loss: 1.3997...  0.4941 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1343...  Training loss: 1.4087...  0.4938 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1344...  Training loss: 1.3783...  0.4942 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1345...  Training loss: 1.3691...  0.4942 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1346...  Training loss: 1.3652...  0.4949 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1347...  Training loss: 1.3841...  0.4934 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1348...  Training loss: 1.3453...  0.4941 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1349...  Training loss: 1.3669...  0.4930 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1350...  Training loss: 1.3609...  0.4932 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1351...  Training loss: 1.3622...  0.4936 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1352...  Training loss: 1.3655...  0.4930 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1353...  Training loss: 1.3422...  0.4927 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1354...  Training loss: 1.3443...  0.4936 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1355...  Training loss: 1.3832...  0.4933 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1356...  Training loss: 1.3898...  0.4934 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1357...  Training loss: 1.3667...  0.4935 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1358...  Training loss: 1.3914...  0.4937 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1359...  Training loss: 1.3903...  0.4932 sec/batch\n",
      "Epoch: 40/100...  Training Step: 1360...  Training loss: 1.3598...  0.4931 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1361...  Training loss: 1.4381...  0.4924 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1362...  Training loss: 1.3538...  0.4933 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1363...  Training loss: 1.3736...  0.4933 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1364...  Training loss: 1.4102...  0.4937 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1365...  Training loss: 1.3885...  0.4934 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1366...  Training loss: 1.3573...  0.4935 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1367...  Training loss: 1.3820...  0.4928 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1368...  Training loss: 1.3494...  0.4929 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1369...  Training loss: 1.3451...  0.4943 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1370...  Training loss: 1.3802...  0.4936 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1371...  Training loss: 1.3776...  0.4945 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1372...  Training loss: 1.3856...  0.4930 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1373...  Training loss: 1.3772...  0.4928 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1374...  Training loss: 1.3809...  0.4944 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1375...  Training loss: 1.3517...  0.4947 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1376...  Training loss: 1.3871...  0.4937 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1377...  Training loss: 1.4013...  0.4952 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1378...  Training loss: 1.3712...  0.4933 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1379...  Training loss: 1.3596...  0.4933 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1380...  Training loss: 1.3503...  0.4932 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1381...  Training loss: 1.3662...  0.4934 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1382...  Training loss: 1.3385...  0.4934 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1383...  Training loss: 1.3589...  0.4930 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1384...  Training loss: 1.3442...  0.4936 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1385...  Training loss: 1.3396...  0.4947 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1386...  Training loss: 1.3443...  0.4947 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1387...  Training loss: 1.3450...  0.4947 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1388...  Training loss: 1.3384...  0.4928 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1389...  Training loss: 1.3669...  0.4933 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1390...  Training loss: 1.3742...  0.4935 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1391...  Training loss: 1.3504...  0.4935 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1392...  Training loss: 1.3790...  0.4928 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1393...  Training loss: 1.3811...  0.4932 sec/batch\n",
      "Epoch: 41/100...  Training Step: 1394...  Training loss: 1.3606...  0.4960 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1395...  Training loss: 1.4208...  0.4929 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1396...  Training loss: 1.3479...  0.4939 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1397...  Training loss: 1.3600...  0.4928 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1398...  Training loss: 1.3929...  0.4935 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1399...  Training loss: 1.3782...  0.4937 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1400...  Training loss: 1.3366...  0.4945 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1401...  Training loss: 1.3707...  0.4927 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1402...  Training loss: 1.3297...  0.4928 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1403...  Training loss: 1.3199...  0.4935 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1404...  Training loss: 1.3689...  0.4924 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1405...  Training loss: 1.3660...  0.4929 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1406...  Training loss: 1.3614...  0.4930 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1407...  Training loss: 1.3591...  0.4940 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1408...  Training loss: 1.3611...  0.4928 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1409...  Training loss: 1.3383...  0.4926 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1410...  Training loss: 1.3661...  0.4930 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1411...  Training loss: 1.3929...  0.4930 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1412...  Training loss: 1.3538...  0.4926 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1413...  Training loss: 1.3470...  0.4953 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1414...  Training loss: 1.3490...  0.4937 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1415...  Training loss: 1.3537...  0.4938 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1416...  Training loss: 1.3290...  0.4941 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1417...  Training loss: 1.3487...  0.4956 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1418...  Training loss: 1.3308...  0.4931 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1419...  Training loss: 1.3332...  0.4930 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1420...  Training loss: 1.3392...  0.4933 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1421...  Training loss: 1.3275...  0.4930 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1422...  Training loss: 1.3281...  0.4925 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1423...  Training loss: 1.3610...  0.4928 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1424...  Training loss: 1.3607...  0.4933 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1425...  Training loss: 1.3394...  0.4945 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1426...  Training loss: 1.3656...  0.4944 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1427...  Training loss: 1.3734...  0.4934 sec/batch\n",
      "Epoch: 42/100...  Training Step: 1428...  Training loss: 1.3399...  0.4931 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1429...  Training loss: 1.4084...  0.4931 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1430...  Training loss: 1.3404...  0.4930 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1431...  Training loss: 1.3389...  0.4924 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1432...  Training loss: 1.3786...  0.4927 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1433...  Training loss: 1.3606...  0.4934 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1434...  Training loss: 1.3208...  0.4932 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1435...  Training loss: 1.3607...  0.4931 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1436...  Training loss: 1.3360...  0.4940 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1437...  Training loss: 1.3179...  0.4937 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1438...  Training loss: 1.3560...  0.4937 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1439...  Training loss: 1.3509...  0.4949 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1440...  Training loss: 1.3477...  0.4937 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1441...  Training loss: 1.3521...  0.4941 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1442...  Training loss: 1.3564...  0.4937 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1443...  Training loss: 1.3293...  0.4937 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1444...  Training loss: 1.3537...  0.4948 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1445...  Training loss: 1.3723...  0.4937 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1446...  Training loss: 1.3455...  0.4932 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1447...  Training loss: 1.3396...  0.4935 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1448...  Training loss: 1.3254...  0.4927 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1449...  Training loss: 1.3426...  0.4936 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1450...  Training loss: 1.3112...  0.4930 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1451...  Training loss: 1.3313...  0.4936 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1452...  Training loss: 1.3289...  0.4945 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1453...  Training loss: 1.3227...  0.4938 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1454...  Training loss: 1.3203...  0.4933 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1455...  Training loss: 1.3111...  0.4941 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1456...  Training loss: 1.3172...  0.4931 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1457...  Training loss: 1.3522...  0.4927 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1458...  Training loss: 1.3550...  0.4933 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1459...  Training loss: 1.3268...  0.4936 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1460...  Training loss: 1.3489...  0.4949 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1461...  Training loss: 1.3582...  0.4930 sec/batch\n",
      "Epoch: 43/100...  Training Step: 1462...  Training loss: 1.3272...  0.4924 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1463...  Training loss: 1.3974...  0.4929 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1464...  Training loss: 1.3174...  0.4935 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1465...  Training loss: 1.3333...  0.4931 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1466...  Training loss: 1.3722...  0.4938 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1467...  Training loss: 1.3498...  0.4935 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1468...  Training loss: 1.3220...  0.4939 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1469...  Training loss: 1.3432...  0.4926 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1470...  Training loss: 1.3134...  0.4931 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1471...  Training loss: 1.3018...  0.4929 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1472...  Training loss: 1.3351...  0.4929 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1473...  Training loss: 1.3365...  0.4931 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1474...  Training loss: 1.3419...  0.4938 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1475...  Training loss: 1.3430...  0.4931 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1476...  Training loss: 1.3549...  0.4933 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1477...  Training loss: 1.3147...  0.4933 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1478...  Training loss: 1.3359...  0.4940 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1479...  Training loss: 1.3582...  0.4941 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1480...  Training loss: 1.3335...  0.4934 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1481...  Training loss: 1.3292...  0.4928 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1482...  Training loss: 1.3127...  0.4927 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1483...  Training loss: 1.3360...  0.4925 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1484...  Training loss: 1.3128...  0.4925 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1485...  Training loss: 1.3295...  0.4925 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1486...  Training loss: 1.3132...  0.4927 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1487...  Training loss: 1.3150...  0.4925 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1488...  Training loss: 1.3129...  0.4931 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1489...  Training loss: 1.3014...  0.4942 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1490...  Training loss: 1.3076...  0.4942 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1491...  Training loss: 1.3399...  0.4934 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1492...  Training loss: 1.3391...  0.4933 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1493...  Training loss: 1.3151...  0.4928 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1494...  Training loss: 1.3445...  0.4947 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1495...  Training loss: 1.3441...  0.4937 sec/batch\n",
      "Epoch: 44/100...  Training Step: 1496...  Training loss: 1.3181...  0.4938 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1497...  Training loss: 1.3848...  0.4934 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1498...  Training loss: 1.3092...  0.4936 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1499...  Training loss: 1.3206...  0.4926 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1500...  Training loss: 1.3627...  0.4940 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1501...  Training loss: 1.3394...  0.4940 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1502...  Training loss: 1.3060...  0.4926 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1503...  Training loss: 1.3314...  0.4928 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1504...  Training loss: 1.3019...  0.4933 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1505...  Training loss: 1.2818...  0.4930 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1506...  Training loss: 1.3369...  0.4929 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1507...  Training loss: 1.3305...  0.4932 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1508...  Training loss: 1.3242...  0.4931 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1509...  Training loss: 1.3217...  0.4927 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1510...  Training loss: 1.3394...  0.4940 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1511...  Training loss: 1.3079...  0.4933 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1512...  Training loss: 1.3345...  0.4923 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1513...  Training loss: 1.3522...  0.4930 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1514...  Training loss: 1.3276...  0.4929 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1515...  Training loss: 1.3200...  0.4934 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1516...  Training loss: 1.3020...  0.4933 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1517...  Training loss: 1.3289...  0.4931 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1518...  Training loss: 1.2946...  0.4935 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1519...  Training loss: 1.3213...  0.4937 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1520...  Training loss: 1.3062...  0.4944 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1521...  Training loss: 1.2938...  0.4924 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1522...  Training loss: 1.3009...  0.4932 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1523...  Training loss: 1.2857...  0.4943 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1524...  Training loss: 1.2902...  0.4939 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1525...  Training loss: 1.3263...  0.4934 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1526...  Training loss: 1.3302...  0.4933 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1527...  Training loss: 1.3028...  0.4929 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1528...  Training loss: 1.3262...  0.4933 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1529...  Training loss: 1.3379...  0.4935 sec/batch\n",
      "Epoch: 45/100...  Training Step: 1530...  Training loss: 1.3038...  0.4941 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1531...  Training loss: 1.3725...  0.4933 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1532...  Training loss: 1.3006...  0.4940 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1533...  Training loss: 1.3105...  0.4931 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1534...  Training loss: 1.3443...  0.4932 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1535...  Training loss: 1.3293...  0.4929 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1536...  Training loss: 1.2902...  0.4928 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1537...  Training loss: 1.3220...  0.4924 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1538...  Training loss: 1.3032...  0.4927 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1539...  Training loss: 1.2756...  0.4926 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1540...  Training loss: 1.3216...  0.4927 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1541...  Training loss: 1.3148...  0.4939 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1542...  Training loss: 1.3208...  0.4944 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1543...  Training loss: 1.3245...  0.4948 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1544...  Training loss: 1.3235...  0.4935 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1545...  Training loss: 1.2946...  0.4930 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1546...  Training loss: 1.3275...  0.4950 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1547...  Training loss: 1.3349...  0.4944 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1548...  Training loss: 1.3066...  0.4938 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1549...  Training loss: 1.3081...  0.4942 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1550...  Training loss: 1.2913...  0.4939 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1551...  Training loss: 1.3150...  0.4933 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1552...  Training loss: 1.2805...  0.4941 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1553...  Training loss: 1.3039...  0.4931 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1554...  Training loss: 1.2981...  0.4935 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1555...  Training loss: 1.2751...  0.4948 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1556...  Training loss: 1.2944...  0.4938 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1557...  Training loss: 1.2848...  0.4937 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1558...  Training loss: 1.2821...  0.4944 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1559...  Training loss: 1.3067...  0.4938 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1560...  Training loss: 1.3101...  0.4932 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1561...  Training loss: 1.2940...  0.4938 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1562...  Training loss: 1.3224...  0.4934 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1563...  Training loss: 1.3235...  0.4938 sec/batch\n",
      "Epoch: 46/100...  Training Step: 1564...  Training loss: 1.2928...  0.4941 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1565...  Training loss: 1.3646...  0.4935 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1566...  Training loss: 1.2970...  0.4934 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1567...  Training loss: 1.2976...  0.4934 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1568...  Training loss: 1.3321...  0.4932 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1569...  Training loss: 1.3265...  0.4961 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1570...  Training loss: 1.2806...  0.4934 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1571...  Training loss: 1.3189...  0.4937 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1572...  Training loss: 1.2822...  0.4941 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1573...  Training loss: 1.2714...  0.4945 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1574...  Training loss: 1.3051...  0.4936 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1575...  Training loss: 1.3029...  0.4936 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1576...  Training loss: 1.3106...  0.4939 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1577...  Training loss: 1.3030...  0.4938 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1578...  Training loss: 1.3115...  0.4942 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1579...  Training loss: 1.2808...  0.4931 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1580...  Training loss: 1.3102...  0.4942 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1581...  Training loss: 1.3279...  0.4947 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1582...  Training loss: 1.3056...  0.4950 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1583...  Training loss: 1.2903...  0.4940 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1584...  Training loss: 1.2830...  0.4934 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1585...  Training loss: 1.2949...  0.4938 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1586...  Training loss: 1.2763...  0.4945 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1587...  Training loss: 1.2891...  0.4935 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1588...  Training loss: 1.2922...  0.4938 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1589...  Training loss: 1.2809...  0.4937 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1590...  Training loss: 1.2833...  0.4941 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1591...  Training loss: 1.2737...  0.4935 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1592...  Training loss: 1.2713...  0.4933 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1593...  Training loss: 1.3022...  0.4940 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1594...  Training loss: 1.3121...  0.4937 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1595...  Training loss: 1.2732...  0.4935 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1596...  Training loss: 1.2994...  0.4936 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1597...  Training loss: 1.3115...  0.4936 sec/batch\n",
      "Epoch: 47/100...  Training Step: 1598...  Training loss: 1.2868...  0.4939 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1599...  Training loss: 1.3615...  0.4940 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1600...  Training loss: 1.2881...  0.4940 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1601...  Training loss: 1.2948...  0.4939 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1602...  Training loss: 1.3260...  0.4937 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1603...  Training loss: 1.3079...  0.4937 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1604...  Training loss: 1.2757...  0.4937 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1605...  Training loss: 1.3085...  0.4938 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1606...  Training loss: 1.2783...  0.4939 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1607...  Training loss: 1.2604...  0.4940 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1608...  Training loss: 1.3030...  0.4939 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1609...  Training loss: 1.2943...  0.4932 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1610...  Training loss: 1.2967...  0.4935 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1611...  Training loss: 1.3036...  0.4940 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1612...  Training loss: 1.3028...  0.4938 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1613...  Training loss: 1.2712...  0.4948 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1614...  Training loss: 1.3109...  0.4938 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1615...  Training loss: 1.3183...  0.4935 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1616...  Training loss: 1.2982...  0.4938 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1617...  Training loss: 1.2965...  0.4941 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1618...  Training loss: 1.2786...  0.4943 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1619...  Training loss: 1.2997...  0.4941 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1620...  Training loss: 1.2659...  0.4936 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1621...  Training loss: 1.2944...  0.4934 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1622...  Training loss: 1.2707...  0.4938 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1623...  Training loss: 1.2713...  0.4935 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1624...  Training loss: 1.2655...  0.4939 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1625...  Training loss: 1.2661...  0.4938 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1626...  Training loss: 1.2704...  0.4943 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1627...  Training loss: 1.2999...  0.4937 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1628...  Training loss: 1.3019...  0.4938 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1629...  Training loss: 1.2755...  0.4941 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1630...  Training loss: 1.2987...  0.4955 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1631...  Training loss: 1.3027...  0.4941 sec/batch\n",
      "Epoch: 48/100...  Training Step: 1632...  Training loss: 1.2707...  0.4938 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1633...  Training loss: 1.3371...  0.4931 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1634...  Training loss: 1.2682...  0.4936 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1635...  Training loss: 1.2717...  0.4936 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1636...  Training loss: 1.3072...  0.4945 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1637...  Training loss: 1.2953...  0.4941 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1638...  Training loss: 1.2565...  0.4948 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1639...  Training loss: 1.2882...  0.4941 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1640...  Training loss: 1.2688...  0.4938 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1641...  Training loss: 1.2526...  0.4935 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1642...  Training loss: 1.2872...  0.4936 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1643...  Training loss: 1.2915...  0.4946 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1644...  Training loss: 1.2809...  0.4946 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1645...  Training loss: 1.2972...  0.4935 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1646...  Training loss: 1.2951...  0.4938 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1647...  Training loss: 1.2692...  0.4942 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1648...  Training loss: 1.2897...  0.4938 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1649...  Training loss: 1.3110...  0.4947 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1650...  Training loss: 1.2791...  0.4932 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1651...  Training loss: 1.2703...  0.4938 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1652...  Training loss: 1.2769...  0.4934 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1653...  Training loss: 1.2763...  0.4941 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1654...  Training loss: 1.2624...  0.4933 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1655...  Training loss: 1.2856...  0.4937 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1656...  Training loss: 1.2689...  0.4939 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1657...  Training loss: 1.2600...  0.4937 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1658...  Training loss: 1.2567...  0.4941 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1659...  Training loss: 1.2602...  0.4938 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1660...  Training loss: 1.2592...  0.4935 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1661...  Training loss: 1.2843...  0.4944 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1662...  Training loss: 1.2782...  0.4951 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1663...  Training loss: 1.2578...  0.4940 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1664...  Training loss: 1.2868...  0.4933 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1665...  Training loss: 1.2885...  0.4941 sec/batch\n",
      "Epoch: 49/100...  Training Step: 1666...  Training loss: 1.2578...  0.4936 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1667...  Training loss: 1.3276...  0.4934 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1668...  Training loss: 1.2545...  0.4939 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1669...  Training loss: 1.2688...  0.4936 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1670...  Training loss: 1.3067...  0.4933 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1671...  Training loss: 1.2834...  0.4941 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1672...  Training loss: 1.2475...  0.4940 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1673...  Training loss: 1.2813...  0.4944 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1674...  Training loss: 1.2570...  0.4941 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1675...  Training loss: 1.2486...  0.4937 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1676...  Training loss: 1.2776...  0.4938 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1677...  Training loss: 1.2724...  0.4934 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1678...  Training loss: 1.2707...  0.4938 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1679...  Training loss: 1.2861...  0.4943 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1680...  Training loss: 1.2807...  0.4937 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1681...  Training loss: 1.2545...  0.4938 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1682...  Training loss: 1.2810...  0.4929 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1683...  Training loss: 1.2988...  0.4937 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1684...  Training loss: 1.2778...  0.4941 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1685...  Training loss: 1.2749...  0.4949 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1686...  Training loss: 1.2643...  0.4935 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1687...  Training loss: 1.2718...  0.4934 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1688...  Training loss: 1.2465...  0.4936 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1689...  Training loss: 1.2727...  0.4942 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1690...  Training loss: 1.2522...  0.4944 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1691...  Training loss: 1.2462...  0.4939 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1692...  Training loss: 1.2520...  0.4951 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1693...  Training loss: 1.2409...  0.4938 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1694...  Training loss: 1.2442...  0.4937 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1695...  Training loss: 1.2716...  0.4934 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1696...  Training loss: 1.2719...  0.4941 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1697...  Training loss: 1.2484...  0.4937 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1698...  Training loss: 1.2828...  0.4934 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1699...  Training loss: 1.2792...  0.4938 sec/batch\n",
      "Epoch: 50/100...  Training Step: 1700...  Training loss: 1.2492...  0.4939 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1701...  Training loss: 1.3300...  0.4935 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1702...  Training loss: 1.2522...  0.4942 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1703...  Training loss: 1.2590...  0.4937 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1704...  Training loss: 1.2923...  0.4939 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1705...  Training loss: 1.2939...  0.4931 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1706...  Training loss: 1.2485...  0.4936 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1707...  Training loss: 1.2770...  0.4938 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1708...  Training loss: 1.2401...  0.4936 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1709...  Training loss: 1.2342...  0.4935 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1710...  Training loss: 1.2568...  0.4948 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1711...  Training loss: 1.2685...  0.4935 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1712...  Training loss: 1.2788...  0.4926 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1713...  Training loss: 1.2808...  0.4935 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1714...  Training loss: 1.2784...  0.4942 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1715...  Training loss: 1.2553...  0.4939 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1716...  Training loss: 1.2754...  0.4938 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1717...  Training loss: 1.2828...  0.4940 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1718...  Training loss: 1.2662...  0.4932 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1719...  Training loss: 1.2582...  0.4940 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1720...  Training loss: 1.2462...  0.4938 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1721...  Training loss: 1.2560...  0.4936 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1722...  Training loss: 1.2307...  0.4938 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1723...  Training loss: 1.2645...  0.4948 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1724...  Training loss: 1.2441...  0.4957 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1725...  Training loss: 1.2377...  0.4939 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1726...  Training loss: 1.2513...  0.4940 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1727...  Training loss: 1.2374...  0.4942 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1728...  Training loss: 1.2374...  0.4932 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1729...  Training loss: 1.2555...  0.4935 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1730...  Training loss: 1.2499...  0.4940 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1731...  Training loss: 1.2390...  0.4932 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1732...  Training loss: 1.2702...  0.4935 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1733...  Training loss: 1.2726...  0.4943 sec/batch\n",
      "Epoch: 51/100...  Training Step: 1734...  Training loss: 1.2435...  0.4942 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1735...  Training loss: 1.3138...  0.4928 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1736...  Training loss: 1.2382...  0.4936 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1737...  Training loss: 1.2487...  0.4935 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1738...  Training loss: 1.2865...  0.4943 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1739...  Training loss: 1.2708...  0.4936 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1740...  Training loss: 1.2350...  0.4930 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1741...  Training loss: 1.2602...  0.4932 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1742...  Training loss: 1.2371...  0.4932 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1743...  Training loss: 1.2254...  0.4946 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1744...  Training loss: 1.2570...  0.4937 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1745...  Training loss: 1.2689...  0.4938 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1746...  Training loss: 1.2583...  0.4932 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1747...  Training loss: 1.2609...  0.4931 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1748...  Training loss: 1.2650...  0.4940 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1749...  Training loss: 1.2380...  0.4939 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1750...  Training loss: 1.2595...  0.4938 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1751...  Training loss: 1.2694...  0.4938 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1752...  Training loss: 1.2509...  0.4938 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1753...  Training loss: 1.2490...  0.4944 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1754...  Training loss: 1.2440...  0.4940 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1755...  Training loss: 1.2510...  0.4935 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1756...  Training loss: 1.2291...  0.4937 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1757...  Training loss: 1.2535...  0.4935 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1758...  Training loss: 1.2384...  0.4943 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1759...  Training loss: 1.2199...  0.4936 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1760...  Training loss: 1.2322...  0.4939 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1761...  Training loss: 1.2280...  0.4934 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1762...  Training loss: 1.2265...  0.4933 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1763...  Training loss: 1.2573...  0.4941 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1764...  Training loss: 1.2560...  0.4942 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1765...  Training loss: 1.2408...  0.4939 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1766...  Training loss: 1.2505...  0.4944 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1767...  Training loss: 1.2686...  0.4942 sec/batch\n",
      "Epoch: 52/100...  Training Step: 1768...  Training loss: 1.2296...  0.4943 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1769...  Training loss: 1.2968...  0.4942 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1770...  Training loss: 1.2307...  0.4934 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1771...  Training loss: 1.2471...  0.4936 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1772...  Training loss: 1.2703...  0.4936 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1773...  Training loss: 1.2542...  0.4942 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1774...  Training loss: 1.2309...  0.4938 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1775...  Training loss: 1.2566...  0.4937 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1776...  Training loss: 1.2197...  0.4943 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1777...  Training loss: 1.2146...  0.4932 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1778...  Training loss: 1.2540...  0.4943 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1779...  Training loss: 1.2514...  0.4939 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1780...  Training loss: 1.2463...  0.4940 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1781...  Training loss: 1.2577...  0.4936 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1782...  Training loss: 1.2594...  0.4931 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1783...  Training loss: 1.2280...  0.4933 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1784...  Training loss: 1.2575...  0.4939 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1785...  Training loss: 1.2668...  0.4932 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1786...  Training loss: 1.2459...  0.4938 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1787...  Training loss: 1.2411...  0.4944 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1788...  Training loss: 1.2247...  0.4938 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1789...  Training loss: 1.2485...  0.4941 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1790...  Training loss: 1.2127...  0.4938 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1791...  Training loss: 1.2439...  0.4936 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1792...  Training loss: 1.2216...  0.4939 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1793...  Training loss: 1.2215...  0.4941 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1794...  Training loss: 1.2215...  0.4946 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1795...  Training loss: 1.2076...  0.4932 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1796...  Training loss: 1.2169...  0.4939 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1797...  Training loss: 1.2352...  0.4944 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1798...  Training loss: 1.2301...  0.4938 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1799...  Training loss: 1.2200...  0.4941 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1800...  Training loss: 1.2563...  0.4942 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1801...  Training loss: 1.2487...  0.4937 sec/batch\n",
      "Epoch: 53/100...  Training Step: 1802...  Training loss: 1.2225...  0.4935 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1803...  Training loss: 1.2923...  0.4933 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1804...  Training loss: 1.2180...  0.4941 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1805...  Training loss: 1.2332...  0.4933 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1806...  Training loss: 1.2673...  0.4930 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1807...  Training loss: 1.2524...  0.4936 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1808...  Training loss: 1.2153...  0.4938 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1809...  Training loss: 1.2464...  0.4943 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1810...  Training loss: 1.2172...  0.4935 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1811...  Training loss: 1.2077...  0.4944 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1812...  Training loss: 1.2226...  0.4934 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1813...  Training loss: 1.2407...  0.4942 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1814...  Training loss: 1.2369...  0.4940 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1815...  Training loss: 1.2504...  0.4937 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1816...  Training loss: 1.2464...  0.4938 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1817...  Training loss: 1.2219...  0.4938 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1818...  Training loss: 1.2457...  0.4936 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1819...  Training loss: 1.2668...  0.4933 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1820...  Training loss: 1.2279...  0.4940 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1821...  Training loss: 1.2247...  0.4942 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1822...  Training loss: 1.2240...  0.4939 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1823...  Training loss: 1.2395...  0.4936 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1824...  Training loss: 1.2090...  0.4940 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1825...  Training loss: 1.2374...  0.4941 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1826...  Training loss: 1.2187...  0.4937 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1827...  Training loss: 1.2074...  0.4949 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1828...  Training loss: 1.2093...  0.4955 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1829...  Training loss: 1.1995...  0.4942 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1830...  Training loss: 1.2075...  0.4934 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1831...  Training loss: 1.2359...  0.4942 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1832...  Training loss: 1.2374...  0.4951 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1833...  Training loss: 1.1982...  0.4936 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1834...  Training loss: 1.2391...  0.4942 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1835...  Training loss: 1.2484...  0.4941 sec/batch\n",
      "Epoch: 54/100...  Training Step: 1836...  Training loss: 1.2111...  0.4947 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1837...  Training loss: 1.2792...  0.4928 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1838...  Training loss: 1.2052...  0.4935 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1839...  Training loss: 1.2201...  0.4939 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1840...  Training loss: 1.2561...  0.4932 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1841...  Training loss: 1.2414...  0.4928 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1842...  Training loss: 1.2072...  0.4936 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1843...  Training loss: 1.2362...  0.4931 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1844...  Training loss: 1.2118...  0.4935 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1845...  Training loss: 1.1970...  0.4928 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1846...  Training loss: 1.2307...  0.4937 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1847...  Training loss: 1.2329...  0.4934 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1848...  Training loss: 1.2284...  0.4932 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1849...  Training loss: 1.2352...  0.4930 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1850...  Training loss: 1.2378...  0.4938 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1851...  Training loss: 1.2047...  0.4929 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1852...  Training loss: 1.2260...  0.4934 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1853...  Training loss: 1.2430...  0.4930 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1854...  Training loss: 1.2193...  0.4934 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1855...  Training loss: 1.2229...  0.4922 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1856...  Training loss: 1.2108...  0.4932 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1857...  Training loss: 1.2221...  0.4943 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1858...  Training loss: 1.2003...  0.4922 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1859...  Training loss: 1.2217...  0.4923 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1860...  Training loss: 1.2034...  0.4938 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1861...  Training loss: 1.2096...  0.4926 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1862...  Training loss: 1.2059...  0.4933 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1863...  Training loss: 1.1963...  0.4931 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1864...  Training loss: 1.2052...  0.4930 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1865...  Training loss: 1.2241...  0.4929 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1866...  Training loss: 1.2184...  0.4936 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1867...  Training loss: 1.1974...  0.4930 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1868...  Training loss: 1.2312...  0.4932 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1869...  Training loss: 1.2345...  0.4936 sec/batch\n",
      "Epoch: 55/100...  Training Step: 1870...  Training loss: 1.1995...  0.4932 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1871...  Training loss: 1.2721...  0.4942 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1872...  Training loss: 1.1978...  0.4940 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1873...  Training loss: 1.2015...  0.4932 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1874...  Training loss: 1.2402...  0.4929 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1875...  Training loss: 1.2313...  0.4935 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1876...  Training loss: 1.1961...  0.4928 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1877...  Training loss: 1.2221...  0.4934 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1878...  Training loss: 1.1988...  0.4943 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1879...  Training loss: 1.1933...  0.4928 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1880...  Training loss: 1.2191...  0.4943 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1881...  Training loss: 1.2236...  0.4932 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1882...  Training loss: 1.2175...  0.4931 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1883...  Training loss: 1.2253...  0.4940 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1884...  Training loss: 1.2264...  0.4935 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1885...  Training loss: 1.1993...  0.4927 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1886...  Training loss: 1.2317...  0.4937 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1887...  Training loss: 1.2277...  0.4935 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1888...  Training loss: 1.2099...  0.4933 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1889...  Training loss: 1.2134...  0.4934 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1890...  Training loss: 1.1990...  0.4936 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1891...  Training loss: 1.2204...  0.4944 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1892...  Training loss: 1.1913...  0.4937 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1893...  Training loss: 1.2134...  0.4933 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1894...  Training loss: 1.1931...  0.4927 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1895...  Training loss: 1.1984...  0.4933 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1896...  Training loss: 1.2076...  0.4926 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1897...  Training loss: 1.1775...  0.4943 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1898...  Training loss: 1.1978...  0.4932 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1899...  Training loss: 1.2186...  0.4929 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1900...  Training loss: 1.2094...  0.4939 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1901...  Training loss: 1.1882...  0.4936 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1902...  Training loss: 1.2240...  0.4932 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1903...  Training loss: 1.2232...  0.4944 sec/batch\n",
      "Epoch: 56/100...  Training Step: 1904...  Training loss: 1.1937...  0.4932 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1905...  Training loss: 1.2779...  0.4930 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1906...  Training loss: 1.1867...  0.4946 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1907...  Training loss: 1.1970...  0.4936 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1908...  Training loss: 1.2382...  0.4928 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1909...  Training loss: 1.2345...  0.4939 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1910...  Training loss: 1.1898...  0.4935 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1911...  Training loss: 1.2177...  0.4934 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1912...  Training loss: 1.1972...  0.4930 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1913...  Training loss: 1.1757...  0.4927 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1914...  Training loss: 1.2052...  0.4934 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1915...  Training loss: 1.2226...  0.4929 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1916...  Training loss: 1.2037...  0.4937 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1917...  Training loss: 1.2196...  0.4937 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1918...  Training loss: 1.2154...  0.4933 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1919...  Training loss: 1.1951...  0.4934 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1920...  Training loss: 1.2237...  0.4937 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1921...  Training loss: 1.2327...  0.4949 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1922...  Training loss: 1.2086...  0.4939 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1923...  Training loss: 1.2060...  0.4939 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1924...  Training loss: 1.1931...  0.4928 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1925...  Training loss: 1.2114...  0.4940 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1926...  Training loss: 1.1698...  0.4943 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1927...  Training loss: 1.2057...  0.4937 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1928...  Training loss: 1.1836...  0.4927 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1929...  Training loss: 1.1865...  0.4936 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1930...  Training loss: 1.1890...  0.4933 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1931...  Training loss: 1.1845...  0.4937 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1932...  Training loss: 1.1789...  0.4939 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1933...  Training loss: 1.1996...  0.4936 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1934...  Training loss: 1.1946...  0.4935 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1935...  Training loss: 1.1881...  0.4939 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1936...  Training loss: 1.2139...  0.4937 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1937...  Training loss: 1.2111...  0.4935 sec/batch\n",
      "Epoch: 57/100...  Training Step: 1938...  Training loss: 1.1855...  0.4939 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1939...  Training loss: 1.2512...  0.4934 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1940...  Training loss: 1.1923...  0.4938 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1941...  Training loss: 1.1904...  0.4939 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1942...  Training loss: 1.2179...  0.4940 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1943...  Training loss: 1.2048...  0.4942 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1944...  Training loss: 1.1828...  0.4925 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1945...  Training loss: 1.2109...  0.4936 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1946...  Training loss: 1.1839...  0.4934 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1947...  Training loss: 1.1699...  0.4936 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1948...  Training loss: 1.1941...  0.4926 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1949...  Training loss: 1.2088...  0.4936 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1950...  Training loss: 1.1971...  0.4941 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1951...  Training loss: 1.2091...  0.4936 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1952...  Training loss: 1.2112...  0.4929 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1953...  Training loss: 1.1955...  0.4929 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1954...  Training loss: 1.2082...  0.4937 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1955...  Training loss: 1.2248...  0.4939 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1956...  Training loss: 1.1941...  0.4935 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1957...  Training loss: 1.1947...  0.4935 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1958...  Training loss: 1.1840...  0.4936 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1959...  Training loss: 1.2054...  0.4942 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1960...  Training loss: 1.1687...  0.4945 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1961...  Training loss: 1.1977...  0.4932 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1962...  Training loss: 1.1701...  0.4937 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1963...  Training loss: 1.1846...  0.4932 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1964...  Training loss: 1.1852...  0.4943 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1965...  Training loss: 1.1586...  0.4931 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1966...  Training loss: 1.1650...  0.4934 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1967...  Training loss: 1.1978...  0.4937 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1968...  Training loss: 1.1984...  0.4932 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1969...  Training loss: 1.1706...  0.4942 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1970...  Training loss: 1.2026...  0.4940 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1971...  Training loss: 1.2143...  0.4934 sec/batch\n",
      "Epoch: 58/100...  Training Step: 1972...  Training loss: 1.1805...  0.4935 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1973...  Training loss: 1.2433...  0.4932 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1974...  Training loss: 1.1780...  0.4941 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1975...  Training loss: 1.1733...  0.4932 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1976...  Training loss: 1.2100...  0.4950 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1977...  Training loss: 1.1994...  0.4941 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1978...  Training loss: 1.1766...  0.4937 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1979...  Training loss: 1.2019...  0.4939 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1980...  Training loss: 1.1711...  0.4933 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1981...  Training loss: 1.1601...  0.4936 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1982...  Training loss: 1.1855...  0.4933 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1983...  Training loss: 1.1969...  0.4934 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1984...  Training loss: 1.1959...  0.4938 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1985...  Training loss: 1.1933...  0.4942 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1986...  Training loss: 1.1998...  0.4934 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1987...  Training loss: 1.1918...  0.4932 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1988...  Training loss: 1.2062...  0.4938 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1989...  Training loss: 1.2101...  0.4936 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1990...  Training loss: 1.1869...  0.4949 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1991...  Training loss: 1.1781...  0.4954 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1992...  Training loss: 1.1679...  0.4947 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1993...  Training loss: 1.1869...  0.4939 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1994...  Training loss: 1.1579...  0.4946 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1995...  Training loss: 1.1904...  0.4931 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1996...  Training loss: 1.1681...  0.4964 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1997...  Training loss: 1.1679...  0.4928 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1998...  Training loss: 1.1698...  0.4932 sec/batch\n",
      "Epoch: 59/100...  Training Step: 1999...  Training loss: 1.1583...  0.4935 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2000...  Training loss: 1.1690...  0.4932 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2001...  Training loss: 1.1872...  0.4938 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2002...  Training loss: 1.1884...  0.4935 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2003...  Training loss: 1.1697...  0.4936 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2004...  Training loss: 1.2018...  0.4960 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2005...  Training loss: 1.1908...  0.4934 sec/batch\n",
      "Epoch: 59/100...  Training Step: 2006...  Training loss: 1.1738...  0.4936 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2007...  Training loss: 1.2266...  0.4933 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2008...  Training loss: 1.1726...  0.4938 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2009...  Training loss: 1.1687...  0.4944 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2010...  Training loss: 1.1997...  0.4939 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2011...  Training loss: 1.1994...  0.4936 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2012...  Training loss: 1.1716...  0.4938 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2013...  Training loss: 1.1907...  0.4943 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2014...  Training loss: 1.1656...  0.4940 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2015...  Training loss: 1.1372...  0.4938 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2016...  Training loss: 1.1759...  0.4931 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2017...  Training loss: 1.1791...  0.4941 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2018...  Training loss: 1.1887...  0.4937 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2019...  Training loss: 1.1802...  0.4953 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2020...  Training loss: 1.1985...  0.4938 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2021...  Training loss: 1.1614...  0.4936 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2022...  Training loss: 1.1907...  0.4938 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2023...  Training loss: 1.2134...  0.4936 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2024...  Training loss: 1.1836...  0.4946 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2025...  Training loss: 1.1803...  0.4935 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2026...  Training loss: 1.1669...  0.4937 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2027...  Training loss: 1.1869...  0.4935 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2028...  Training loss: 1.1460...  0.4933 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2029...  Training loss: 1.1833...  0.4952 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2030...  Training loss: 1.1563...  0.4933 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2031...  Training loss: 1.1654...  0.4929 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2032...  Training loss: 1.1759...  0.4948 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2033...  Training loss: 1.1504...  0.4941 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2034...  Training loss: 1.1545...  0.4939 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2035...  Training loss: 1.1783...  0.4930 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2036...  Training loss: 1.1718...  0.4937 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2037...  Training loss: 1.1586...  0.4935 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2038...  Training loss: 1.1903...  0.4942 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2039...  Training loss: 1.1983...  0.4944 sec/batch\n",
      "Epoch: 60/100...  Training Step: 2040...  Training loss: 1.1682...  0.4936 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2041...  Training loss: 1.2390...  0.4945 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2042...  Training loss: 1.1613...  0.4945 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2043...  Training loss: 1.1624...  0.4940 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2044...  Training loss: 1.2008...  0.4949 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2045...  Training loss: 1.1817...  0.4936 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2046...  Training loss: 1.1586...  0.4939 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2047...  Training loss: 1.1781...  0.4941 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2048...  Training loss: 1.1586...  0.4935 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2049...  Training loss: 1.1440...  0.4937 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2050...  Training loss: 1.1716...  0.4939 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2051...  Training loss: 1.1896...  0.4934 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2052...  Training loss: 1.1792...  0.4936 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2053...  Training loss: 1.1815...  0.4946 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2054...  Training loss: 1.1865...  0.4935 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2055...  Training loss: 1.1588...  0.4937 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2056...  Training loss: 1.1795...  0.4940 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2057...  Training loss: 1.1827...  0.4934 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2058...  Training loss: 1.1711...  0.4942 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2059...  Training loss: 1.1698...  0.4931 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2060...  Training loss: 1.1648...  0.4946 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2061...  Training loss: 1.1760...  0.4941 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2062...  Training loss: 1.1462...  0.4936 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2063...  Training loss: 1.1704...  0.4939 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2064...  Training loss: 1.1626...  0.4931 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2065...  Training loss: 1.1597...  0.4937 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2066...  Training loss: 1.1722...  0.4938 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2067...  Training loss: 1.1393...  0.4934 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2068...  Training loss: 1.1622...  0.4949 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2069...  Training loss: 1.1798...  0.4948 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2070...  Training loss: 1.1764...  0.4941 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2071...  Training loss: 1.1559...  0.4962 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2072...  Training loss: 1.1720...  0.4936 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2073...  Training loss: 1.1824...  0.4931 sec/batch\n",
      "Epoch: 61/100...  Training Step: 2074...  Training loss: 1.1534...  0.4951 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2075...  Training loss: 1.2250...  0.4937 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2076...  Training loss: 1.1484...  0.4947 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2077...  Training loss: 1.1519...  0.4946 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2078...  Training loss: 1.1914...  0.4942 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2079...  Training loss: 1.1729...  0.4942 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2080...  Training loss: 1.1528...  0.4936 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2081...  Training loss: 1.1721...  0.4934 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2082...  Training loss: 1.1444...  0.4933 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2083...  Training loss: 1.1243...  0.4933 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2084...  Training loss: 1.1655...  0.4946 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2085...  Training loss: 1.1777...  0.4928 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2086...  Training loss: 1.1695...  0.4932 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2087...  Training loss: 1.1755...  0.4947 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2088...  Training loss: 1.1826...  0.4941 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2089...  Training loss: 1.1436...  0.4939 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2090...  Training loss: 1.1713...  0.4942 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2091...  Training loss: 1.1779...  0.4938 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2092...  Training loss: 1.1608...  0.4945 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2093...  Training loss: 1.1542...  0.4940 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2094...  Training loss: 1.1506...  0.4950 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2095...  Training loss: 1.1602...  0.4935 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2096...  Training loss: 1.1438...  0.4951 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2097...  Training loss: 1.1608...  0.4944 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2098...  Training loss: 1.1416...  0.4943 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2099...  Training loss: 1.1610...  0.4934 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2100...  Training loss: 1.1476...  0.4941 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2101...  Training loss: 1.1312...  0.4934 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2102...  Training loss: 1.1392...  0.4943 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2103...  Training loss: 1.1672...  0.4937 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2104...  Training loss: 1.1615...  0.4948 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2105...  Training loss: 1.1385...  0.4931 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2106...  Training loss: 1.1785...  0.4947 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2107...  Training loss: 1.1661...  0.4938 sec/batch\n",
      "Epoch: 62/100...  Training Step: 2108...  Training loss: 1.1449...  0.4940 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2109...  Training loss: 1.2039...  0.4937 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2110...  Training loss: 1.1424...  0.4935 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2111...  Training loss: 1.1496...  0.4940 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2112...  Training loss: 1.1771...  0.4945 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2113...  Training loss: 1.1623...  0.4932 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2114...  Training loss: 1.1460...  0.4945 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2115...  Training loss: 1.1694...  0.4932 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2116...  Training loss: 1.1439...  0.4937 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2117...  Training loss: 1.1159...  0.4938 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2118...  Training loss: 1.1624...  0.4939 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2119...  Training loss: 1.1588...  0.4937 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2120...  Training loss: 1.1605...  0.4937 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2121...  Training loss: 1.1653...  0.4940 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2122...  Training loss: 1.1722...  0.4937 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2123...  Training loss: 1.1442...  0.4935 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2124...  Training loss: 1.1647...  0.4949 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2125...  Training loss: 1.1715...  0.4937 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2126...  Training loss: 1.1553...  0.4934 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2127...  Training loss: 1.1463...  0.4932 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2128...  Training loss: 1.1523...  0.4942 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2129...  Training loss: 1.1592...  0.4938 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2130...  Training loss: 1.1276...  0.4939 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2131...  Training loss: 1.1597...  0.4937 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2132...  Training loss: 1.1322...  0.4943 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2133...  Training loss: 1.1378...  0.4936 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2134...  Training loss: 1.1433...  0.4936 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2135...  Training loss: 1.1259...  0.4947 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2136...  Training loss: 1.1344...  0.4941 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2137...  Training loss: 1.1463...  0.4939 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2138...  Training loss: 1.1489...  0.4943 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2139...  Training loss: 1.1397...  0.4938 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2140...  Training loss: 1.1601...  0.4930 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2141...  Training loss: 1.1571...  0.4939 sec/batch\n",
      "Epoch: 63/100...  Training Step: 2142...  Training loss: 1.1291...  0.4939 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2143...  Training loss: 1.2099...  0.4947 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2144...  Training loss: 1.1312...  0.4939 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2145...  Training loss: 1.1376...  0.4934 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2146...  Training loss: 1.1639...  0.4930 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2147...  Training loss: 1.1592...  0.4937 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2148...  Training loss: 1.1332...  0.4944 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2149...  Training loss: 1.1520...  0.4940 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2150...  Training loss: 1.1234...  0.4934 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2151...  Training loss: 1.1223...  0.4936 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2152...  Training loss: 1.1421...  0.4936 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2153...  Training loss: 1.1478...  0.4948 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2154...  Training loss: 1.1543...  0.4936 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2155...  Training loss: 1.1531...  0.4944 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2156...  Training loss: 1.1526...  0.4930 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2157...  Training loss: 1.1318...  0.4936 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2158...  Training loss: 1.1467...  0.4941 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2159...  Training loss: 1.1707...  0.4942 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2160...  Training loss: 1.1518...  0.4943 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2161...  Training loss: 1.1452...  0.4936 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2162...  Training loss: 1.1287...  0.4934 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2163...  Training loss: 1.1448...  0.4936 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2164...  Training loss: 1.1182...  0.4939 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2165...  Training loss: 1.1485...  0.4950 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2166...  Training loss: 1.1304...  0.4934 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2167...  Training loss: 1.1279...  0.4940 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2168...  Training loss: 1.1352...  0.4931 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2169...  Training loss: 1.1112...  0.4942 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2170...  Training loss: 1.1328...  0.4933 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2171...  Training loss: 1.1484...  0.4939 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2172...  Training loss: 1.1355...  0.4945 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2173...  Training loss: 1.1338...  0.4948 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2174...  Training loss: 1.1476...  0.4937 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2175...  Training loss: 1.1462...  0.4939 sec/batch\n",
      "Epoch: 64/100...  Training Step: 2176...  Training loss: 1.1234...  0.4935 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2177...  Training loss: 1.1939...  0.4944 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2178...  Training loss: 1.1331...  0.4942 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2179...  Training loss: 1.1214...  0.4934 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2180...  Training loss: 1.1546...  0.4934 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2181...  Training loss: 1.1534...  0.4931 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2182...  Training loss: 1.1258...  0.4935 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2183...  Training loss: 1.1517...  0.4951 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2184...  Training loss: 1.1308...  0.4942 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2185...  Training loss: 1.0976...  0.4946 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2186...  Training loss: 1.1392...  0.4939 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2187...  Training loss: 1.1444...  0.4936 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2188...  Training loss: 1.1288...  0.4940 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2189...  Training loss: 1.1372...  0.4940 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2190...  Training loss: 1.1552...  0.4937 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2191...  Training loss: 1.1248...  0.4938 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2192...  Training loss: 1.1454...  0.4951 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2193...  Training loss: 1.1540...  0.4948 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2194...  Training loss: 1.1394...  0.4939 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2195...  Training loss: 1.1444...  0.4941 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2196...  Training loss: 1.1233...  0.4930 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2197...  Training loss: 1.1376...  0.4946 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2198...  Training loss: 1.1110...  0.4958 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2199...  Training loss: 1.1432...  0.4937 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2200...  Training loss: 1.1191...  0.4945 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2201...  Training loss: 1.1192...  0.4937 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2202...  Training loss: 1.1330...  0.4939 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2203...  Training loss: 1.1136...  0.4942 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2204...  Training loss: 1.1276...  0.4949 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2205...  Training loss: 1.1491...  0.4934 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2206...  Training loss: 1.1396...  0.4942 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2207...  Training loss: 1.1181...  0.4953 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2208...  Training loss: 1.1507...  0.4934 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2209...  Training loss: 1.1505...  0.4938 sec/batch\n",
      "Epoch: 65/100...  Training Step: 2210...  Training loss: 1.1251...  0.4945 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2211...  Training loss: 1.1729...  0.4936 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2212...  Training loss: 1.1169...  0.4938 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2213...  Training loss: 1.1242...  0.4933 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2214...  Training loss: 1.1498...  0.4938 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2215...  Training loss: 1.1408...  0.4940 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2216...  Training loss: 1.1171...  0.4933 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2217...  Training loss: 1.1338...  0.4942 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2218...  Training loss: 1.1233...  0.4933 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2219...  Training loss: 1.1050...  0.4939 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2220...  Training loss: 1.1245...  0.4942 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2221...  Training loss: 1.1387...  0.4942 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2222...  Training loss: 1.1332...  0.4937 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2223...  Training loss: 1.1304...  0.4937 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2224...  Training loss: 1.1369...  0.4938 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2225...  Training loss: 1.1121...  0.4936 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2226...  Training loss: 1.1474...  0.4945 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2227...  Training loss: 1.1509...  0.4952 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2228...  Training loss: 1.1354...  0.4944 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2229...  Training loss: 1.1248...  0.4952 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2230...  Training loss: 1.1095...  0.4936 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2231...  Training loss: 1.1389...  0.4938 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2232...  Training loss: 1.1078...  0.4936 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2233...  Training loss: 1.1292...  0.4937 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2234...  Training loss: 1.1097...  0.4945 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2235...  Training loss: 1.1177...  0.4942 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2236...  Training loss: 1.1125...  0.4935 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2237...  Training loss: 1.0930...  0.4934 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2238...  Training loss: 1.1055...  0.4936 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2239...  Training loss: 1.1199...  0.4940 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2240...  Training loss: 1.1221...  0.4939 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2241...  Training loss: 1.1038...  0.4945 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2242...  Training loss: 1.1401...  0.4937 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2243...  Training loss: 1.1353...  0.4947 sec/batch\n",
      "Epoch: 66/100...  Training Step: 2244...  Training loss: 1.0969...  0.4942 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2245...  Training loss: 1.1748...  0.4938 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2246...  Training loss: 1.1081...  0.4935 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2247...  Training loss: 1.1135...  0.4933 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2248...  Training loss: 1.1276...  0.4939 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2249...  Training loss: 1.1347...  0.4934 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2250...  Training loss: 1.1096...  0.4939 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2251...  Training loss: 1.1298...  0.4945 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2252...  Training loss: 1.1134...  0.4938 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2253...  Training loss: 1.0985...  0.4943 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2254...  Training loss: 1.1304...  0.4933 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2255...  Training loss: 1.1350...  0.4937 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2256...  Training loss: 1.1250...  0.4938 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2257...  Training loss: 1.1295...  0.4937 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2258...  Training loss: 1.1405...  0.4933 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2259...  Training loss: 1.1160...  0.4930 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2260...  Training loss: 1.1444...  0.4935 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2261...  Training loss: 1.1414...  0.4931 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2262...  Training loss: 1.1276...  0.4953 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2263...  Training loss: 1.1172...  0.4940 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2264...  Training loss: 1.1112...  0.4954 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2265...  Training loss: 1.1279...  0.4940 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2266...  Training loss: 1.1075...  0.4932 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2267...  Training loss: 1.1298...  0.4938 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2268...  Training loss: 1.1008...  0.4942 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2269...  Training loss: 1.1109...  0.4944 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2270...  Training loss: 1.1102...  0.4951 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2271...  Training loss: 1.1034...  0.4941 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2272...  Training loss: 1.1062...  0.4935 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2273...  Training loss: 1.1223...  0.4936 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2274...  Training loss: 1.1212...  0.4942 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2275...  Training loss: 1.1042...  0.4934 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2276...  Training loss: 1.1128...  0.4943 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2277...  Training loss: 1.1193...  0.4943 sec/batch\n",
      "Epoch: 67/100...  Training Step: 2278...  Training loss: 1.1051...  0.4937 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2279...  Training loss: 1.1632...  0.4936 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2280...  Training loss: 1.0936...  0.4939 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2281...  Training loss: 1.1051...  0.4940 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2282...  Training loss: 1.1280...  0.4936 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2283...  Training loss: 1.1118...  0.4934 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2284...  Training loss: 1.0904...  0.4938 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2285...  Training loss: 1.1206...  0.4937 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2286...  Training loss: 1.1055...  0.4933 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2287...  Training loss: 1.0906...  0.4939 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2288...  Training loss: 1.1085...  0.4952 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2289...  Training loss: 1.1197...  0.4939 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2290...  Training loss: 1.1083...  0.4935 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2291...  Training loss: 1.1155...  0.4939 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2292...  Training loss: 1.1282...  0.4939 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2293...  Training loss: 1.0877...  0.4936 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2294...  Training loss: 1.1238...  0.4933 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2295...  Training loss: 1.1342...  0.4941 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2296...  Training loss: 1.1127...  0.4955 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2297...  Training loss: 1.1078...  0.4933 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2298...  Training loss: 1.0945...  0.4939 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2299...  Training loss: 1.1048...  0.4940 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2300...  Training loss: 1.0888...  0.4947 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2301...  Training loss: 1.1170...  0.4939 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2302...  Training loss: 1.0972...  0.4945 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2303...  Training loss: 1.1022...  0.4943 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2304...  Training loss: 1.0964...  0.4945 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2305...  Training loss: 1.0945...  0.4941 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2306...  Training loss: 1.0986...  0.4943 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2307...  Training loss: 1.1163...  0.4944 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2308...  Training loss: 1.1005...  0.4943 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2309...  Training loss: 1.0890...  0.4935 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2310...  Training loss: 1.1187...  0.4937 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2311...  Training loss: 1.1165...  0.4935 sec/batch\n",
      "Epoch: 68/100...  Training Step: 2312...  Training loss: 1.0865...  0.4940 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2313...  Training loss: 1.1589...  0.4940 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2314...  Training loss: 1.0868...  0.4939 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2315...  Training loss: 1.1085...  0.4936 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2316...  Training loss: 1.1288...  0.4936 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2317...  Training loss: 1.1169...  0.4941 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2318...  Training loss: 1.0886...  0.4943 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2319...  Training loss: 1.1198...  0.4939 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2320...  Training loss: 1.1037...  0.4945 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2321...  Training loss: 1.0689...  0.4937 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2322...  Training loss: 1.1101...  0.4935 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2323...  Training loss: 1.1279...  0.4944 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2324...  Training loss: 1.0953...  0.4936 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2325...  Training loss: 1.1043...  0.4937 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2326...  Training loss: 1.1180...  0.4941 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2327...  Training loss: 1.0981...  0.4941 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2328...  Training loss: 1.1193...  0.4939 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2329...  Training loss: 1.1334...  0.4937 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2330...  Training loss: 1.0974...  0.4941 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2331...  Training loss: 1.1048...  0.4948 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2332...  Training loss: 1.0944...  0.4941 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2333...  Training loss: 1.1102...  0.4938 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2334...  Training loss: 1.0775...  0.4942 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2335...  Training loss: 1.1008...  0.4949 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2336...  Training loss: 1.0912...  0.4942 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2337...  Training loss: 1.0943...  0.4937 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2338...  Training loss: 1.0969...  0.4938 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2339...  Training loss: 1.0748...  0.4943 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2340...  Training loss: 1.0884...  0.4936 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2341...  Training loss: 1.0987...  0.4929 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2342...  Training loss: 1.1002...  0.4936 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2343...  Training loss: 1.0720...  0.4943 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2344...  Training loss: 1.1095...  0.4944 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2345...  Training loss: 1.1186...  0.4939 sec/batch\n",
      "Epoch: 69/100...  Training Step: 2346...  Training loss: 1.0893...  0.4935 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2347...  Training loss: 1.1579...  0.4936 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2348...  Training loss: 1.0858...  0.4941 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2349...  Training loss: 1.0826...  0.4934 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2350...  Training loss: 1.1104...  0.4937 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2351...  Training loss: 1.1039...  0.4939 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2352...  Training loss: 1.0856...  0.4937 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2353...  Training loss: 1.0999...  0.4944 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2354...  Training loss: 1.0853...  0.4938 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2355...  Training loss: 1.0608...  0.4944 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2356...  Training loss: 1.0986...  0.4940 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2357...  Training loss: 1.1133...  0.4938 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2358...  Training loss: 1.0986...  0.4938 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2359...  Training loss: 1.1061...  0.4939 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2360...  Training loss: 1.1105...  0.4940 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2361...  Training loss: 1.0832...  0.4939 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2362...  Training loss: 1.1127...  0.4949 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2363...  Training loss: 1.1321...  0.4944 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2364...  Training loss: 1.0942...  0.4942 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2365...  Training loss: 1.0984...  0.4940 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2366...  Training loss: 1.0904...  0.4946 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2367...  Training loss: 1.1019...  0.4944 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2368...  Training loss: 1.0743...  0.4941 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2369...  Training loss: 1.0925...  0.4943 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2370...  Training loss: 1.0843...  0.4936 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2371...  Training loss: 1.0921...  0.4940 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2372...  Training loss: 1.0885...  0.4942 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2373...  Training loss: 1.0859...  0.4938 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2374...  Training loss: 1.0727...  0.4934 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2375...  Training loss: 1.0876...  0.4942 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2376...  Training loss: 1.0870...  0.4943 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2377...  Training loss: 1.0856...  0.4946 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2378...  Training loss: 1.1086...  0.4941 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2379...  Training loss: 1.1082...  0.4950 sec/batch\n",
      "Epoch: 70/100...  Training Step: 2380...  Training loss: 1.0777...  0.4943 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2381...  Training loss: 1.1458...  0.4937 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2382...  Training loss: 1.0836...  0.4946 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2383...  Training loss: 1.0850...  0.4936 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2384...  Training loss: 1.1042...  0.4943 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2385...  Training loss: 1.1034...  0.4938 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2386...  Training loss: 1.0801...  0.4934 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2387...  Training loss: 1.1032...  0.4950 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2388...  Training loss: 1.0837...  0.4945 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2389...  Training loss: 1.0565...  0.4941 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2390...  Training loss: 1.0875...  0.4943 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2391...  Training loss: 1.1084...  0.4946 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2392...  Training loss: 1.0930...  0.4943 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2393...  Training loss: 1.1010...  0.4939 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2394...  Training loss: 1.1003...  0.4950 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2395...  Training loss: 1.0840...  0.4937 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2396...  Training loss: 1.1076...  0.4950 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2397...  Training loss: 1.1186...  0.4940 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2398...  Training loss: 1.0990...  0.4939 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2399...  Training loss: 1.0814...  0.4939 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2400...  Training loss: 1.0746...  0.4937 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2401...  Training loss: 1.0982...  0.4947 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2402...  Training loss: 1.0643...  0.4936 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2403...  Training loss: 1.1027...  0.4925 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2404...  Training loss: 1.0798...  0.4931 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2405...  Training loss: 1.0877...  0.4937 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2406...  Training loss: 1.0769...  0.4941 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2407...  Training loss: 1.0747...  0.4935 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2408...  Training loss: 1.0750...  0.4935 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2409...  Training loss: 1.0865...  0.4930 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2410...  Training loss: 1.0872...  0.4933 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2411...  Training loss: 1.0722...  0.4934 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2412...  Training loss: 1.1039...  0.4947 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2413...  Training loss: 1.1072...  0.4941 sec/batch\n",
      "Epoch: 71/100...  Training Step: 2414...  Training loss: 1.0737...  0.4948 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2415...  Training loss: 1.1437...  0.4931 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2416...  Training loss: 1.0684...  0.4944 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2417...  Training loss: 1.0766...  0.4940 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2418...  Training loss: 1.0981...  0.4937 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2419...  Training loss: 1.0955...  0.4940 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2420...  Training loss: 1.0662...  0.4939 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2421...  Training loss: 1.0912...  0.4934 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2422...  Training loss: 1.0757...  0.4939 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2423...  Training loss: 1.0488...  0.4945 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2424...  Training loss: 1.0853...  0.4934 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2425...  Training loss: 1.0937...  0.4952 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2426...  Training loss: 1.0959...  0.4936 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2427...  Training loss: 1.0919...  0.4933 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2428...  Training loss: 1.0846...  0.4940 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2429...  Training loss: 1.0841...  0.4938 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2430...  Training loss: 1.0974...  0.4945 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2431...  Training loss: 1.1138...  0.4938 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2432...  Training loss: 1.1009...  0.4932 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2433...  Training loss: 1.0844...  0.4937 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2434...  Training loss: 1.0674...  0.4946 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2435...  Training loss: 1.0781...  0.4940 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2436...  Training loss: 1.0662...  0.4945 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2437...  Training loss: 1.0897...  0.4936 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2438...  Training loss: 1.0725...  0.4940 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2439...  Training loss: 1.0824...  0.4944 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2440...  Training loss: 1.0870...  0.4944 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2441...  Training loss: 1.0691...  0.4942 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2442...  Training loss: 1.0802...  0.4941 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2443...  Training loss: 1.0822...  0.4935 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2444...  Training loss: 1.0757...  0.4934 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2445...  Training loss: 1.0508...  0.4940 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2446...  Training loss: 1.0934...  0.4935 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2447...  Training loss: 1.0783...  0.4940 sec/batch\n",
      "Epoch: 72/100...  Training Step: 2448...  Training loss: 1.0573...  0.4938 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2449...  Training loss: 1.1382...  0.4932 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2450...  Training loss: 1.0695...  0.4936 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2451...  Training loss: 1.0731...  0.4946 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2452...  Training loss: 1.0929...  0.4939 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2453...  Training loss: 1.0866...  0.4939 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2454...  Training loss: 1.0649...  0.4938 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2455...  Training loss: 1.0853...  0.4936 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2456...  Training loss: 1.0733...  0.4933 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2457...  Training loss: 1.0509...  0.4938 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2458...  Training loss: 1.0777...  0.4942 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2459...  Training loss: 1.0918...  0.4931 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2460...  Training loss: 1.0890...  0.4939 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2461...  Training loss: 1.0944...  0.4935 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2462...  Training loss: 1.0901...  0.4936 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2463...  Training loss: 1.0702...  0.4935 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2464...  Training loss: 1.0857...  0.4934 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2465...  Training loss: 1.1069...  0.4934 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2466...  Training loss: 1.0737...  0.4932 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2467...  Training loss: 1.0822...  0.4927 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2468...  Training loss: 1.0770...  0.4931 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2469...  Training loss: 1.0780...  0.4937 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2470...  Training loss: 1.0566...  0.4932 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2471...  Training loss: 1.0874...  0.4933 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2472...  Training loss: 1.0662...  0.4931 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2473...  Training loss: 1.0730...  0.4939 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2474...  Training loss: 1.0723...  0.4932 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2475...  Training loss: 1.0494...  0.4929 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2476...  Training loss: 1.0675...  0.4939 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2477...  Training loss: 1.0848...  0.4938 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2478...  Training loss: 1.0790...  0.4936 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2479...  Training loss: 1.0610...  0.4932 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2480...  Training loss: 1.0863...  0.4932 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2481...  Training loss: 1.0955...  0.4933 sec/batch\n",
      "Epoch: 73/100...  Training Step: 2482...  Training loss: 1.0495...  0.4947 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2483...  Training loss: 1.1301...  0.4932 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2484...  Training loss: 1.0560...  0.4938 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2485...  Training loss: 1.0595...  0.4936 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2486...  Training loss: 1.0927...  0.4939 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2487...  Training loss: 1.0791...  0.4937 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2488...  Training loss: 1.0508...  0.4939 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2489...  Training loss: 1.0803...  0.4930 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2490...  Training loss: 1.0661...  0.4939 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2491...  Training loss: 1.0436...  0.4938 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2492...  Training loss: 1.0681...  0.4948 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2493...  Training loss: 1.0778...  0.4938 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2494...  Training loss: 1.0761...  0.4938 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2495...  Training loss: 1.0840...  0.4938 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2496...  Training loss: 1.0844...  0.4934 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2497...  Training loss: 1.0626...  0.4946 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2498...  Training loss: 1.0857...  0.4928 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2499...  Training loss: 1.0890...  0.4940 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2500...  Training loss: 1.0742...  0.4934 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2501...  Training loss: 1.0805...  0.4927 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2502...  Training loss: 1.0683...  0.4928 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2503...  Training loss: 1.0768...  0.4930 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2504...  Training loss: 1.0540...  0.4943 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2505...  Training loss: 1.0791...  0.4949 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2506...  Training loss: 1.0551...  0.4947 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2507...  Training loss: 1.0554...  0.4938 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2508...  Training loss: 1.0509...  0.4934 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2509...  Training loss: 1.0505...  0.4940 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2510...  Training loss: 1.0523...  0.4939 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2511...  Training loss: 1.0785...  0.4938 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2512...  Training loss: 1.0745...  0.4942 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2513...  Training loss: 1.0564...  0.4930 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2514...  Training loss: 1.0751...  0.4939 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2515...  Training loss: 1.0787...  0.4959 sec/batch\n",
      "Epoch: 74/100...  Training Step: 2516...  Training loss: 1.0466...  0.4932 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2517...  Training loss: 1.1234...  0.4939 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2518...  Training loss: 1.0563...  0.4936 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2519...  Training loss: 1.0564...  0.4933 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2520...  Training loss: 1.0780...  0.4936 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2521...  Training loss: 1.0718...  0.4944 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2522...  Training loss: 1.0513...  0.4936 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2523...  Training loss: 1.0748...  0.4943 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2524...  Training loss: 1.0609...  0.4937 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2525...  Training loss: 1.0325...  0.4940 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2526...  Training loss: 1.0651...  0.4938 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2527...  Training loss: 1.0808...  0.4938 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2528...  Training loss: 1.0760...  0.4941 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2529...  Training loss: 1.0609...  0.4951 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2530...  Training loss: 1.0772...  0.4929 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2531...  Training loss: 1.0629...  0.4932 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2532...  Training loss: 1.0672...  0.4936 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2533...  Training loss: 1.0835...  0.4943 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2534...  Training loss: 1.0708...  0.4939 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2535...  Training loss: 1.0636...  0.4934 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2536...  Training loss: 1.0486...  0.4960 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2537...  Training loss: 1.0796...  0.4934 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2538...  Training loss: 1.0477...  0.4941 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2539...  Training loss: 1.0685...  0.4940 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2540...  Training loss: 1.0560...  0.4932 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2541...  Training loss: 1.0550...  0.4929 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2542...  Training loss: 1.0691...  0.4937 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2543...  Training loss: 1.0380...  0.4946 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2544...  Training loss: 1.0522...  0.4938 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2545...  Training loss: 1.0587...  0.4935 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2546...  Training loss: 1.0638...  0.4934 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2547...  Training loss: 1.0464...  0.4936 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2548...  Training loss: 1.0645...  0.4936 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2549...  Training loss: 1.0628...  0.4941 sec/batch\n",
      "Epoch: 75/100...  Training Step: 2550...  Training loss: 1.0470...  0.4940 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2551...  Training loss: 1.1115...  0.4940 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2552...  Training loss: 1.0404...  0.4947 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2553...  Training loss: 1.0517...  0.4933 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2554...  Training loss: 1.0758...  0.4932 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2555...  Training loss: 1.0655...  0.4928 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2556...  Training loss: 1.0383...  0.4943 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2557...  Training loss: 1.0689...  0.4942 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2558...  Training loss: 1.0453...  0.4951 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2559...  Training loss: 1.0306...  0.4938 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2560...  Training loss: 1.0677...  0.4924 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2561...  Training loss: 1.0862...  0.4940 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2562...  Training loss: 1.0681...  0.4946 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2563...  Training loss: 1.0582...  0.4943 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2564...  Training loss: 1.0705...  0.4934 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2565...  Training loss: 1.0468...  0.4939 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2566...  Training loss: 1.0738...  0.4936 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2567...  Training loss: 1.0868...  0.4936 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2568...  Training loss: 1.0614...  0.4939 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2569...  Training loss: 1.0630...  0.4944 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2570...  Training loss: 1.0390...  0.4935 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2571...  Training loss: 1.0615...  0.4939 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2572...  Training loss: 1.0436...  0.4952 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2573...  Training loss: 1.0661...  0.4936 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2574...  Training loss: 1.0453...  0.4938 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2575...  Training loss: 1.0499...  0.4943 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2576...  Training loss: 1.0545...  0.4937 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2577...  Training loss: 1.0388...  0.4939 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2578...  Training loss: 1.0462...  0.4942 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2579...  Training loss: 1.0483...  0.4939 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2580...  Training loss: 1.0595...  0.4935 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2581...  Training loss: 1.0436...  0.4943 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2582...  Training loss: 1.0663...  0.4939 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2583...  Training loss: 1.0579...  0.4943 sec/batch\n",
      "Epoch: 76/100...  Training Step: 2584...  Training loss: 1.0326...  0.4953 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2585...  Training loss: 1.1052...  0.4936 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2586...  Training loss: 1.0451...  0.4938 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2587...  Training loss: 1.0555...  0.4939 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2588...  Training loss: 1.0763...  0.4933 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2589...  Training loss: 1.0703...  0.4939 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2590...  Training loss: 1.0410...  0.4937 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2591...  Training loss: 1.0654...  0.4933 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2592...  Training loss: 1.0366...  0.4935 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2593...  Training loss: 1.0312...  0.4936 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2594...  Training loss: 1.0615...  0.4940 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2595...  Training loss: 1.0632...  0.4945 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2596...  Training loss: 1.0487...  0.4944 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2597...  Training loss: 1.0579...  0.4939 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2598...  Training loss: 1.0695...  0.4950 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2599...  Training loss: 1.0412...  0.4936 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2600...  Training loss: 1.0677...  0.4941 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2601...  Training loss: 1.0803...  0.4940 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2602...  Training loss: 1.0503...  0.4936 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2603...  Training loss: 1.0609...  0.4944 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2604...  Training loss: 1.0453...  0.4935 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2605...  Training loss: 1.0646...  0.4948 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2606...  Training loss: 1.0431...  0.4941 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2607...  Training loss: 1.0593...  0.4938 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2608...  Training loss: 1.0436...  0.4938 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2609...  Training loss: 1.0385...  0.4936 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2610...  Training loss: 1.0436...  0.4935 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2611...  Training loss: 1.0395...  0.4936 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2612...  Training loss: 1.0422...  0.4943 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2613...  Training loss: 1.0442...  0.4940 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2614...  Training loss: 1.0293...  0.4938 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2615...  Training loss: 1.0244...  0.4944 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2616...  Training loss: 1.0701...  0.4941 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2617...  Training loss: 1.0611...  0.4943 sec/batch\n",
      "Epoch: 77/100...  Training Step: 2618...  Training loss: 1.0229...  0.4940 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2619...  Training loss: 1.0960...  0.4938 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2620...  Training loss: 1.0287...  0.4938 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2621...  Training loss: 1.0388...  0.4930 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2622...  Training loss: 1.0613...  0.4935 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2623...  Training loss: 1.0530...  0.4945 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2624...  Training loss: 1.0276...  0.4935 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2625...  Training loss: 1.0487...  0.4938 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2626...  Training loss: 1.0359...  0.4938 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2627...  Training loss: 1.0147...  0.4936 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2628...  Training loss: 1.0516...  0.4938 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2629...  Training loss: 1.0558...  0.4954 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2630...  Training loss: 1.0437...  0.4952 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2631...  Training loss: 1.0347...  0.4937 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2632...  Training loss: 1.0590...  0.4941 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2633...  Training loss: 1.0384...  0.4933 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2634...  Training loss: 1.0531...  0.4936 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2635...  Training loss: 1.0665...  0.4936 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2636...  Training loss: 1.0449...  0.4941 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2637...  Training loss: 1.0455...  0.4944 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2638...  Training loss: 1.0299...  0.4942 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2639...  Training loss: 1.0468...  0.4928 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2640...  Training loss: 1.0248...  0.4937 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2641...  Training loss: 1.0438...  0.4944 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2642...  Training loss: 1.0327...  0.4931 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2643...  Training loss: 1.0223...  0.4935 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2644...  Training loss: 1.0390...  0.4934 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2645...  Training loss: 1.0244...  0.4944 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2646...  Training loss: 1.0335...  0.4942 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2647...  Training loss: 1.0491...  0.4937 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2648...  Training loss: 1.0364...  0.4942 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2649...  Training loss: 1.0242...  0.4937 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2650...  Training loss: 1.0616...  0.4943 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2651...  Training loss: 1.0477...  0.4933 sec/batch\n",
      "Epoch: 78/100...  Training Step: 2652...  Training loss: 1.0265...  0.4949 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2653...  Training loss: 1.0834...  0.4941 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2654...  Training loss: 1.0182...  0.4942 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2655...  Training loss: 1.0141...  0.4939 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2656...  Training loss: 1.0514...  0.4938 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2657...  Training loss: 1.0402...  0.4937 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2658...  Training loss: 1.0121...  0.4938 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2659...  Training loss: 1.0396...  0.4943 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2660...  Training loss: 1.0273...  0.4941 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2661...  Training loss: 1.0070...  0.4943 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2662...  Training loss: 1.0407...  0.4941 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2663...  Training loss: 1.0404...  0.4935 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2664...  Training loss: 1.0422...  0.4935 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2665...  Training loss: 1.0484...  0.4935 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2666...  Training loss: 1.0484...  0.4939 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2667...  Training loss: 1.0232...  0.4943 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2668...  Training loss: 1.0573...  0.4945 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2669...  Training loss: 1.0606...  0.4932 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2670...  Training loss: 1.0354...  0.4939 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2671...  Training loss: 1.0299...  0.4928 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2672...  Training loss: 1.0085...  0.4941 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2673...  Training loss: 1.0414...  0.4938 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2674...  Training loss: 1.0205...  0.4940 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2675...  Training loss: 1.0405...  0.4941 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2676...  Training loss: 1.0228...  0.4936 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2677...  Training loss: 1.0179...  0.4941 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2678...  Training loss: 1.0336...  0.4936 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2679...  Training loss: 1.0103...  0.4945 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2680...  Training loss: 1.0185...  0.4941 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2681...  Training loss: 1.0406...  0.4932 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2682...  Training loss: 1.0218...  0.4937 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2683...  Training loss: 1.0207...  0.4939 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2684...  Training loss: 1.0462...  0.4940 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2685...  Training loss: 1.0359...  0.4925 sec/batch\n",
      "Epoch: 79/100...  Training Step: 2686...  Training loss: 0.9976...  0.4932 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2687...  Training loss: 1.0696...  0.4928 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2688...  Training loss: 1.0093...  0.4943 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2689...  Training loss: 1.0124...  0.4945 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2690...  Training loss: 1.0457...  0.4943 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2691...  Training loss: 1.0368...  0.4933 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2692...  Training loss: 0.9968...  0.4933 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2693...  Training loss: 1.0225...  0.4934 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2694...  Training loss: 1.0166...  0.4937 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2695...  Training loss: 0.9968...  0.4935 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2696...  Training loss: 1.0173...  0.4947 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2697...  Training loss: 1.0325...  0.4935 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2698...  Training loss: 1.0222...  0.4944 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2699...  Training loss: 1.0373...  0.4941 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2700...  Training loss: 1.0352...  0.4935 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2701...  Training loss: 1.0113...  0.4937 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2702...  Training loss: 1.0448...  0.4940 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2703...  Training loss: 1.0521...  0.4942 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2704...  Training loss: 1.0185...  0.4945 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2705...  Training loss: 1.0282...  0.4956 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2706...  Training loss: 1.0146...  0.4949 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2707...  Training loss: 1.0252...  0.4936 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2708...  Training loss: 1.0077...  0.4939 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2709...  Training loss: 1.0321...  0.4943 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2710...  Training loss: 1.0103...  0.4944 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2711...  Training loss: 0.9973...  0.4946 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2712...  Training loss: 1.0155...  0.4944 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2713...  Training loss: 0.9993...  0.4937 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2714...  Training loss: 1.0152...  0.4925 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2715...  Training loss: 1.0265...  0.4930 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2716...  Training loss: 1.0118...  0.4950 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2717...  Training loss: 1.0026...  0.4941 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2718...  Training loss: 1.0227...  0.4935 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2719...  Training loss: 1.0274...  0.4934 sec/batch\n",
      "Epoch: 80/100...  Training Step: 2720...  Training loss: 1.0046...  0.4939 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2721...  Training loss: 1.0698...  0.4934 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2722...  Training loss: 1.0036...  0.4936 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2723...  Training loss: 1.0042...  0.4938 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2724...  Training loss: 1.0257...  0.4946 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2725...  Training loss: 1.0312...  0.4933 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2726...  Training loss: 1.0148...  0.4940 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2727...  Training loss: 1.0147...  0.4940 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2728...  Training loss: 1.0062...  0.4936 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2729...  Training loss: 0.9804...  0.4938 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2730...  Training loss: 1.0189...  0.4945 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2731...  Training loss: 1.0189...  0.4923 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2732...  Training loss: 1.0172...  0.4954 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2733...  Training loss: 1.0188...  0.4934 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2734...  Training loss: 1.0224...  0.4940 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2735...  Training loss: 0.9940...  0.4938 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2736...  Training loss: 1.0319...  0.4935 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2737...  Training loss: 1.0436...  0.4935 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2738...  Training loss: 1.0178...  0.4939 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2739...  Training loss: 1.0100...  0.4935 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2740...  Training loss: 1.0023...  0.4934 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2741...  Training loss: 1.0312...  0.4942 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2742...  Training loss: 1.0006...  0.4937 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2743...  Training loss: 1.0194...  0.4947 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2744...  Training loss: 0.9982...  0.4939 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2745...  Training loss: 1.0022...  0.4943 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2746...  Training loss: 1.0084...  0.4926 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2747...  Training loss: 0.9945...  0.4930 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2748...  Training loss: 0.9977...  0.4933 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2749...  Training loss: 1.0131...  0.4935 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2750...  Training loss: 1.0064...  0.4936 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2751...  Training loss: 0.9980...  0.4939 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2752...  Training loss: 1.0210...  0.4929 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2753...  Training loss: 1.0186...  0.4938 sec/batch\n",
      "Epoch: 81/100...  Training Step: 2754...  Training loss: 0.9975...  0.4940 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2755...  Training loss: 1.0621...  0.4926 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2756...  Training loss: 0.9941...  0.4930 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2757...  Training loss: 0.9992...  0.4937 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2758...  Training loss: 1.0222...  0.4932 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2759...  Training loss: 1.0199...  0.4935 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2760...  Training loss: 0.9947...  0.4940 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2761...  Training loss: 1.0069...  0.4938 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2762...  Training loss: 0.9899...  0.4932 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2763...  Training loss: 0.9750...  0.4934 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2764...  Training loss: 1.0071...  0.4936 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2765...  Training loss: 1.0111...  0.4937 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2766...  Training loss: 1.0069...  0.4935 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2767...  Training loss: 1.0033...  0.4935 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2768...  Training loss: 1.0226...  0.4936 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2769...  Training loss: 1.0060...  0.4938 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2770...  Training loss: 1.0250...  0.4947 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2771...  Training loss: 1.0303...  0.4940 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2772...  Training loss: 1.0033...  0.4936 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2773...  Training loss: 1.0137...  0.4936 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2774...  Training loss: 0.9956...  0.4951 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2775...  Training loss: 1.0214...  0.4937 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2776...  Training loss: 1.0011...  0.4935 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2777...  Training loss: 1.0208...  0.4929 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2778...  Training loss: 1.0042...  0.4937 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2779...  Training loss: 1.0014...  0.4931 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2780...  Training loss: 1.0147...  0.4940 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2781...  Training loss: 0.9936...  0.4945 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2782...  Training loss: 0.9944...  0.4933 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2783...  Training loss: 0.9997...  0.4936 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2784...  Training loss: 1.0006...  0.4938 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2785...  Training loss: 0.9892...  0.4937 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2786...  Training loss: 1.0168...  0.4934 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2787...  Training loss: 1.0063...  0.4933 sec/batch\n",
      "Epoch: 82/100...  Training Step: 2788...  Training loss: 0.9873...  0.4928 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2789...  Training loss: 1.0527...  0.4940 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2790...  Training loss: 0.9821...  0.4929 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2791...  Training loss: 0.9908...  0.4940 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2792...  Training loss: 1.0121...  0.4936 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2793...  Training loss: 0.9999...  0.4932 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2794...  Training loss: 1.0031...  0.4937 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2795...  Training loss: 1.0067...  0.4948 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2796...  Training loss: 0.9838...  0.4940 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2797...  Training loss: 0.9652...  0.4936 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2798...  Training loss: 0.9944...  0.4937 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2799...  Training loss: 1.0021...  0.4938 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2800...  Training loss: 0.9884...  0.4938 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2801...  Training loss: 1.0001...  0.4940 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2802...  Training loss: 1.0081...  0.4943 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2803...  Training loss: 0.9963...  0.4938 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2804...  Training loss: 1.0177...  0.4939 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2805...  Training loss: 1.0120...  0.4942 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2806...  Training loss: 1.0074...  0.4937 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2807...  Training loss: 1.0080...  0.4951 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2808...  Training loss: 0.9924...  0.4936 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2809...  Training loss: 1.0170...  0.4942 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2810...  Training loss: 0.9934...  0.4948 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2811...  Training loss: 1.0157...  0.4940 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2812...  Training loss: 0.9925...  0.4943 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2813...  Training loss: 1.0001...  0.4938 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2814...  Training loss: 0.9992...  0.4936 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2815...  Training loss: 0.9769...  0.4936 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2816...  Training loss: 0.9931...  0.4929 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2817...  Training loss: 1.0033...  0.4942 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2818...  Training loss: 0.9910...  0.4934 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2819...  Training loss: 0.9803...  0.4948 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2820...  Training loss: 1.0077...  0.4942 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2821...  Training loss: 0.9978...  0.4942 sec/batch\n",
      "Epoch: 83/100...  Training Step: 2822...  Training loss: 0.9807...  0.4936 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2823...  Training loss: 1.0444...  0.4932 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2824...  Training loss: 0.9865...  0.4933 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2825...  Training loss: 0.9857...  0.4937 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2826...  Training loss: 0.9978...  0.4937 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2827...  Training loss: 1.0070...  0.4939 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2828...  Training loss: 0.9839...  0.4932 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2829...  Training loss: 0.9982...  0.4936 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2830...  Training loss: 0.9811...  0.4938 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2831...  Training loss: 0.9719...  0.4946 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2832...  Training loss: 0.9911...  0.4938 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2833...  Training loss: 0.9955...  0.4941 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2834...  Training loss: 0.9965...  0.4941 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2835...  Training loss: 0.9883...  0.4942 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2836...  Training loss: 1.0108...  0.4934 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2837...  Training loss: 0.9831...  0.4939 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2838...  Training loss: 1.0071...  0.4940 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2839...  Training loss: 1.0166...  0.4931 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2840...  Training loss: 0.9928...  0.4936 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2841...  Training loss: 0.9892...  0.4948 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2842...  Training loss: 0.9826...  0.4936 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2843...  Training loss: 1.0007...  0.4932 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2844...  Training loss: 0.9845...  0.4933 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2845...  Training loss: 1.0037...  0.4939 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2846...  Training loss: 0.9919...  0.4936 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2847...  Training loss: 0.9855...  0.4932 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2848...  Training loss: 1.0033...  0.4934 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2849...  Training loss: 0.9840...  0.4945 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2850...  Training loss: 0.9767...  0.4938 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2851...  Training loss: 0.9966...  0.4937 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2852...  Training loss: 0.9919...  0.4931 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2853...  Training loss: 0.9875...  0.4933 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2854...  Training loss: 1.0101...  0.4932 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2855...  Training loss: 1.0025...  0.4940 sec/batch\n",
      "Epoch: 84/100...  Training Step: 2856...  Training loss: 0.9606...  0.4956 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2857...  Training loss: 1.0331...  0.4935 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2858...  Training loss: 0.9754...  0.4928 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2859...  Training loss: 0.9797...  0.4948 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2860...  Training loss: 0.9966...  0.4942 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2861...  Training loss: 1.0025...  0.4938 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2862...  Training loss: 0.9692...  0.4961 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2863...  Training loss: 0.9926...  0.4934 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2864...  Training loss: 0.9783...  0.4938 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2865...  Training loss: 0.9578...  0.4935 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2866...  Training loss: 0.9885...  0.4932 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2867...  Training loss: 0.9999...  0.4940 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2868...  Training loss: 0.9758...  0.4937 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2869...  Training loss: 0.9949...  0.4938 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2870...  Training loss: 0.9902...  0.4931 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2871...  Training loss: 0.9750...  0.4935 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2872...  Training loss: 1.0012...  0.4933 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2873...  Training loss: 1.0060...  0.4934 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2874...  Training loss: 0.9885...  0.4943 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2875...  Training loss: 0.9849...  0.4942 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2876...  Training loss: 0.9760...  0.4936 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2877...  Training loss: 0.9857...  0.4948 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2878...  Training loss: 0.9732...  0.4936 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2879...  Training loss: 1.0038...  0.4932 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2880...  Training loss: 0.9741...  0.4946 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2881...  Training loss: 0.9785...  0.4935 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2882...  Training loss: 0.9851...  0.4944 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2883...  Training loss: 0.9721...  0.4936 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2884...  Training loss: 0.9785...  0.4941 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2885...  Training loss: 0.9894...  0.4939 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2886...  Training loss: 0.9857...  0.4946 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2887...  Training loss: 0.9742...  0.4937 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2888...  Training loss: 0.9915...  0.4947 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2889...  Training loss: 0.9976...  0.4933 sec/batch\n",
      "Epoch: 85/100...  Training Step: 2890...  Training loss: 0.9679...  0.4940 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2891...  Training loss: 1.0343...  0.4929 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2892...  Training loss: 0.9810...  0.4936 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2893...  Training loss: 0.9803...  0.4946 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2894...  Training loss: 0.9974...  0.4931 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2895...  Training loss: 1.0075...  0.4943 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2896...  Training loss: 0.9704...  0.4942 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2897...  Training loss: 0.9894...  0.4946 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2898...  Training loss: 0.9724...  0.4945 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2899...  Training loss: 0.9602...  0.4940 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2900...  Training loss: 0.9789...  0.4938 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2901...  Training loss: 0.9972...  0.4937 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2902...  Training loss: 0.9841...  0.4941 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2903...  Training loss: 0.9838...  0.4933 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2904...  Training loss: 0.9963...  0.4936 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2905...  Training loss: 0.9769...  0.4953 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2906...  Training loss: 0.9827...  0.4939 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2907...  Training loss: 0.9944...  0.4942 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2908...  Training loss: 0.9726...  0.4937 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2909...  Training loss: 0.9799...  0.4943 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2910...  Training loss: 0.9715...  0.4944 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2911...  Training loss: 0.9807...  0.4940 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2912...  Training loss: 0.9730...  0.4941 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2913...  Training loss: 0.9959...  0.4920 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2914...  Training loss: 0.9741...  0.4930 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2915...  Training loss: 0.9779...  0.4940 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2916...  Training loss: 0.9910...  0.4936 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2917...  Training loss: 0.9593...  0.4932 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2918...  Training loss: 0.9670...  0.4941 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2919...  Training loss: 0.9799...  0.4935 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2920...  Training loss: 0.9750...  0.4939 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2921...  Training loss: 0.9697...  0.4951 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2922...  Training loss: 0.9883...  0.4934 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2923...  Training loss: 0.9831...  0.4936 sec/batch\n",
      "Epoch: 86/100...  Training Step: 2924...  Training loss: 0.9516...  0.4934 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2925...  Training loss: 1.0285...  0.4934 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2926...  Training loss: 0.9593...  0.4939 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2927...  Training loss: 0.9588...  0.4946 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2928...  Training loss: 0.9830...  0.4936 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2929...  Training loss: 0.9894...  0.4934 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2930...  Training loss: 0.9673...  0.4934 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2931...  Training loss: 0.9923...  0.4948 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2932...  Training loss: 0.9722...  0.4948 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2933...  Training loss: 0.9481...  0.4946 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2934...  Training loss: 0.9742...  0.4937 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2935...  Training loss: 0.9833...  0.4931 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2936...  Training loss: 0.9685...  0.4937 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2937...  Training loss: 0.9813...  0.4934 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2938...  Training loss: 0.9838...  0.4941 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2939...  Training loss: 0.9693...  0.4934 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2940...  Training loss: 1.0005...  0.4937 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2941...  Training loss: 0.9943...  0.4940 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2942...  Training loss: 0.9654...  0.4940 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2943...  Training loss: 0.9809...  0.4942 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2944...  Training loss: 0.9600...  0.4935 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2945...  Training loss: 0.9859...  0.4947 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2946...  Training loss: 0.9617...  0.4939 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2947...  Training loss: 0.9966...  0.4940 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2948...  Training loss: 0.9682...  0.4933 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2949...  Training loss: 0.9659...  0.4937 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2950...  Training loss: 0.9771...  0.4939 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2951...  Training loss: 0.9537...  0.4941 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2952...  Training loss: 0.9685...  0.4939 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2953...  Training loss: 0.9754...  0.4936 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2954...  Training loss: 0.9659...  0.4942 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2955...  Training loss: 0.9493...  0.4937 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2956...  Training loss: 0.9995...  0.4934 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2957...  Training loss: 0.9819...  0.4942 sec/batch\n",
      "Epoch: 87/100...  Training Step: 2958...  Training loss: 0.9602...  0.4942 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2959...  Training loss: 1.0231...  0.4933 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2960...  Training loss: 0.9493...  0.4940 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2961...  Training loss: 0.9552...  0.4933 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2962...  Training loss: 0.9839...  0.4958 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2963...  Training loss: 0.9734...  0.4931 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2964...  Training loss: 0.9608...  0.4944 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2965...  Training loss: 0.9772...  0.4934 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2966...  Training loss: 0.9733...  0.4936 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2967...  Training loss: 0.9552...  0.4940 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2968...  Training loss: 0.9695...  0.4938 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2969...  Training loss: 0.9793...  0.4933 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2970...  Training loss: 0.9716...  0.4935 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2971...  Training loss: 0.9757...  0.4934 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2972...  Training loss: 0.9880...  0.4947 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2973...  Training loss: 0.9660...  0.4935 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2974...  Training loss: 0.9832...  0.4938 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2975...  Training loss: 0.9874...  0.4940 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2976...  Training loss: 0.9699...  0.4934 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2977...  Training loss: 0.9684...  0.4935 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2978...  Training loss: 0.9507...  0.4933 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2979...  Training loss: 0.9798...  0.4946 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2980...  Training loss: 0.9545...  0.4938 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2981...  Training loss: 0.9821...  0.4936 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2982...  Training loss: 0.9736...  0.4941 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2983...  Training loss: 0.9661...  0.4936 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2984...  Training loss: 0.9692...  0.4951 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2985...  Training loss: 0.9558...  0.4936 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2986...  Training loss: 0.9653...  0.4940 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2987...  Training loss: 0.9649...  0.4940 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2988...  Training loss: 0.9476...  0.4939 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2989...  Training loss: 0.9595...  0.4936 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2990...  Training loss: 0.9903...  0.4932 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2991...  Training loss: 0.9770...  0.4939 sec/batch\n",
      "Epoch: 88/100...  Training Step: 2992...  Training loss: 0.9529...  0.4933 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2993...  Training loss: 1.0069...  0.4938 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2994...  Training loss: 0.9455...  0.4938 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2995...  Training loss: 0.9543...  0.4943 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2996...  Training loss: 0.9686...  0.4937 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2997...  Training loss: 0.9750...  0.4941 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2998...  Training loss: 0.9444...  0.4935 sec/batch\n",
      "Epoch: 89/100...  Training Step: 2999...  Training loss: 0.9653...  0.4945 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3000...  Training loss: 0.9611...  0.4934 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3001...  Training loss: 0.9406...  0.4939 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3002...  Training loss: 0.9661...  0.4937 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3003...  Training loss: 0.9724...  0.4946 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3004...  Training loss: 0.9654...  0.4937 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3005...  Training loss: 0.9719...  0.4945 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3006...  Training loss: 0.9651...  0.4945 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3007...  Training loss: 0.9479...  0.4939 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3008...  Training loss: 0.9714...  0.4936 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3009...  Training loss: 0.9848...  0.4945 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3010...  Training loss: 0.9671...  0.4933 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3011...  Training loss: 0.9650...  0.4933 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3012...  Training loss: 0.9539...  0.4933 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3013...  Training loss: 0.9703...  0.4934 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3014...  Training loss: 0.9508...  0.4935 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3015...  Training loss: 0.9674...  0.4943 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3016...  Training loss: 0.9554...  0.4940 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3017...  Training loss: 0.9559...  0.4937 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3018...  Training loss: 0.9721...  0.4937 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3019...  Training loss: 0.9416...  0.4941 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3020...  Training loss: 0.9499...  0.4939 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3021...  Training loss: 0.9570...  0.4941 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3022...  Training loss: 0.9588...  0.4940 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3023...  Training loss: 0.9327...  0.4937 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3024...  Training loss: 0.9857...  0.4954 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3025...  Training loss: 0.9716...  0.4935 sec/batch\n",
      "Epoch: 89/100...  Training Step: 3026...  Training loss: 0.9433...  0.4934 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3027...  Training loss: 1.0101...  0.4935 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3028...  Training loss: 0.9442...  0.4934 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3029...  Training loss: 0.9446...  0.4939 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3030...  Training loss: 0.9633...  0.4940 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3031...  Training loss: 0.9638...  0.4938 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3032...  Training loss: 0.9415...  0.4932 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3033...  Training loss: 0.9731...  0.4935 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3034...  Training loss: 0.9563...  0.4936 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3035...  Training loss: 0.9287...  0.4949 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3036...  Training loss: 0.9633...  0.4935 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3037...  Training loss: 0.9710...  0.4944 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3038...  Training loss: 0.9663...  0.4937 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3039...  Training loss: 0.9566...  0.4940 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3040...  Training loss: 0.9624...  0.4940 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3041...  Training loss: 0.9498...  0.4939 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3042...  Training loss: 0.9741...  0.4938 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3043...  Training loss: 0.9838...  0.4931 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3044...  Training loss: 0.9650...  0.4933 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3045...  Training loss: 0.9657...  0.4938 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3046...  Training loss: 0.9505...  0.4935 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3047...  Training loss: 0.9656...  0.4934 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3048...  Training loss: 0.9510...  0.4935 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3049...  Training loss: 0.9750...  0.4939 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3050...  Training loss: 0.9509...  0.4939 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3051...  Training loss: 0.9481...  0.4942 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3052...  Training loss: 0.9524...  0.4943 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3053...  Training loss: 0.9369...  0.4935 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3054...  Training loss: 0.9505...  0.4940 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3055...  Training loss: 0.9562...  0.4937 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3056...  Training loss: 0.9575...  0.4938 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3057...  Training loss: 0.9301...  0.4937 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3058...  Training loss: 0.9561...  0.4932 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3059...  Training loss: 0.9636...  0.4936 sec/batch\n",
      "Epoch: 90/100...  Training Step: 3060...  Training loss: 0.9306...  0.4936 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3061...  Training loss: 1.0033...  0.4935 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3062...  Training loss: 0.9373...  0.4934 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3063...  Training loss: 0.9497...  0.4931 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3064...  Training loss: 0.9532...  0.4931 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3065...  Training loss: 0.9566...  0.4939 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3066...  Training loss: 0.9263...  0.4936 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3067...  Training loss: 0.9572...  0.4940 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3068...  Training loss: 0.9609...  0.4940 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3069...  Training loss: 0.9160...  0.4945 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3070...  Training loss: 0.9423...  0.4925 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3071...  Training loss: 0.9613...  0.4943 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3072...  Training loss: 0.9598...  0.4931 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3073...  Training loss: 0.9536...  0.4934 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3074...  Training loss: 0.9614...  0.4935 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3075...  Training loss: 0.9421...  0.4932 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3076...  Training loss: 0.9543...  0.4935 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3077...  Training loss: 0.9676...  0.4933 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3078...  Training loss: 0.9364...  0.4943 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3079...  Training loss: 0.9603...  0.4934 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3080...  Training loss: 0.9307...  0.4938 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3081...  Training loss: 0.9659...  0.4942 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3082...  Training loss: 0.9395...  0.4934 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3083...  Training loss: 0.9758...  0.4941 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3084...  Training loss: 0.9516...  0.4938 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3085...  Training loss: 0.9389...  0.4941 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3086...  Training loss: 0.9582...  0.4939 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3087...  Training loss: 0.9378...  0.4938 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3088...  Training loss: 0.9418...  0.4948 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3089...  Training loss: 0.9487...  0.4944 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3090...  Training loss: 0.9523...  0.4948 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3091...  Training loss: 0.9451...  0.4944 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3092...  Training loss: 0.9569...  0.4944 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3093...  Training loss: 0.9510...  0.4931 sec/batch\n",
      "Epoch: 91/100...  Training Step: 3094...  Training loss: 0.9293...  0.4941 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3095...  Training loss: 0.9973...  0.4933 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3096...  Training loss: 0.9275...  0.4936 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3097...  Training loss: 0.9266...  0.4931 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3098...  Training loss: 0.9482...  0.4939 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3099...  Training loss: 0.9589...  0.4937 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3100...  Training loss: 0.9270...  0.4941 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3101...  Training loss: 0.9476...  0.4940 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3102...  Training loss: 0.9418...  0.4934 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3103...  Training loss: 0.9157...  0.4931 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3104...  Training loss: 0.9410...  0.4949 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3105...  Training loss: 0.9645...  0.4937 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3106...  Training loss: 0.9499...  0.4929 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3107...  Training loss: 0.9382...  0.4937 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3108...  Training loss: 0.9463...  0.4932 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3109...  Training loss: 0.9292...  0.4935 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3110...  Training loss: 0.9592...  0.4952 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3111...  Training loss: 0.9750...  0.4936 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3112...  Training loss: 0.9446...  0.4936 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3113...  Training loss: 0.9456...  0.4942 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3114...  Training loss: 0.9323...  0.4934 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3115...  Training loss: 0.9516...  0.4942 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3116...  Training loss: 0.9246...  0.4949 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3117...  Training loss: 0.9593...  0.4930 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3118...  Training loss: 0.9347...  0.4931 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3119...  Training loss: 0.9414...  0.4931 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3120...  Training loss: 0.9404...  0.4939 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3121...  Training loss: 0.9204...  0.4935 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3122...  Training loss: 0.9310...  0.4940 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3123...  Training loss: 0.9414...  0.4941 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3124...  Training loss: 0.9481...  0.4939 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3125...  Training loss: 0.9298...  0.4932 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3126...  Training loss: 0.9461...  0.4934 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3127...  Training loss: 0.9370...  0.4936 sec/batch\n",
      "Epoch: 92/100...  Training Step: 3128...  Training loss: 0.9149...  0.4953 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3129...  Training loss: 0.9953...  0.4939 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3130...  Training loss: 0.9366...  0.4937 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3131...  Training loss: 0.9335...  0.4943 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3132...  Training loss: 0.9466...  0.4936 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3133...  Training loss: 0.9459...  0.4937 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3134...  Training loss: 0.9265...  0.4937 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3135...  Training loss: 0.9450...  0.4936 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3136...  Training loss: 0.9195...  0.4930 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3137...  Training loss: 0.9071...  0.4938 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3138...  Training loss: 0.9307...  0.4936 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3139...  Training loss: 0.9481...  0.4939 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3140...  Training loss: 0.9415...  0.4941 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3141...  Training loss: 0.9392...  0.4939 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3142...  Training loss: 0.9424...  0.4944 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3143...  Training loss: 0.9336...  0.4933 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3144...  Training loss: 0.9340...  0.4933 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3145...  Training loss: 0.9487...  0.4936 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3146...  Training loss: 0.9328...  0.4934 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3147...  Training loss: 0.9385...  0.4940 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3148...  Training loss: 0.9328...  0.4934 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3149...  Training loss: 0.9488...  0.4936 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3150...  Training loss: 0.9256...  0.4936 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3151...  Training loss: 0.9420...  0.4928 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3152...  Training loss: 0.9288...  0.4933 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3153...  Training loss: 0.9302...  0.4934 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3154...  Training loss: 0.9372...  0.4935 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3155...  Training loss: 0.9128...  0.4943 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3156...  Training loss: 0.9212...  0.4936 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3157...  Training loss: 0.9425...  0.4937 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3158...  Training loss: 0.9286...  0.4943 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3159...  Training loss: 0.9267...  0.4952 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3160...  Training loss: 0.9401...  0.4942 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3161...  Training loss: 0.9446...  0.4937 sec/batch\n",
      "Epoch: 93/100...  Training Step: 3162...  Training loss: 0.9169...  0.4930 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3163...  Training loss: 0.9833...  0.4926 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3164...  Training loss: 0.9213...  0.4948 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3165...  Training loss: 0.9259...  0.4941 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3166...  Training loss: 0.9491...  0.4936 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3167...  Training loss: 0.9337...  0.4936 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3168...  Training loss: 0.9178...  0.4937 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3169...  Training loss: 0.9363...  0.4929 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3170...  Training loss: 0.9240...  0.4938 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3171...  Training loss: 0.9036...  0.4935 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3172...  Training loss: 0.9215...  0.4941 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3173...  Training loss: 0.9261...  0.4948 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3174...  Training loss: 0.9249...  0.4942 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3175...  Training loss: 0.9327...  0.4940 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3176...  Training loss: 0.9421...  0.4931 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3177...  Training loss: 0.9207...  0.4945 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3178...  Training loss: 0.9358...  0.4935 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3179...  Training loss: 0.9467...  0.4937 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3180...  Training loss: 0.9218...  0.4940 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3181...  Training loss: 0.9337...  0.4943 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3182...  Training loss: 0.9169...  0.4939 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3183...  Training loss: 0.9415...  0.4935 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3184...  Training loss: 0.9147...  0.4932 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3185...  Training loss: 0.9432...  0.4952 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3186...  Training loss: 0.9027...  0.4938 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3187...  Training loss: 0.9277...  0.4942 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3188...  Training loss: 0.9216...  0.4943 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3189...  Training loss: 0.9061...  0.4938 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3190...  Training loss: 0.9202...  0.4941 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3191...  Training loss: 0.9271...  0.4939 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3192...  Training loss: 0.9272...  0.4940 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3193...  Training loss: 0.9124...  0.4939 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3194...  Training loss: 0.9404...  0.4936 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3195...  Training loss: 0.9381...  0.4933 sec/batch\n",
      "Epoch: 94/100...  Training Step: 3196...  Training loss: 0.9090...  0.4937 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3197...  Training loss: 0.9844...  0.4936 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3198...  Training loss: 0.9073...  0.4939 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3199...  Training loss: 0.9160...  0.4938 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3200...  Training loss: 0.9390...  0.4929 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3201...  Training loss: 0.9293...  0.4927 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3202...  Training loss: 0.9113...  0.4937 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3203...  Training loss: 0.9166...  0.4941 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3204...  Training loss: 0.9134...  0.4942 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3205...  Training loss: 0.8905...  0.4948 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3206...  Training loss: 0.9249...  0.4944 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3207...  Training loss: 0.9338...  0.4935 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3208...  Training loss: 0.9272...  0.4936 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3209...  Training loss: 0.9182...  0.4938 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3210...  Training loss: 0.9335...  0.4943 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3211...  Training loss: 0.9067...  0.4936 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3212...  Training loss: 0.9387...  0.4941 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3213...  Training loss: 0.9465...  0.4938 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3214...  Training loss: 0.9201...  0.4933 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3215...  Training loss: 0.9348...  0.4941 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3216...  Training loss: 0.9138...  0.4933 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3217...  Training loss: 0.9382...  0.4936 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3218...  Training loss: 0.9203...  0.4931 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3219...  Training loss: 0.9420...  0.4951 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3220...  Training loss: 0.9085...  0.4936 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3221...  Training loss: 0.9251...  0.4945 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3222...  Training loss: 0.9277...  0.4938 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3223...  Training loss: 0.9079...  0.4948 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3224...  Training loss: 0.9131...  0.4936 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3225...  Training loss: 0.9203...  0.4934 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3226...  Training loss: 0.9145...  0.4939 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3227...  Training loss: 0.9043...  0.4935 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3228...  Training loss: 0.9315...  0.4939 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3229...  Training loss: 0.9347...  0.4942 sec/batch\n",
      "Epoch: 95/100...  Training Step: 3230...  Training loss: 0.9108...  0.4938 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3231...  Training loss: 0.9563...  0.4931 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3232...  Training loss: 0.9068...  0.4939 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3233...  Training loss: 0.9106...  0.4938 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3234...  Training loss: 0.9381...  0.4936 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3235...  Training loss: 0.9358...  0.4941 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3236...  Training loss: 0.9012...  0.4947 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3237...  Training loss: 0.9200...  0.4931 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3238...  Training loss: 0.9083...  0.4948 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3239...  Training loss: 0.8930...  0.4945 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3240...  Training loss: 0.9214...  0.4945 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3241...  Training loss: 0.9235...  0.4934 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3242...  Training loss: 0.9143...  0.4946 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3243...  Training loss: 0.9234...  0.4935 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3244...  Training loss: 0.9325...  0.4943 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3245...  Training loss: 0.9173...  0.4944 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3246...  Training loss: 0.9229...  0.4939 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3247...  Training loss: 0.9258...  0.4941 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3248...  Training loss: 0.9049...  0.4955 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3249...  Training loss: 0.9125...  0.4934 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3250...  Training loss: 0.9022...  0.4936 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3251...  Training loss: 0.9200...  0.4943 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3252...  Training loss: 0.9117...  0.4943 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3253...  Training loss: 0.9296...  0.4942 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3254...  Training loss: 0.9050...  0.4935 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3255...  Training loss: 0.9036...  0.4937 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3256...  Training loss: 0.9124...  0.4944 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3257...  Training loss: 0.9095...  0.4937 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3258...  Training loss: 0.9082...  0.4937 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3259...  Training loss: 0.9213...  0.4949 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3260...  Training loss: 0.8926...  0.4940 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3261...  Training loss: 0.9043...  0.4944 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3262...  Training loss: 0.9216...  0.4936 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3263...  Training loss: 0.9299...  0.4940 sec/batch\n",
      "Epoch: 96/100...  Training Step: 3264...  Training loss: 0.8979...  0.4943 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3265...  Training loss: 0.9668...  0.4937 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3266...  Training loss: 0.9029...  0.4942 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3267...  Training loss: 0.8868...  0.4936 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3268...  Training loss: 0.9163...  0.4941 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3269...  Training loss: 0.9179...  0.4936 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3270...  Training loss: 0.8862...  0.4932 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3271...  Training loss: 0.9150...  0.4950 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3272...  Training loss: 0.8944...  0.4939 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3273...  Training loss: 0.8826...  0.4933 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3274...  Training loss: 0.9134...  0.4935 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3275...  Training loss: 0.9219...  0.4934 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3276...  Training loss: 0.9220...  0.4937 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3277...  Training loss: 0.9239...  0.4934 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3278...  Training loss: 0.9239...  0.4943 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3279...  Training loss: 0.8885...  0.4936 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3280...  Training loss: 0.9186...  0.4937 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3281...  Training loss: 0.9370...  0.4940 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3282...  Training loss: 0.9147...  0.4950 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3283...  Training loss: 0.9064...  0.4936 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3284...  Training loss: 0.8987...  0.4939 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3285...  Training loss: 0.9260...  0.4945 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3286...  Training loss: 0.9116...  0.4941 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3287...  Training loss: 0.9278...  0.4938 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3288...  Training loss: 0.9096...  0.4950 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3289...  Training loss: 0.8938...  0.4944 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3290...  Training loss: 0.9110...  0.4940 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3291...  Training loss: 0.8853...  0.4952 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3292...  Training loss: 0.8945...  0.4929 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3293...  Training loss: 0.9092...  0.4941 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3294...  Training loss: 0.9037...  0.4939 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3295...  Training loss: 0.8876...  0.4946 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3296...  Training loss: 0.9221...  0.4940 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3297...  Training loss: 0.9046...  0.4941 sec/batch\n",
      "Epoch: 97/100...  Training Step: 3298...  Training loss: 0.8938...  0.4934 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3299...  Training loss: 0.9584...  0.4932 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3300...  Training loss: 0.8968...  0.4947 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3301...  Training loss: 0.8956...  0.4939 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3302...  Training loss: 0.9105...  0.4951 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3303...  Training loss: 0.9088...  0.4951 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3304...  Training loss: 0.8875...  0.4938 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3305...  Training loss: 0.9087...  0.4941 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3306...  Training loss: 0.8946...  0.4945 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3307...  Training loss: 0.8675...  0.4928 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3308...  Training loss: 0.9049...  0.4943 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3309...  Training loss: 0.9049...  0.4938 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3310...  Training loss: 0.9020...  0.4950 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3311...  Training loss: 0.8988...  0.4947 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3312...  Training loss: 0.9138...  0.4951 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3313...  Training loss: 0.8938...  0.4933 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3314...  Training loss: 0.9091...  0.4938 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3315...  Training loss: 0.9189...  0.4950 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3316...  Training loss: 0.9048...  0.4932 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3317...  Training loss: 0.9038...  0.4936 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3318...  Training loss: 0.8990...  0.4935 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3319...  Training loss: 0.9176...  0.4938 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3320...  Training loss: 0.8787...  0.4942 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3321...  Training loss: 0.9257...  0.4939 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3322...  Training loss: 0.8956...  0.4943 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3323...  Training loss: 0.9005...  0.4934 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3324...  Training loss: 0.9048...  0.4937 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3325...  Training loss: 0.8957...  0.4939 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3326...  Training loss: 0.8903...  0.4945 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3327...  Training loss: 0.9087...  0.4950 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3328...  Training loss: 0.8942...  0.4951 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3329...  Training loss: 0.8882...  0.4943 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3330...  Training loss: 0.9143...  0.4945 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3331...  Training loss: 0.9015...  0.4944 sec/batch\n",
      "Epoch: 98/100...  Training Step: 3332...  Training loss: 0.8873...  0.4928 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3333...  Training loss: 0.9530...  0.4934 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3334...  Training loss: 0.8974...  0.4944 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3335...  Training loss: 0.8868...  0.4938 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3336...  Training loss: 0.9017...  0.4937 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3337...  Training loss: 0.9144...  0.4933 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3338...  Training loss: 0.8813...  0.4935 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3339...  Training loss: 0.8966...  0.4934 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3340...  Training loss: 0.8923...  0.4935 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3341...  Training loss: 0.8664...  0.4938 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3342...  Training loss: 0.8989...  0.4934 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3343...  Training loss: 0.8994...  0.4936 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3344...  Training loss: 0.9057...  0.4935 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3345...  Training loss: 0.9089...  0.4940 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3346...  Training loss: 0.9106...  0.4938 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3347...  Training loss: 0.8958...  0.4938 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3348...  Training loss: 0.9088...  0.4936 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3349...  Training loss: 0.9176...  0.4940 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3350...  Training loss: 0.8907...  0.4939 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3351...  Training loss: 0.9030...  0.4935 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3352...  Training loss: 0.8900...  0.4945 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3353...  Training loss: 0.9012...  0.4938 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3354...  Training loss: 0.8934...  0.4943 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3355...  Training loss: 0.9116...  0.4941 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3356...  Training loss: 0.8958...  0.4946 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3357...  Training loss: 0.9012...  0.4944 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3358...  Training loss: 0.8978...  0.4937 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3359...  Training loss: 0.8809...  0.4935 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3360...  Training loss: 0.8914...  0.4930 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3361...  Training loss: 0.9104...  0.4934 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3362...  Training loss: 0.8902...  0.4939 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3363...  Training loss: 0.8821...  0.4934 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3364...  Training loss: 0.9059...  0.4934 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3365...  Training loss: 0.9037...  0.4932 sec/batch\n",
      "Epoch: 99/100...  Training Step: 3366...  Training loss: 0.8824...  0.4945 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3367...  Training loss: 0.9491...  0.4944 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3368...  Training loss: 0.8717...  0.4934 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3369...  Training loss: 0.8871...  0.4944 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3370...  Training loss: 0.9060...  0.4932 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3371...  Training loss: 0.9004...  0.4953 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3372...  Training loss: 0.8689...  0.4942 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3373...  Training loss: 0.8948...  0.4939 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3374...  Training loss: 0.8907...  0.4939 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3375...  Training loss: 0.8722...  0.4939 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3376...  Training loss: 0.8935...  0.4934 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3377...  Training loss: 0.8945...  0.4936 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3378...  Training loss: 0.8974...  0.4933 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3379...  Training loss: 0.8906...  0.4933 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3380...  Training loss: 0.9053...  0.4932 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3381...  Training loss: 0.8808...  0.4939 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3382...  Training loss: 0.9137...  0.4943 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3383...  Training loss: 0.9132...  0.4941 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3384...  Training loss: 0.8952...  0.4961 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3385...  Training loss: 0.8951...  0.4938 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3386...  Training loss: 0.8828...  0.4929 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3387...  Training loss: 0.9057...  0.4935 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3388...  Training loss: 0.8820...  0.4931 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3389...  Training loss: 0.9071...  0.4935 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3390...  Training loss: 0.8858...  0.4935 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3391...  Training loss: 0.8857...  0.4936 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3392...  Training loss: 0.8977...  0.4933 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3393...  Training loss: 0.8828...  0.4932 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3394...  Training loss: 0.8808...  0.4933 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3395...  Training loss: 0.8866...  0.4934 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3396...  Training loss: 0.8938...  0.4936 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3397...  Training loss: 0.8771...  0.4938 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3398...  Training loss: 0.8966...  0.4934 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3399...  Training loss: 0.8934...  0.4941 sec/batch\n",
      "Epoch: 100/100...  Training Step: 3400...  Training loss: 0.8709...  0.4936 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "save_every_n = 300\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(initial_state)\n",
    "        for x, y in generate_batches(embeddings, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed_dict = {inputs: x,\n",
    "                    targets: y,\n",
    "                    keep_prob: prob,\n",
    "                    initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([loss, \n",
    "                                                 final_state, \n",
    "                                                 optimizer], \n",
    "                                                 feed_dict = feed_dict)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i300_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i900_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1500_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2100_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2700_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3300_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Choosing top n characters for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def select_characters(predictions, vocab_size, top = 5):\n",
    "    probs = np.squeeze(predictions)\n",
    "    probs[np.argsort(probs)[: -top]] = 0\n",
    "    probs = probs / np.sum(probs)\n",
    "    chars = np.random.choice(vocab_size, 1, p=probs)[0]\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Predicting (Sampling) to generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_and_select(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime] \n",
    "    \n",
    "    # During sampling and selecting, we pass  one character at a time\n",
    "    batch_size, num_steps = 1, 1\n",
    "    num_layers = 2\n",
    "    num_classes = len(vocab)\n",
    "    learning_rate = 0.001  \n",
    "    grad_clip = 5\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    ## Getting input tensors\n",
    "    inputs, targets, keep_prob = placeholders(batch_size, num_steps)\n",
    "\n",
    "    ## Creating LSTM Cell\n",
    "    cell, initial_state = lstms(lstm_size, num_layers, batch_size, keep_prob)\n",
    "\n",
    "    ## One hot encode inputs to run the data through RNN layers\n",
    "    one_hot_inputs = tf.one_hot(inputs, num_classes)\n",
    "\n",
    "    ## Collect outputs\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, one_hot_inputs, initial_state = initial_state)\n",
    "    final_state = state\n",
    "\n",
    "    ## Predictions and Logits\n",
    "    prediction, logits = rnn_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "    ## Getting losses and optimizer(ADAM)\n",
    "    loss = losses(logits, targets, lstm_size, num_classes)\n",
    "    optimizer = cal_optimizer(loss, learning_rate, grad_clip)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(initial_state)\n",
    "        \n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {inputs: x,\n",
    "                    keep_prob: 1.,\n",
    "                    initial_state: new_state}\n",
    "            preds, new_state = sess.run([prediction, final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = select_characters(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {inputs: x,\n",
    "                    keep_prob: 1.,\n",
    "                    initial_state: new_state}\n",
    "            preds, new_state = sess.run([prediction, final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = select_characters(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3400_l512.ckpt'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a few weeks to be troll on the third floor. He was starting to return the cloak on the fire. The small reached a parser, a hore, but before Filch spat and say inside. \n",
      "\n",
      "\"There's a note -- shot yeh to go?\" \n",
      "\n",
      "Harry and Ron with a bag in the tirle's back by anyence. \n",
      "\n",
      "And thene, the boy mad his newspaper by his suddenly hurs, all sorrd, the studing wind smiled. \n",
      "\n",
      "\"I heard it -- look --\" \n",
      "\n",
      "\"You could have mind her to do anyone else -- the counter stray him by a classroom. He had been pressed on anything.\" \n",
      "\n",
      "Harry saw the deak shoppes. \n",
      "\n",
      "\"Should have been being in the arrsadd to speed to tall him in front.\" \n",
      "\n",
      "\"What about that, I'll guarding you.\" \n",
      "\n",
      "A letter with the back in the corred of Harry's but bot of second, which was still glasing at him, and a whispered. \n",
      "\n",
      "Harry thought this was standing around instead, of course, the best greated was was moved to sel him from by here. He was looking at his seem. Harry had sand, \"It wouldn't know what to do with the Dursleys, he said, He had see a large probembre and silver teles yet wishall the sam Hagrid;s they still dained the card even forget that all. \n",
      "\n",
      "When Mrsain Mrs. Drooss, Mr bally had almost glanced at Harry, but Ron had seen her with his feet, but Harry had gone to Harry. \"There's so magic that I'd leave it to catch the library. A sudden silver hig hand on you looke -- he shouldn't be too madly from the Gryffindor than, I might he say on? It wust a bot ow not and not turn about Hogwarts -- in the entranded on the next morning. What an' I still gateded?\" \n",
      "\n",
      "\"No, I sep t in,\" he said in a tine pink cluspered. \"Harry Potter, don't you all balk over the Stone!\" \n",
      "\n",
      "Their sind was a hoget by. It wasn't the other two from the catching alout this sitning look, and he started to sell a large postow while they could tell them, who was still standing on his face; they came the noise, excitedly clearly that Harry's school back in his shoulder at Ron. \n",
      "\n",
      "\"I was thenking his freezy,\" said Ron, thought hap nearly titeled at Hagrid. \n",
      "\n",
      "\"See\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample_and_select(checkpoint, 2000, lstm_size, len(vocab), prime='It was a ')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a few leaver that. \n",
      "\n",
      "\"You drink on the train? I've never get it.\" \n",
      "\n",
      "\"Give me a loud, I'm not try only going?\" \n",
      "\n",
      "\"Yes,\" said Ron. \n",
      "\n",
      "\"I say he's got how to ktop yer late?\" Hagrid said, \"Shaghim,\" Harry said. \n",
      "\n",
      "\"You've been sayin' an owd?\" He gaspping at the stander book. \n",
      "\n",
      "\"He's lott his protecting to do with you, that same yes?\" \n",
      "\n",
      "\"Wonder, I'm going to do you than?\" \n",
      "\n",
      "\"What happen't this in the moment,\" he began to think twere any most moment as this start something lestens bowe was a surprise to but Hermione Granger had sighe. The seat had started and still himself into his cloak, but he was take the boy was a few seconds and still had a boit of stratgers with the gable. Hagrid gave a second later, but harry thought he had gone on the still. \n",
      "\n",
      "\"Yes,, he's a shapp pick to get hurryed up the corner of them, who don't something to go to Harry.\" \n",
      "\n",
      "\"Why were to something,\" said Harry. \"Yeh, we're not seen that the points he's going to be an ord.\" \n",
      "\n",
      "He looked looking at another tood to the forest flames. \n",
      "\n",
      "\"The fealls thile out the pack goin' the wands to the first time. It's a few sort of that more than.\" \n",
      "\n",
      "Harry took out their first to because they'd go how he doone, but he said her was almost as so though he was going to go on Hermione Gogger, whe which he would have back to the cheskmen to stay through the corner to the moonlight and the Wood was almost stearing that a sintle pait of the catch and with one of the class, which was a large pile of pointed at a great, staring at hem a more pleased at the table in a cloak. The stood sprained: Harry he had seen it, and the letter wanter to get the steeps, because his centlaused had thought. It was a bit more strange that the Dursleys could think an owl and stranged into his car and hudred, but the people could steal himself in times. The points of good was footing that with the flien start teams to the boys's like beotwe cold, what there were stopped through the only one where he was going to be seen't missakey as a someth\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i2700_l512.ckpt'\n",
    "samp = sample_and_select(checkpoint, 2000, lstm_size, len(vocab), prime=\"It was\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the teachers whe wan to be about to trind to see the flating out of a start for a more to see it to bit on Harry his head, but Harry. Hagrid sweeped to his fant. He'd then wouldn't be allare that was a standed for andone they had that the troll stumbned. \n",
      "\n",
      "\"Hagrid!\" she was said that they started, something about the the stand of his brink. Harry whispered to him forthed. He was seeming to be to trist the trush and shilly started to straight her that they went onto the conntry her take, and Harry was a boitt boy of his bright, ship han than his fingers of them was she stick, harring him as the big on the start of the cat on the firelots shaded as the boy was shating his found, but Hermione were sayed. \n",
      "\n",
      "\"See?\" he said. \"I don't trought heard to be in the cold the fores --\" \n",
      "\n",
      "Harry. Hagrid was soired, and there was stunding to the chowe to the both she and his later to the sturing with Hagrid the seat of they crunk and told him when they shived as he was a bit of a boy a bit on the stood and stragget into that to a seckne somewhite shook and he had been a stratched back of the foreat of his feet on the sat of the way. He hud an and that were to they were said, and her wondering though the dark way so her wand. The tran scratged onto the forst to them, but Harry wouldn't to than they weren all the one of them turned a for blew, whates the what the train started and still stire on the door and all about a fightairs and started and had tround not a bit broag ticked to the bight boy and the game a walked for the forest week and sat his hair tear they saw, too. \n",
      "\n",
      "Harry turned the tory of him. They had said about their should see his hand with a tilk again the told where had ther thing this morther. \n",
      "\n",
      "He thought they didn't to cang out a little boy one their with and the story tail to sig though he didn't know. He tulled a big about. Harry started to staying and stopped and still clicked itto a contentior, where they were saying in the top him they spint at astle, but the \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1500_l512.ckpt'\n",
    "samp = sample_and_select(checkpoint, 2000, lstm_size, len(vocab), prime=\"It was\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was sall ald ond at he was tin with sitk and worly were thourle to bet the thoud, the wis to tout on was salle thim has sor to tout he ald thering sat sing an wished tithed, but how his tored. \n",
      "\n",
      "Ant to tand war sherise tere win her tally thand thing, tore and.\" \n",
      "\n",
      "\"Harry he sould of thine. Ther whas stin ghe pilking what ard and whe he and thene to bere ther hing. \"Was ha the tho chas hind and the pasteren. I wal an the thit wardes was ane har wored tome bothed him aredes there sit tones was ha the pelined touther an what and and wat tite ha corle aldering sours to than simested and. \n",
      "\n",
      "\"Wat war had seedile stoung of hith he tore the thand allent wim stat inght. \n",
      "\n",
      "\"Yus tat ant his, steing, ber at ass ot wint,\" seis as ind the tol ander tom har stoud be ald has inged and thing ald and wast the card an hourd wont the tha tas. \n",
      "\n",
      "\"They and an the wort he the pat them the sallist, has at whast at at at withe tor. I wath wall see hem. Hearr the thint the chis and to the had thene hed hing him he wast. \n",
      "\n",
      "\"You soud the wat he the west an win ther tim. \n",
      "\n",
      "\"Nit at sary tor hiss at and toule to selly them ast the chear one has an the simes tind, win he walld thoung had the pare to har salde had wiss that he tile aned thenerer tare sad had. \n",
      "\n",
      "Them soore at aly ale hore. Th whass wos tha ke wout the coust. \"\n",
      "He wad was the past ind the whis the was ind the ceroll th momthar thar sadd the whorses at and who the was and the then won the custhan to had saly,\" siid Harry seed.\"\"I s aid soud thist to sto boun to steald. \n",
      "\n",
      "He rat had the to cound, the heare so at wall the wat he and to hist of ther. \n",
      "\n",
      "\"Shas he sould ande was had the the wats ha dorind the wats to touns them thing the wass they with as he soud at ant tone hist, the the pusted the wint to hast the wat arlod and the has thenes ther whand sat if tha kich of wis ar ath wald to bo kit to the heren he whe tard the hedsend wean thar sored hour hise hes ard hom anly. \n",
      "\n",
      "\"Thas the there tom his has ind ant hit ary atre head the sim on \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i300_l512.ckpt'\n",
    "samp = sample_and_select(checkpoint, 2000, lstm_size, len(vocab), prime=\"It was\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
