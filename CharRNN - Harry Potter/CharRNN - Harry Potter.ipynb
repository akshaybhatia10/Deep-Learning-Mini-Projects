{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharRNN - Harry Potter\n",
    "\n",
    "Character wise RNN to generate new text based on Harry Potter Books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text file and convert it into integers using dictionaries to convert the characters to and from integers. We will only use the 1st book Harry Potter and Sorcerer's Stone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample text from the book -->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('HPBook1.txt', 'r')\n",
    "text = text.read()\n",
    "print('Some sample text from the book -->')\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in book : 78449\n",
      "Total characters in book : 442744\n",
      "Unique words in book : 11897\n",
      "Unique characters in book : 80\n"
     ]
    }
   ],
   "source": [
    "total_words = len(text.split())\n",
    "total_characters = len(text)\n",
    "unique_words = len(set(text.split()))\n",
    "unique_characters = len(set(text))\n",
    "\n",
    "print (\"Total words in book :\", total_words)\n",
    "print (\"Total characters in book :\", total_characters)\n",
    "print (\"Unique words in book :\", unique_words)\n",
    "print (\"Unique characters in book :\", unique_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Paragraphs : 3033\n"
     ]
    }
   ],
   "source": [
    "paragraphs = text.split('\\n\\n')\n",
    "print (\"Total Paragraphs :\", len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_words = Counter()\n",
    "for i in range(len(paragraphs)):\n",
    "    for x in paragraphs[i]:\n",
    "        most_words[x] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 78449),\n",
       " ('e', 39628),\n",
       " ('t', 27993),\n",
       " ('a', 25887),\n",
       " ('o', 25809),\n",
       " ('n', 21337),\n",
       " ('r', 20990),\n",
       " ('h', 19535),\n",
       " ('i', 19422),\n",
       " ('s', 18870),\n",
       " ('d', 15932),\n",
       " ('l', 14385),\n",
       " ('u', 9562),\n",
       " ('y', 8293),\n",
       " ('g', 8127),\n",
       " ('w', 7744),\n",
       " ('m', 6729),\n",
       " ('f', 6431),\n",
       " ('c', 6403),\n",
       " ('.', 6136),\n",
       " (',', 5658),\n",
       " ('b', 4980),\n",
       " ('p', 4909),\n",
       " ('\"', 4747),\n",
       " ('k', 3930),\n",
       " (\"'\", 3141),\n",
       " ('H', 2996),\n",
       " ('v', 2716),\n",
       " ('-', 1986),\n",
       " ('I', 1393),\n",
       " ('T', 1055),\n",
       " ('S', 844),\n",
       " ('?', 754),\n",
       " ('A', 703),\n",
       " ('D', 685),\n",
       " ('M', 665),\n",
       " ('R', 660),\n",
       " ('W', 653),\n",
       " ('P', 639),\n",
       " ('G', 492),\n",
       " ('N', 488),\n",
       " ('!', 474),\n",
       " ('F', 426),\n",
       " ('x', 381),\n",
       " ('B', 348),\n",
       " ('O', 332),\n",
       " ('Y', 326),\n",
       " ('j', 319),\n",
       " ('C', 293),\n",
       " ('E', 287),\n",
       " ('z', 259),\n",
       " ('q', 217),\n",
       " ('L', 209),\n",
       " ('Q', 203),\n",
       " ('U', 193),\n",
       " ('V', 192),\n",
       " (';', 135),\n",
       " ('K', 79),\n",
       " (':', 69),\n",
       " ('J', 51),\n",
       " (')', 33),\n",
       " ('(', 30),\n",
       " ('“', 11),\n",
       " ('1', 11),\n",
       " ('3', 8),\n",
       " ('4', 6),\n",
       " ('Z', 5),\n",
       " ('0', 5),\n",
       " ('7', 4),\n",
       " ('9', 4),\n",
       " ('2', 3),\n",
       " ('X', 2),\n",
       " ('5', 2),\n",
       " ('*', 2),\n",
       " ('–', 1),\n",
       " ('~', 1),\n",
       " ('8', 1),\n",
       " ('6', 1),\n",
       " ('\\\\', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '\\\\',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '~',\n",
       " '–',\n",
       " '“'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c : i for i,c in enumerate(vocab)}\n",
    "#int_to_vocab = {i : c for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "embeddings = np.array([vocab_to_int[i] for i in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '?': 23,\n",
       " 'A': 24,\n",
       " 'B': 25,\n",
       " 'C': 26,\n",
       " 'D': 27,\n",
       " 'E': 28,\n",
       " 'F': 29,\n",
       " 'G': 30,\n",
       " 'H': 31,\n",
       " 'I': 32,\n",
       " 'J': 33,\n",
       " 'K': 34,\n",
       " 'L': 35,\n",
       " 'M': 36,\n",
       " 'N': 37,\n",
       " 'O': 38,\n",
       " 'P': 39,\n",
       " 'Q': 40,\n",
       " 'R': 41,\n",
       " 'S': 42,\n",
       " 'T': 43,\n",
       " 'U': 44,\n",
       " 'V': 45,\n",
       " 'W': 46,\n",
       " 'X': 47,\n",
       " 'Y': 48,\n",
       " 'Z': 49,\n",
       " '\\\\': 50,\n",
       " 'a': 51,\n",
       " 'b': 52,\n",
       " 'c': 53,\n",
       " 'd': 54,\n",
       " 'e': 55,\n",
       " 'f': 56,\n",
       " 'g': 57,\n",
       " 'h': 58,\n",
       " 'i': 59,\n",
       " 'j': 60,\n",
       " 'k': 61,\n",
       " 'l': 62,\n",
       " 'm': 63,\n",
       " 'n': 64,\n",
       " 'o': 65,\n",
       " 'p': 66,\n",
       " 'q': 67,\n",
       " 'r': 68,\n",
       " 's': 69,\n",
       " 't': 70,\n",
       " 'u': 71,\n",
       " 'v': 72,\n",
       " 'w': 73,\n",
       " 'x': 74,\n",
       " 'y': 75,\n",
       " 'z': 76,\n",
       " '~': 77,\n",
       " '–': 78,\n",
       " '“': 79}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples encodings (inputs to our NN) -->>\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet D\n",
      "[31 51 68 68 75  1 39 65 70 70 55 68  1 51 64 54  1 70 58 55  1 42 65 68 53\n",
      " 55 68 55 68  4 69  1 42 70 65 64 55  1  0  0 26 31 24 39 43 28 41  1 38 37\n",
      " 28  1  0  0 43 31 28  1 25 38 48  1 46 31 38  1 35 32 45 28 27  1  0  0 36\n",
      " 68 10  1 51 64 54  1 36 68 69 10  1 27 71 68 69 62 55 75  8  1 65 56  1 64\n",
      " 71 63 52 55 68  1 56 65 71 68  8  1 39 68 59 72 55 70  1 27]\n"
     ]
    }
   ],
   "source": [
    "print (\"Examples encodings (inputs to our NN) -->>\")\n",
    "print (text[:120])\n",
    "print (embeddings[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with individual characters, it's a classification problem in which we are trying to predict the next character from the previous text. Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining placeholders for inputs, targets, learning rate and dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholders(batch_size, step_size):\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size, step_size), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size, step_size), name='targets')\n",
    "    #lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name= 'prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating LSTM cells for our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstms(lstm_size, layers, batch_size, keep_prob):\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(layers)])\n",
    "    state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn_output(lstm_output, input_size, output_size):\n",
    "    sequence = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(sequence, [-1, input_size])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        weights = tf.Variable(tf.truncated_normal(shape=(input_size, output_size), stddev=0.1))\n",
    "        bias = tf.Variable(tf.zeros(output_size))\n",
    "        \n",
    "    logits = tf.matmul(x, weights) + bias\n",
    "    predictions = tf.nn.softmax(logits, name='predictions')                          \n",
    "    \n",
    "    return predictions, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualting loss for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def losses(logits, y, lstm_size, num_classes):\n",
    "    labels = tf.reshape(tf.one_hot(y, num_classes), logits.get_shape())\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_steps = 50        \n",
    "lstm_size = 128 \n",
    "num_layers = 2         \n",
    "learning_rate = 0.001  \n",
    "prob = 0.5 \n",
    "grad_clip = 5\n",
    "num_classes = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating batches to feed into NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(inputs, batch_size, num_steps):\n",
    "    char_batch = batch_size * num_steps\n",
    "    num_batches = len(inputs)//char_batch\n",
    "    \n",
    "    idx = char_batch * num_batches \n",
    "    inputs = inputs[:idx]\n",
    "    inputs = inputs.reshape((batch_size, -1))\n",
    "    \n",
    "    for i in range(0, inputs.shape[1], num_steps):\n",
    "        x = inputs[:, i : i+num_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = generate_batches(embeddings, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 51 68 68 75  1 39 65 70 70]\n",
      " [ 1 69 66 65 61 55  1 51 52 65]\n",
      " [70 69  1 52 55 51 61 10  1  0]\n",
      " [64 65 69 55 10  1  0  0  3 48]\n",
      " [70 58 55 59 68  1 56 51 53 55]\n",
      " [56 65 75 69  1 69 64 55 55 68]\n",
      " [64 70  1 57 51 59 64 69  1 70]\n",
      " [ 1 51 64 65 70 58 55 68  1 68]\n",
      " [51 68 54  1 58 59 63  1 51 64]\n",
      " [65 62 65 57 75  8  1 31 55 68]]\n",
      "\n",
      "y\n",
      " [[51 68 68 75  1 39 65 70 70 55]\n",
      " [69 66 65 61 55  1 51 52 65 71]\n",
      " [69  1 52 55 51 61 10  1  0  0]\n",
      " [65 69 55 10  1  0  0  3 48 55]\n",
      " [58 55 59 68  1 56 51 53 55 69]\n",
      " [65 75 69  1 69 64 55 55 68 59]\n",
      " [70  1 57 51 59 64 69  1 70 58]\n",
      " [51 64 65 70 58 55 68  1 68 55]\n",
      " [68 54  1 58 59 63  1 51 64 54]\n",
      " [62 65 57 75  8  1 31 55 68 63]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer and fixing exploding gradient problem by clipping beyond a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_optimizer(loss, learning_rate, clipping):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), clipping)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up all variables and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## Getting input tensors\n",
    "inputs, targets, keep_prob = placeholders(batch_size, num_steps)\n",
    "\n",
    "## Creating LSTM Cell\n",
    "cell, initial_state = lstms(lstm_size, num_layers, batch_size, keep_prob)\n",
    "\n",
    "## One hot encode inputs to run the data through RNN layers\n",
    "one_hot_inputs = tf.one_hot(inputs, num_classes)\n",
    "\n",
    "## Collect outputs\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, one_hot_inputs, initial_state = initial_state)\n",
    "final_state = state\n",
    "\n",
    "## Predictions and Logits\n",
    "predictions, logits = rnn_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "## Getting losses and optimizer(ADAM)\n",
    "loss = losses(logits, targets, lstm_size, num_classes)\n",
    "optimizer = cal_optimizer(loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1...  Training Step: 1...  Training loss: 4.3837...  0.3658 sec/batch\n",
      "Epoch: 1/1...  Training Step: 2...  Training loss: 4.3693...  0.2659 sec/batch\n",
      "Epoch: 1/1...  Training Step: 3...  Training loss: 4.3496...  0.2857 sec/batch\n",
      "Epoch: 1/1...  Training Step: 4...  Training loss: 4.3212...  0.2787 sec/batch\n",
      "Epoch: 1/1...  Training Step: 5...  Training loss: 4.2757...  0.2832 sec/batch\n",
      "Epoch: 1/1...  Training Step: 6...  Training loss: 4.1771...  0.4035 sec/batch\n",
      "Epoch: 1/1...  Training Step: 7...  Training loss: 4.0143...  0.3118 sec/batch\n",
      "Epoch: 1/1...  Training Step: 8...  Training loss: 3.8255...  0.2695 sec/batch\n",
      "Epoch: 1/1...  Training Step: 9...  Training loss: 3.6912...  0.3325 sec/batch\n",
      "Epoch: 1/1...  Training Step: 10...  Training loss: 3.5966...  0.3020 sec/batch\n",
      "Epoch: 1/1...  Training Step: 11...  Training loss: 3.5142...  0.2702 sec/batch\n",
      "Epoch: 1/1...  Training Step: 12...  Training loss: 3.5075...  0.2715 sec/batch\n",
      "Epoch: 1/1...  Training Step: 13...  Training loss: 3.4371...  0.2732 sec/batch\n",
      "Epoch: 1/1...  Training Step: 14...  Training loss: 3.4503...  0.2736 sec/batch\n",
      "Epoch: 1/1...  Training Step: 15...  Training loss: 3.3723...  0.2793 sec/batch\n",
      "Epoch: 1/1...  Training Step: 16...  Training loss: 3.4059...  0.2697 sec/batch\n",
      "Epoch: 1/1...  Training Step: 17...  Training loss: 3.3852...  0.2771 sec/batch\n",
      "Epoch: 1/1...  Training Step: 18...  Training loss: 3.3525...  0.2731 sec/batch\n",
      "Epoch: 1/1...  Training Step: 19...  Training loss: 3.3456...  0.2811 sec/batch\n",
      "Epoch: 1/1...  Training Step: 20...  Training loss: 3.3396...  0.2722 sec/batch\n",
      "Epoch: 1/1...  Training Step: 21...  Training loss: 3.3304...  0.2582 sec/batch\n",
      "Epoch: 1/1...  Training Step: 22...  Training loss: 3.3473...  0.2844 sec/batch\n",
      "Epoch: 1/1...  Training Step: 23...  Training loss: 3.3616...  0.2949 sec/batch\n",
      "Epoch: 1/1...  Training Step: 24...  Training loss: 3.3003...  0.2677 sec/batch\n",
      "Epoch: 1/1...  Training Step: 25...  Training loss: 3.3046...  0.2718 sec/batch\n",
      "Epoch: 1/1...  Training Step: 26...  Training loss: 3.2970...  0.2576 sec/batch\n",
      "Epoch: 1/1...  Training Step: 27...  Training loss: 3.3261...  0.3188 sec/batch\n",
      "Epoch: 1/1...  Training Step: 28...  Training loss: 3.3412...  0.3986 sec/batch\n",
      "Epoch: 1/1...  Training Step: 29...  Training loss: 3.3043...  0.2971 sec/batch\n",
      "Epoch: 1/1...  Training Step: 30...  Training loss: 3.3295...  0.3689 sec/batch\n",
      "Epoch: 1/1...  Training Step: 31...  Training loss: 3.3226...  0.2926 sec/batch\n",
      "Epoch: 1/1...  Training Step: 32...  Training loss: 3.2803...  0.2761 sec/batch\n",
      "Epoch: 1/1...  Training Step: 33...  Training loss: 3.2847...  0.2798 sec/batch\n",
      "Epoch: 1/1...  Training Step: 34...  Training loss: 3.2982...  0.2859 sec/batch\n",
      "Epoch: 1/1...  Training Step: 35...  Training loss: 3.2836...  0.2700 sec/batch\n",
      "Epoch: 1/1...  Training Step: 36...  Training loss: 3.2701...  0.2923 sec/batch\n",
      "Epoch: 1/1...  Training Step: 37...  Training loss: 3.2942...  0.2946 sec/batch\n",
      "Epoch: 1/1...  Training Step: 38...  Training loss: 3.2559...  0.3008 sec/batch\n",
      "Epoch: 1/1...  Training Step: 39...  Training loss: 3.2490...  0.2659 sec/batch\n",
      "Epoch: 1/1...  Training Step: 40...  Training loss: 3.2343...  0.2612 sec/batch\n",
      "Epoch: 1/1...  Training Step: 41...  Training loss: 3.2724...  0.3156 sec/batch\n",
      "Epoch: 1/1...  Training Step: 42...  Training loss: 3.2486...  0.2981 sec/batch\n",
      "Epoch: 1/1...  Training Step: 43...  Training loss: 3.1959...  0.2971 sec/batch\n",
      "Epoch: 1/1...  Training Step: 44...  Training loss: 3.2428...  0.3088 sec/batch\n",
      "Epoch: 1/1...  Training Step: 45...  Training loss: 3.2238...  0.3336 sec/batch\n",
      "Epoch: 1/1...  Training Step: 46...  Training loss: 3.2555...  0.2701 sec/batch\n",
      "Epoch: 1/1...  Training Step: 47...  Training loss: 3.2739...  0.2703 sec/batch\n",
      "Epoch: 1/1...  Training Step: 48...  Training loss: 3.2685...  0.3199 sec/batch\n",
      "Epoch: 1/1...  Training Step: 49...  Training loss: 3.2310...  0.3217 sec/batch\n",
      "Epoch: 1/1...  Training Step: 50...  Training loss: 3.2221...  0.2903 sec/batch\n",
      "Epoch: 1/1...  Training Step: 51...  Training loss: 3.2399...  0.2779 sec/batch\n",
      "Epoch: 1/1...  Training Step: 52...  Training loss: 3.2504...  0.2936 sec/batch\n",
      "Epoch: 1/1...  Training Step: 53...  Training loss: 3.2697...  0.2788 sec/batch\n",
      "Epoch: 1/1...  Training Step: 54...  Training loss: 3.2197...  0.2790 sec/batch\n",
      "Epoch: 1/1...  Training Step: 55...  Training loss: 3.2248...  0.2847 sec/batch\n",
      "Epoch: 1/1...  Training Step: 56...  Training loss: 3.2417...  0.2838 sec/batch\n",
      "Epoch: 1/1...  Training Step: 57...  Training loss: 3.2077...  0.2711 sec/batch\n",
      "Epoch: 1/1...  Training Step: 58...  Training loss: 3.2494...  0.2853 sec/batch\n",
      "Epoch: 1/1...  Training Step: 59...  Training loss: 3.2743...  0.2734 sec/batch\n",
      "Epoch: 1/1...  Training Step: 60...  Training loss: 3.2940...  0.2952 sec/batch\n",
      "Epoch: 1/1...  Training Step: 61...  Training loss: 3.2331...  0.3054 sec/batch\n",
      "Epoch: 1/1...  Training Step: 62...  Training loss: 3.2212...  0.3284 sec/batch\n",
      "Epoch: 1/1...  Training Step: 63...  Training loss: 3.2398...  0.2787 sec/batch\n",
      "Epoch: 1/1...  Training Step: 64...  Training loss: 3.2348...  0.3204 sec/batch\n",
      "Epoch: 1/1...  Training Step: 65...  Training loss: 3.2139...  0.2757 sec/batch\n",
      "Epoch: 1/1...  Training Step: 66...  Training loss: 3.2391...  0.2742 sec/batch\n",
      "Epoch: 1/1...  Training Step: 67...  Training loss: 3.2272...  0.3153 sec/batch\n",
      "Epoch: 1/1...  Training Step: 68...  Training loss: 3.2025...  0.2906 sec/batch\n",
      "Epoch: 1/1...  Training Step: 69...  Training loss: 3.2413...  0.3200 sec/batch\n",
      "Epoch: 1/1...  Training Step: 70...  Training loss: 3.2167...  0.2724 sec/batch\n",
      "Epoch: 1/1...  Training Step: 71...  Training loss: 3.1898...  0.2950 sec/batch\n",
      "Epoch: 1/1...  Training Step: 72...  Training loss: 3.2722...  0.3059 sec/batch\n",
      "Epoch: 1/1...  Training Step: 73...  Training loss: 3.1968...  0.2802 sec/batch\n",
      "Epoch: 1/1...  Training Step: 74...  Training loss: 3.1941...  0.3220 sec/batch\n",
      "Epoch: 1/1...  Training Step: 75...  Training loss: 3.1785...  0.2746 sec/batch\n",
      "Epoch: 1/1...  Training Step: 76...  Training loss: 3.2311...  0.2689 sec/batch\n",
      "Epoch: 1/1...  Training Step: 77...  Training loss: 3.2305...  0.3241 sec/batch\n",
      "Epoch: 1/1...  Training Step: 78...  Training loss: 3.2255...  0.2752 sec/batch\n",
      "Epoch: 1/1...  Training Step: 79...  Training loss: 3.2342...  0.3364 sec/batch\n",
      "Epoch: 1/1...  Training Step: 80...  Training loss: 3.2374...  0.2727 sec/batch\n",
      "Epoch: 1/1...  Training Step: 81...  Training loss: 3.2297...  0.2880 sec/batch\n",
      "Epoch: 1/1...  Training Step: 82...  Training loss: 3.2344...  0.3417 sec/batch\n",
      "Epoch: 1/1...  Training Step: 83...  Training loss: 3.2115...  0.3603 sec/batch\n",
      "Epoch: 1/1...  Training Step: 84...  Training loss: 3.1980...  0.3187 sec/batch\n",
      "Epoch: 1/1...  Training Step: 85...  Training loss: 3.2066...  0.2808 sec/batch\n",
      "Epoch: 1/1...  Training Step: 86...  Training loss: 3.1860...  0.3121 sec/batch\n",
      "Epoch: 1/1...  Training Step: 87...  Training loss: 3.1920...  0.2690 sec/batch\n",
      "Epoch: 1/1...  Training Step: 88...  Training loss: 3.2066...  0.2675 sec/batch\n",
      "Epoch: 1/1...  Training Step: 89...  Training loss: 3.1975...  0.2911 sec/batch\n",
      "Epoch: 1/1...  Training Step: 90...  Training loss: 3.2026...  0.2703 sec/batch\n",
      "Epoch: 1/1...  Training Step: 91...  Training loss: 3.2402...  0.2754 sec/batch\n",
      "Epoch: 1/1...  Training Step: 92...  Training loss: 3.1911...  0.2683 sec/batch\n",
      "Epoch: 1/1...  Training Step: 93...  Training loss: 3.2471...  0.2648 sec/batch\n",
      "Epoch: 1/1...  Training Step: 94...  Training loss: 3.1787...  0.2754 sec/batch\n",
      "Epoch: 1/1...  Training Step: 95...  Training loss: 3.1741...  0.2670 sec/batch\n",
      "Epoch: 1/1...  Training Step: 96...  Training loss: 3.1696...  0.2677 sec/batch\n",
      "Epoch: 1/1...  Training Step: 97...  Training loss: 3.1959...  0.2720 sec/batch\n",
      "Epoch: 1/1...  Training Step: 98...  Training loss: 3.2069...  0.3051 sec/batch\n",
      "Epoch: 1/1...  Training Step: 99...  Training loss: 3.2350...  0.3181 sec/batch\n",
      "Epoch: 1/1...  Training Step: 100...  Training loss: 3.1822...  0.2929 sec/batch\n",
      "Epoch: 1/1...  Training Step: 101...  Training loss: 3.1984...  0.2937 sec/batch\n",
      "Epoch: 1/1...  Training Step: 102...  Training loss: 3.2003...  0.2921 sec/batch\n",
      "Epoch: 1/1...  Training Step: 103...  Training loss: 3.2072...  0.2701 sec/batch\n",
      "Epoch: 1/1...  Training Step: 104...  Training loss: 3.1738...  0.2916 sec/batch\n",
      "Epoch: 1/1...  Training Step: 105...  Training loss: 3.1752...  0.2925 sec/batch\n",
      "Epoch: 1/1...  Training Step: 106...  Training loss: 3.1953...  0.2915 sec/batch\n",
      "Epoch: 1/1...  Training Step: 107...  Training loss: 3.1738...  0.2769 sec/batch\n",
      "Epoch: 1/1...  Training Step: 108...  Training loss: 3.1923...  0.3217 sec/batch\n",
      "Epoch: 1/1...  Training Step: 109...  Training loss: 3.2660...  0.2749 sec/batch\n",
      "Epoch: 1/1...  Training Step: 110...  Training loss: 3.1543...  0.2717 sec/batch\n",
      "Epoch: 1/1...  Training Step: 111...  Training loss: 3.1745...  0.3295 sec/batch\n",
      "Epoch: 1/1...  Training Step: 112...  Training loss: 3.1774...  0.2700 sec/batch\n",
      "Epoch: 1/1...  Training Step: 113...  Training loss: 3.1542...  0.3169 sec/batch\n",
      "Epoch: 1/1...  Training Step: 114...  Training loss: 3.1564...  0.2769 sec/batch\n",
      "Epoch: 1/1...  Training Step: 115...  Training loss: 3.2596...  0.2747 sec/batch\n",
      "Epoch: 1/1...  Training Step: 116...  Training loss: 3.1589...  0.2742 sec/batch\n",
      "Epoch: 1/1...  Training Step: 117...  Training loss: 3.1842...  0.2561 sec/batch\n",
      "Epoch: 1/1...  Training Step: 118...  Training loss: 3.1727...  0.2935 sec/batch\n",
      "Epoch: 1/1...  Training Step: 119...  Training loss: 3.2108...  0.3405 sec/batch\n",
      "Epoch: 1/1...  Training Step: 120...  Training loss: 3.1264...  0.2773 sec/batch\n",
      "Epoch: 1/1...  Training Step: 121...  Training loss: 3.1910...  0.2652 sec/batch\n",
      "Epoch: 1/1...  Training Step: 122...  Training loss: 3.1736...  0.2783 sec/batch\n",
      "Epoch: 1/1...  Training Step: 123...  Training loss: 3.1386...  0.2683 sec/batch\n",
      "Epoch: 1/1...  Training Step: 124...  Training loss: 3.1572...  0.2693 sec/batch\n",
      "Epoch: 1/1...  Training Step: 125...  Training loss: 3.1596...  0.3605 sec/batch\n",
      "Epoch: 1/1...  Training Step: 126...  Training loss: 3.1667...  0.3307 sec/batch\n",
      "Epoch: 1/1...  Training Step: 127...  Training loss: 3.1830...  0.2863 sec/batch\n",
      "Epoch: 1/1...  Training Step: 128...  Training loss: 3.1906...  0.2721 sec/batch\n",
      "Epoch: 1/1...  Training Step: 129...  Training loss: 3.1912...  0.3211 sec/batch\n",
      "Epoch: 1/1...  Training Step: 130...  Training loss: 3.1781...  0.2717 sec/batch\n",
      "Epoch: 1/1...  Training Step: 131...  Training loss: 3.1461...  0.3167 sec/batch\n",
      "Epoch: 1/1...  Training Step: 132...  Training loss: 3.1684...  0.2688 sec/batch\n",
      "Epoch: 1/1...  Training Step: 133...  Training loss: 3.1871...  0.2727 sec/batch\n",
      "Epoch: 1/1...  Training Step: 134...  Training loss: 3.1555...  0.2868 sec/batch\n",
      "Epoch: 1/1...  Training Step: 135...  Training loss: 3.1688...  0.2801 sec/batch\n",
      "Epoch: 1/1...  Training Step: 136...  Training loss: 3.1425...  0.2730 sec/batch\n",
      "Epoch: 1/1...  Training Step: 137...  Training loss: 3.2068...  0.2824 sec/batch\n",
      "Epoch: 1/1...  Training Step: 138...  Training loss: 3.1439...  0.2677 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 10\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(initial_state)\n",
    "        for x, y in generate_batches(embeddings, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed_dict = {inputs: x,\n",
    "                    targets: y,\n",
    "                    keep_prob: prob,\n",
    "                    initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([loss, \n",
    "                                                 final_state, \n",
    "                                                 optimizer], \n",
    "                                                 feed_dict = feed_dict)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i138_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i10_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i40_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i50_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i60_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i70_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i80_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i90_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i100_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i110_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i120_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i130_l128.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i138_l128.ckpt\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing top n characters for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_characters(predictions, vocab_size, top = 5):\n",
    "    probs = np.squeeze(predictions)\n",
    "    probs[np.argsort(probs)[: -top]] = 0\n",
    "    probs = probs / np.sum(probs)\n",
    "    chars = np.random.choice(vocab_size, 1, p=probs)[0]\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting (Sampling) to generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_and_select(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime] \n",
    "    \n",
    "    # During sampling and selecting, we pass  one character at a time\n",
    "    batch_size, num_steps = 1, 1\n",
    "    num_layers = 2\n",
    "    num_classes = len(vocab)\n",
    "    learning_rate = 0.001  \n",
    "    grad_clip = 5\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    ## Getting input tensors\n",
    "    inputs, targets, keep_prob = placeholders(batch_size, num_steps)\n",
    "\n",
    "    ## Creating LSTM Cell\n",
    "    cell, initial_state = lstms(lstm_size, num_layers, batch_size, keep_prob)\n",
    "\n",
    "    ## One hot encode inputs to run the data through RNN layers\n",
    "    one_hot_inputs = tf.one_hot(inputs, num_classes)\n",
    "\n",
    "    ## Collect outputs\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, one_hot_inputs, initial_state = initial_state)\n",
    "    final_state = state\n",
    "\n",
    "    ## Predictions and Logits\n",
    "    prediction, logits = rnn_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "    ## Getting losses and optimizer(ADAM)\n",
    "    loss = losses(logits, targets, lstm_size, num_classes)\n",
    "    optimizer = cal_optimizer(loss, learning_rate, grad_clip)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(initial_state)\n",
    "        \n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {inputs: x,\n",
    "                    keep_prob: 1.,\n",
    "                    initial_state: new_state}\n",
    "            preds, new_state = sess.run([prediction, final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = select_characters(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {inputs: x,\n",
    "                    keep_prob: 1.,\n",
    "                    initial_state: new_state}\n",
    "            preds, new_state = sess.run([prediction, final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = select_characters(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i138_l128.ckpt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i138_l128.ckpt\n",
      "Faraaeatoettoa  o eee tte  a otea  a a e o e aeeteaa  e a eeee  toa ee  e   oat tt   oat ot  ea eatta   tea  t oaa e ee a e ea oettoa ot   eeeao  eaot     ete   tt  t ota et  a o ao oeate    t too  e too aeae te ot ao eao t  aotat e  ee a tea e t otaea ee t ooto totaee eoetae oto  ttoet ae  aaootot a oeaeeeee eteoo ttt ta ooae aatt o e oaee o  t  ot eae e oeaa  a a oto ae  ae e e   at e tee  ta ooat   oe o  aeteteaeeettt teee a aaoo  t e t eo e aaot ao tt  aa    aaoe t  o ao   ette t  o a   te  ee   ee e e e eoooete oto  a ttaee tae  ooa oa  tao   t teeeta  eetae aat t t a oet t  aa   o   eoo oo e  teo  ea  t eea    ta  e aa eee eee e a et ta  aao t ao e   otoeaotea  tot e e   e  ee  t o ooo eet t e o eo a    atao  toe  tto a    oeto  toa aoe o o ee o teoaoeoe   oet eoaot oa a tt   ee eo  eate  t oa toteooa  a t eot a oattoa eatao ao ttta a  o a   e  eee taa  a  te tat  o o aea     a  tt tatt   aeoeee   ea ao e ao a    t  t oe    ee tt aae  eoat o t otoeet t  tea    atetaaeoooa ata  o a  a\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample_and_select(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
